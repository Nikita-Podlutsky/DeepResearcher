import time
import os
import logging
import re
import random
from urllib.parse import urlparse, quote_plus
from typing import List, Dict, Any, Optional, Set, Tuple

# --- Scrapy Imports ---
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy.spiders import Spider
from scrapy import signals
from scrapy.exceptions import CloseSpider

# --- Other Libraries ---
from dotenv import load_dotenv
from duckduckgo_search import DDGS
import trafilatura
from newspaper import Article, ArticleException

# --- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ ---
import requests
from bs4 import BeautifulSoup
import urllib.parse

# --- –ù–∞—á–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ ---
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# –£–º–µ–Ω—å—à–∞–µ–º —à—É–º –æ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫
logging.getLogger('scrapy').propagate = False
# logging.getLogger('scrapy').setLevel(logging.INFO) # Scrapy –±—É–¥–µ—Ç —É–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —á–µ—Ä–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∏–∂–µ
logging.getLogger('duckduckgo_search').setLevel(logging.INFO)
logging.getLogger('urllib3').propagate = False
logging.getLogger('trafilatura').setLevel(logging.WARNING)
logging.getLogger('newspaper').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING) # –£–º–µ–Ω—å—à–∞–µ–º —à—É–º –æ—Ç requests

# –°–æ–∑–¥–∞–µ–º —Å–≤–æ–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
logger = logging.getLogger('search_spider')
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s [%(name)s] %(levelname)s: %(message)s', '%Y-%m-%d %H:%M:%S')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ ---
SEARCH_RESULTS_PER_QUERY = 8  # –£–≤–µ–ª–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
SEARCH_DELAY = 4.0  # –ù–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤
MIN_CONTENT_LENGTH = 150  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
MAX_RETRIES = 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ (DDG)
DIVERSE_QUERY_COUNT = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞

# –°–ø–∏—Å–æ–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö user agents –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36', # More recent
    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1' # Mobile
]

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def generate_alternative_queries(original_query: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    query_templates = [
        "{} –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ",
        "—á—Ç–æ —Ç–∞–∫–æ–µ {}",
        "{} —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
        "{} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
        "{} –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
        "{} tutorial",
        "{} how to",
        "{} explained",
        "understanding {}",
        "{} guide",
        "{} best practices",
        "{} introduction"
    ]

    # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω–æ–≤ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, —Å—Ç–∞—Ä–∞—è—Å—å –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è —Å–∏–ª—å–Ω–æ
    num_to_select = min(DIVERSE_QUERY_COUNT, len(query_templates))
    selected_templates = random.sample(query_templates, num_to_select)
    alternative_queries = [template.format(original_query) for template in selected_templates]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞, –µ—Å–ª–∏ –µ–≥–æ —Ç–∞–º –µ—â–µ –Ω–µ—Ç
    if original_query not in alternative_queries:
        alternative_queries.insert(0, original_query)

    return alternative_queries[:DIVERSE_QUERY_COUNT + 1] # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ —á–∏—Å–ª–æ

def is_valid_url(url: Optional[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    if not url or not isinstance(url, str):
        return False

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if not (url.startswith('http://') or url.startswith('https://')):
        return False

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    excluded_extensions = ('.pdf', '.docx', '.xlsx', '.pptx', '.zip', '.rar', '.jpg', '.png', '.gif', '.mp3', '.mp4',
                          '.avi', '.exe', '.dmg', '.iso', '.xml', '.json', '.css', '.js', '.svg', '.webp', '.ico')
    try:
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        if path and path.endswith(excluded_extensions):
            return False
    except Exception:
        # –ï—Å–ª–∏ URL –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è, —Å—á–∏—Ç–∞–µ–º –µ–≥–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã (—Å–æ—Ü—Å–µ—Ç–∏, –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Ç.–¥.)
    excluded_domains = ('facebook.com', 'twitter.com', 'instagram.com', 'youtube.com', 'tiktok.com', 'pinterest.com',
                       'linkedin.com', 't.me', 'telegram.org', 'vk.com', 'ok.ru', 'quora.com', 'reddit.com',
                       'amazon.', 'ebay.', 'aliexpress.', 'google.com/search', 'yandex.ru/search', 'bing.com/search',
                       'slideshare.net', 'scribd.com', 'academia.edu', 'researchgate.net', # –ß–∞—Å—Ç–æ —Ç—Ä–µ–±—É—é—Ç –ª–æ–≥–∏–Ω
                       'codepen.io', 'jsfiddle.net' # –ü–µ—Å–æ—á–Ω–∏—Ü—ã –∫–æ–¥–∞, –Ω–µ —Å—Ç–∞—Ç—å–∏
                       )
    try:
        domain = parsed_url.netloc.lower()
        # –£–±–∏—Ä–∞–µ–º 'www.' –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        if domain.startswith('www.'):
            domain = domain[4:]
        if domain and any(bad_domain in domain for bad_domain in excluded_domains):
            #logger.debug(f"Excluding URL due to domain: {url}")
            return False
    except Exception:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–º–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        pass

    # –ò—Å–∫–ª—é—á–∞–µ–º URL, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –ø–æ–∏—Å–∫ –≤–Ω—É—Ç—Ä–∏ —Å–∞–π—Ç–∞
    if 'search' in url.lower() or 'find' in url.lower() or '?' in url and ('q=' in url or 'query=' in url):
        # logger.debug(f"Excluding URL due to potential search path: {url}")
        return False

    return True

def fallback_search_yandex(query: str, num_results: int = 10) -> List[Dict[str, str]]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex (–±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API)."""
    results = []
    encoded_query = quote_plus(query)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º lr=213 –¥–ª—è –ú–æ—Å–∫–≤—ã, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    search_url = f"https://yandex.ru/search/?text={encoded_query}&lr=213"

    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Referer': 'https://yandex.ru/',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0',
    }

    try:
        #logger.info(f"Fallback search: Requesting Yandex for '{query}'")
        response = requests.get(search_url, headers=headers, timeout=15) # –£–≤–µ–ª–∏—á–∏–º —Ç–∞–π–º–∞—É—Ç
        response.raise_for_status() # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ HTTP –æ—à–∏–±–∫–∏

        soup = BeautifulSoup(response.text, 'html.parser')
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞. –°–µ–ª–µ–∫—Ç–æ—Ä—ã –º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å—Å—è!
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –æ–±—â–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        links = soup.select('li.serp-item h2 a[href]')

        found_count = 0
        for link in links:
            url = link.get('href')
            # –Ø–Ω–¥–µ–∫—Å –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å –º—É—Å–æ—Ä, —á–∏—Å—Ç–∏–º URL
            if url and url.startswith('http') and 'yandex.ru/clck/' not in url:
                 # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å *–ø–æ—Å–ª–µ* –±–∞–∑–æ–≤–æ–π –æ—á–∏—Å—Ç–∫–∏
                if is_valid_url(url):
                    title = link.get_text(strip=True)
                    # logger.debug(f"  [Yandex Found]: {title} - {url}")
                    results.append({'href': url, 'title': title})
                    found_count += 1
                    if found_count >= num_results:
                        break
            # else: logger.debug(f"  [Yandex Skipped Invalid URL]: {url}")

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ Yandex ({type(e).__name__}): {e}")
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ Yandex: {e}")

    logger.info(f"Yandex fallback for '{query}' returned {len(results)} valid results.")
    return results

def fallback_search_bing(query: str, num_results: int = 10) -> List[Dict[str, str]]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing (–±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API)."""
    results = []
    encoded_query = quote_plus(query)
    search_url = f"https://www.bing.com/search?q={encoded_query}"

    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8', # –î–æ–±–∞–≤–∏–º —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
        'Referer': 'https://www.bing.com/',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    try:
        #logger.info(f"Fallback search: Requesting Bing for '{query}'")
        response = requests.get(search_url, headers=headers, timeout=15) # –£–≤–µ–ª–∏—á–∏–º —Ç–∞–π–º–∞—É—Ç
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        # –°–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö Bing (–º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è)
        links = soup.select("li.b_algo h2 a")

        found_count = 0
        for link in links:
            url = link.get('href')
            if url and is_valid_url(url): # Bing –æ–±—ã—á–Ω–æ –¥–∞–µ—Ç —á–∏—Å—Ç—ã–µ URL
                title = link.get_text(strip=True)
                # logger.debug(f"  [Bing Found]: {title} - {url}")
                results.append({'href': url, 'title': title})
                found_count += 1
                if found_count >= num_results:
                    break
            # else: logger.debug(f"  [Bing Skipped Invalid URL]: {url}")

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ Bing ({type(e).__name__}): {e}")
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ Bing: {e}")

    logger.info(f"Bing fallback for '{query}' returned {len(results)} valid results.")
    return results

# --- –£–ª—É—á—à–µ–Ω–Ω—ã–π Spider ---

class EnhancedArticleSpider(Spider):
    name = 'enhanced_article_spider'

    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    urls_found_for_task: Dict[Tuple[str, str], Set[str]] # {(plan_item_id, query_id): {set_of_urls}}
    urls_to_scrape: Dict[str, Dict[str, Any]]           # {url: original_task_info}
    failed_searches: List[Dict[str, Any]]               # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ *–∑–∞–¥–∞—á–∏* (–µ—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω URL –Ω–µ –Ω–∞–π–¥–µ–Ω)
    processed_urls: Set[str]                            # URL, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –±—ã–ª yield Request
    visited_urls: Set[str]                              # URL, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏–ª–∏ –Ω–µ—É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã parse_article/handle_error

    def __init__(self, search_tasks: List[Dict[str, Any]] = None, results_per_query: int = 3, *args, **kwargs):
        super(EnhancedArticleSpider, self).__init__(*args, **kwargs)
        if search_tasks is None or not isinstance(search_tasks, list):
            raise ValueError("Spider needs 'search_tasks' argument (list of dicts)")
        if not search_tasks:
            raise ValueError("'search_tasks' list cannot be empty.")

        self.search_tasks = search_tasks
        self.results_per_query = results_per_query

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö
        self.urls_found_for_task = {}
        self.urls_to_scrape = {}
        self.failed_searches = []
        self.processed_urls = set()
        self.visited_urls = set()

        self.logger.info(f"Spider initialized for {len(search_tasks)} search tasks (target: {results_per_query} results per query).")

    def start_requests(self):
        # –®–∞–≥ 1: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ URL –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.logger.info("--- Starting Search Phase ---")
        search_requests_made = 0
        total_urls_collected = 0

        for task_index, task_info in enumerate(self.search_tasks):
            base_query = task_info.get('query')
            plan_item_id = task_info.get('plan_item_id', f'task_{task_index}') # ID –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            query_id = task_info.get('query_id', 'q_0')                   # ID –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            task_key = (plan_item_id, query_id)

            if not base_query:
                self.logger.warning(f"Skipping task with empty query: {task_info}")
                continue

            self.logger.info(f"\n--- Processing Task {task_index+1}/{len(self.search_tasks)} (ID: {task_key}): Base Query = '{base_query}' ---")
            self.urls_found_for_task[task_key] = set() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–±–æ—Ä URL –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            alternative_queries = generate_alternative_queries(base_query)
            self.logger.info(f"Generated {len(alternative_queries)} query variations: {alternative_queries}")

            task_urls_found_count = 0
            attempted_queries = 0

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
            for query_index, query in enumerate(alternative_queries):
                if task_urls_found_count >= self.results_per_query:
                    self.logger.info(f"Target of {self.results_per_query} URLs reached for task {task_key}, stopping search variations.")
                    break

                self.logger.info(f"Trying query variation {query_index+1}/{len(alternative_queries)}: '{query}'")
                attempted_queries += 1
                new_urls_from_ddg = []

                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ DuckDuckGo
                try:
                    new_urls_from_ddg = self._search_with_ddg(query, task_info, task_key)
                    task_urls_found_count += len(new_urls_from_ddg)
                    search_requests_made += 1
                    self.logger.info(f"DDG added {len(new_urls_from_ddg)} new URLs. Total for task {task_key}: {task_urls_found_count}")
                except Exception as e:
                    self.logger.error(f"Unexpected error during DDG search for '{query}': {e}")


                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞–º (–¥–∞–∂–µ –µ—Å–ª–∏ DDG –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
                if query_index < len(alternative_queries) - 1 or task_urls_found_count < self.results_per_query:
                     # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ fallback –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º DDG –∑–∞–ø—Ä–æ—Å–æ–º
                     delay = SEARCH_DELAY * (0.8 + 0.4 * random.random()) # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ ¬±20%
                     self.logger.info(f"Pausing for {delay:.2f}s before next search action")
                     time.sleep(delay)

                # –ï—Å–ª–∏ DuckDuckGo –Ω–µ –Ω–∞—à–µ–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ *–¥–ª—è —ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–∏*
                # –ò –µ—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è (–æ—Å–Ω–æ–≤–Ω–∞—è) —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –ü–°
                if not new_urls_from_ddg and query_index == 0 and task_urls_found_count < self.results_per_query:
                    self.logger.info(f"DDG found no new URLs for the primary query variation, trying fallback search...")

                    # –ü—Ä–æ–±—É–µ–º Yandex
                    if task_urls_found_count < self.results_per_query:
                        try:
                             new_urls_from_yandex = self._search_with_fallback(query, task_info, task_key, 'yandex')
                             task_urls_found_count += len(new_urls_from_yandex)
                             self.logger.info(f"Yandex added {len(new_urls_from_yandex)} new URLs. Total for task {task_key}: {task_urls_found_count}")
                             if new_urls_from_yandex: time.sleep(SEARCH_DELAY * 0.5) # –ö—Ä–∞—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ fallback
                        except Exception as e:
                             self.logger.error(f"Unexpected error during Yandex fallback for '{query}': {e}")


                    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –ø—Ä–æ–±—É–µ–º Bing
                    if task_urls_found_count < self.results_per_query:
                        try:
                            new_urls_from_bing = self._search_with_fallback(query, task_info, task_key, 'bing')
                            task_urls_found_count += len(new_urls_from_bing)
                            self.logger.info(f"Bing added {len(new_urls_from_bing)} new URLs. Total for task {task_key}: {task_urls_found_count}")
                            if new_urls_from_bing: time.sleep(SEARCH_DELAY * 0.5) # –ö—Ä–∞—Ç–∫–∞—è –ø–∞—É–∑–∞
                        except Exception as e:
                            self.logger.error(f"Unexpected error during Bing fallback for '{query}': {e}")

            # –ò—Ç–æ–≥–∏ –ø–æ –∑–∞–¥–∞—á–µ
            self.logger.info(f"--- Task {task_key} Search Summary ---")
            self.logger.info(f"Attempted {attempted_queries} query variations.")
            self.logger.info(f"Collected {task_urls_found_count} unique valid URLs for this task.")
            total_urls_collected += task_urls_found_count

            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ –Ω–µ –Ω–∞—à–ª–∏ URLs, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ—É–¥–∞—á–Ω—ã—Ö *–∑–∞–¥–∞—á*
            if task_urls_found_count == 0:
                self.failed_searches.append(task_info)
                self.logger.warning(f"‚ùå FAILED TASK: No URLs found for task {task_key} (query: '{base_query}') after all attempts.")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –µ—â–µ –∑–∞–¥–∞—á–∏
            if task_index < len(self.search_tasks) - 1:
                delay = SEARCH_DELAY * 1.2 * (0.9 + 0.2 * random.random()) # –ù–µ–º–Ω–æ–≥–æ –¥–æ–ª—å—à–µ, ¬±10%
                self.logger.info(f"--- Pausing for {delay:.2f}s before next task ---")
                time.sleep(delay)

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ allowed_domains –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å Scrapy)
        allowed_domains_set = set()
        for url in self.urls_to_scrape.keys():
            try:
                domain = urlparse(url).netloc
                if domain:
                    # –£–±–∏—Ä–∞–µ–º www. –¥–ª—è –±–æ–ª–µ–µ –æ–±—â–µ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
                    if domain.startswith('www.'):
                         domain = domain[4:]
                    allowed_domains_set.add(domain)
            except Exception as e:
                self.logger.warning(f"Could not parse domain from URL '{url}': {e}")
        self.allowed_domains = list(allowed_domains_set)
        self.logger.info(f"Configured allowed_domains: {len(self.allowed_domains)} domains")


        self.logger.info(f"\n--- Search Phase Complete ---")
        self.logger.info(f"Total unique URLs collected across all tasks: {len(self.urls_to_scrape)}")
        self.logger.info(f"Total search engine requests made (approx): {search_requests_made} (excluding fallbacks)")
        if self.failed_searches:
             self.logger.warning(f"Found {len(self.failed_searches)} tasks with zero results.")

        # –®–∞–≥ 2: –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã Scrapy –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL
        if not self.urls_to_scrape:
            self.logger.warning("No valid URLs found to scrape after all searches. Stopping spider.")
            # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º CloseSpider –∑–¥–µ—Å—å, —Ç.–∫. –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è —à—Ç–∞—Ç–Ω–æ
            return # –ü—Ä–æ—Å—Ç–æ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã

        self.logger.info(f"\n--- Starting Scrapy Download Phase for {len(self.urls_to_scrape)} URLs ---")
        request_count = 0
        for url, task_info in self.urls_to_scrape.items():
            if url not in self.processed_urls:
                request_count += 1
                self.logger.debug(f"Yielding request {request_count}/{len(self.urls_to_scrape)}: {url}")
                self.processed_urls.add(url)
                yield scrapy.Request(
                    url,
                    callback=self.parse_article,
                    errback=self.handle_error,
                    meta={
                        'task_info': task_info,
                        # –†–∞–∑—Ä–µ—à–∞–µ–º Scrapy –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —ç—Ç–∏ –∫–æ–¥—ã –±–µ–∑ –æ—à–∏–±–æ–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        'handle_httpstatus_list': [403, 404, 500, 503, 429, 502, 504],
                        'download_timeout': 30,  # –¢–∞–π–º–∞—É—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                        # –î–æ–±–∞–≤–∏–º –∏–∑–Ω–∞—á–∞–ª—å–Ω—É—é –ø–æ–ø—ã—Ç–∫—É (Scrapy –¥–æ–±–∞–≤–∏—Ç —Å–≤–æ–∏ –ø—Ä–∏ RETRY_ENABLED)
                        'retry_times': 0
                    },
                    # –î–æ–±–∞–≤–∏–º —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    headers={'User-Agent': random.choice(USER_AGENTS)}
                )
            else:
                 self.logger.debug(f"Skipping already processed URL: {url}")


    def _add_url_if_valid(self, url: str, task_info: Dict[str, Any], task_key: Tuple[str, str], source: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL, –¥–æ–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ –æ–±—â–∏–µ –∏ –∑–∞–¥–∞—á–Ω—ã–µ —Å–ø–∏—Å–∫–∏, –µ—Å–ª–∏ –æ–Ω –≤–∞–ª–∏–¥–µ–Ω –∏ –Ω–æ–≤."""
        task_urls = self.urls_found_for_task.setdefault(task_key, set())

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –ª–∏–º–∏—Ç–∞ –¥–ª—è *—ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏*
        if len(task_urls) >= self.results_per_query:
            # self.logger.debug(f"  Limit reached for task {task_key}, skipping URL: {url}")
            return False

        if is_valid_url(url):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —ç—Ç–æ—Ç URL —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω –¥–ª—è *–ª—é–±–æ–π* –∑–∞–¥–∞—á–∏
            if url not in self.urls_to_scrape:
                self.urls_to_scrape[url] = task_info # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
                task_urls.add(url)              # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ URL –¥–ª—è *—ç—Ç–æ–π* –∑–∞–¥–∞—á–∏
                self.logger.debug(f"  [+] Added URL from {source}: {url} (Task: {task_key})")
                return True
            else:
                # URL —É–∂–µ –µ—Å—Ç—å, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–∏. –ü—Ä–æ–≤–µ—Ä–∏–º, –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ –æ–Ω –∫ *—ç—Ç–æ–π* –∑–∞–¥–∞—á–µ.
                if url not in task_urls:
                     task_urls.add(url)
                     self.logger.debug(f"  [=] Added existing URL to task {task_key}: {url} (From: {source})")
                     # –ù–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ "–Ω–æ–≤—ã–º" –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º, –Ω–æ —Å–≤—è–∑–∞–ª–∏ —Å –∑–∞–¥–∞—á–µ–π
                     return False # –ù–µ —Å—á–∏—Ç–∞–µ–º –∑–∞ –Ω–æ–≤–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫ –æ–±—â–µ–º—É —Å—á–µ—Ç—á–∏–∫—É
                else:
                     # self.logger.debug(f"  [=] URL already collected for task {task_key}: {url} (From: {source})")
                     return False # –£–∂–µ –µ—Å—Ç—å –∏ –≤ –æ–±—â–µ–º, –∏ –≤ –∑–∞–¥–∞—á–Ω–æ–º —Å–ø–∏—Å–∫–µ
        else:
            # self.logger.debug(f"  [-] Invalid URL skipped: {url} (From: {source})")
            return False

    def _search_with_ddg(self, query: str, task_info: Dict[str, Any], task_key: Tuple[str, str]) -> List[str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ *–Ω–æ–≤—ã—Ö* URL, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏."""
        newly_added_urls = []
        retry_count = 0
        task_urls = self.urls_found_for_task.setdefault(task_key, set())
        results_needed_for_task = self.results_per_query - len(task_urls)

        if results_needed_for_task <= 0:
             # self.logger.debug(f"DDG search skipped for '{query}', task {task_key} already has enough URLs.")
             return newly_added_urls # –£–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ URL –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏

        while retry_count < MAX_RETRIES:
            try:
                # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Ç.–∫. –±—É–¥–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å
                # –£—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —É–∂–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏
                max_results_to_fetch = results_needed_for_task + 8
                self.logger.info(f"DDG search for '{query}' (Task: {task_key}, Attempt: {retry_count+1}/{MAX_RETRIES}, Need: {results_needed_for_task}, Fetching: {max_results_to_fetch})")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä DDGS
                with DDGS(headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=20) as ddgs:
                    results_iterator = ddgs.text(query, max_results=max_results_to_fetch)

                    results_processed = 0
                    if results_iterator:
                        for r in results_iterator:
                             # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞–±—Ä–∞–ª–∏ –ª–∏ —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∑–∞–¥–∞—á–∏ *–≤–æ –≤—Ä–µ–º—è* –∏—Ç–µ—Ä–∞—Ü–∏–∏
                             if len(self.urls_found_for_task[task_key]) >= self.results_per_query:
                                 break

                             if r and isinstance(r, dict) and 'href' in r:
                                 url = r.get('href')
                                 results_processed += 1
                                 if self._add_url_if_valid(url, task_info, task_key, "DDG"):
                                     newly_added_urls.append(url)
                             else:
                                 self.logger.debug(f"  [DDG Invalid Result Format]: {r}")


                        self.logger.debug(f"DDG processed {results_processed} results for '{query}'.")
                        # –ï—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫
                        break
                    else:
                        self.logger.warning(f"DDG returned no results iterator for query '{query}'")
                        # –ù–µ –≤—ã—Ö–æ–¥–∏–º, –ø–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑

            except Exception as e:
                self.logger.error(f"Error during DDG search for '{query}' (Attempt {retry_count+1}): {type(e).__name__} - {e}")
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞–π–º–∞—É—Ç–æ–≤

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –∏ –¥–µ–ª–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
            retry_count += 1
            if retry_count < MAX_RETRIES:
                retry_delay = SEARCH_DELAY * (0.5 + retry_count * 0.3) # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                self.logger.info(f"Retrying DDG search in {retry_delay:.2f}s...")
                time.sleep(retry_delay)
            else:
                 self.logger.warning(f"Max retries reached for DDG search on '{query}'.")

        return newly_added_urls

    def _search_with_fallback(self, query: str, task_info: Dict[str, Any], task_key: Tuple[str, str], search_engine: str) -> List[str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∑–∞–ø–∞—Å–Ω–æ–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ *–Ω–æ–≤—ã—Ö* URL, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏."""
        newly_added_urls = []
        task_urls = self.urls_found_for_task.setdefault(task_key, set())
        results_needed_for_task = self.results_per_query - len(task_urls)

        if results_needed_for_task <= 0:
            # self.logger.debug(f"{search_engine.capitalize()} fallback skipped for '{query}', task {task_key} already has enough URLs.")
            return newly_added_urls

        self.logger.info(f"Trying fallback search via {search_engine.capitalize()} for '{query}' (Task: {task_key}, Need: {results_needed_for_task})")

        try:
            results = []
            # –í—ã–±–∏—Ä–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞
            if search_engine == 'yandex':
                results = fallback_search_yandex(query, num_results=results_needed_for_task + 5)
            elif search_engine == 'bing':
                results = fallback_search_bing(query, num_results=results_needed_for_task + 5)
            else:
                self.logger.error(f"Unknown fallback search engine: {search_engine}")
                return newly_added_urls

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if results:
                results_processed = 0
                for r in results:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–¥–∞—á–∏ –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞
                    if len(self.urls_found_for_task[task_key]) >= self.results_per_query:
                        break

                    url = r.get('href')
                    results_processed += 1
                    if self._add_url_if_valid(url, task_info, task_key, search_engine.capitalize()):
                        newly_added_urls.append(url)

                self.logger.debug(f"{search_engine.capitalize()} processed {results_processed} results for '{query}'.")
            else:
                self.logger.warning(f"No results from {search_engine.capitalize()} fallback for query '{query}'")

        except Exception as e:
            self.logger.error(f"Error during {search_engine.capitalize()} fallback search for '{query}': {type(e).__name__} - {e}")

        return newly_added_urls

    def parse_article(self, response):
        # –®–∞–≥ 3: –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        url = response.url
        task_info = response.meta.get('task_info', {})
        status = response.status
        self.visited_urls.add(url) # –û—Ç–º–µ—á–∞–µ–º URL –∫–∞–∫ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–π

        self.logger.info(f"Processing response from: {url} (Status: {status})")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        if status >= 400:
             self.logger.warning(f"Received non-2xx status {status} for {url}. Skipping content parsing.")
             # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º item
             return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –Ω–µ HTML)
        content_type = response.headers.get('Content-Type', b'').decode('utf-8', errors='ignore').lower()
        if 'html' not in content_type and 'text' not in content_type:
            self.logger.warning(f"Skipping non-HTML content: {url} (Type: {content_type})")
            return

        extracted_text = None
        extraction_method = None
        title = ""

        # 0. –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å –¥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞)
        try:
            title = response.css('title::text').get() or ""
            title = title.strip()
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ h1, –µ—Å–ª–∏ title –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π
            if not title or title.lower() in ["home", "index", "blog", "article"]:
                 h1_text = response.css('h1::text').get()
                 if h1_text:
                      title = h1_text.strip()
            # self.logger.debug(f"  Title extracted: '{title}'")
        except Exception as e:
            self.logger.debug(f"  Could not extract title for {url}: {e}")
            title = ""


        # 1. –ü–æ–ø—ã—Ç–∫–∞ —Å Trafilatura (–æ–±—ã—á–Ω–æ –ª—É—á—à–∏–π)
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Trafilatura –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            extracted_text = trafilatura.extract(
                response.body,
                include_comments=False,    # –ù–µ –≤–∫–ª—é—á–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                include_tables=True,       # –í–∫–ª—é—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã (–º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã)
                include_formatting=True,   # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –±–∞–∑–æ–≤–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∞–±–∑–∞—Ü—ã)
                include_links=False,       # –ù–µ –≤–∫–ª—é—á–∞—Ç—å —Å–∞–º–∏ —Å—Å—ã–ª–∫–∏
                output_format='text',      # –ü–æ–ª—É—á–∏—Ç—å —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
                url=url                    # –ü–µ—Ä–µ–¥–∞–µ–º URL –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            )
            if extracted_text:
                 extracted_text = extracted_text.strip() # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
                 if len(extracted_text) >= MIN_CONTENT_LENGTH:
                    extraction_method = "trafilatura"
                    # self.logger.debug(f"  Extracted ~{len(extracted_text)} chars using Trafilatura.")
                 else:
                    # self.logger.debug(f"  Trafilatura extracted short text ({len(extracted_text)} chars). Discarding.")
                    extracted_text = None
            else:
                 # self.logger.debug(f"  Trafilatura extracted no text.")
                 extracted_text = None
        except Exception as e:
            extracted_text = None
            self.logger.warning(f"  Trafilatura failed for {url}: {e}")

        # 2. –ü–æ–ø—ã—Ç–∫–∞ —Å Newspaper3k (–µ—Å–ª–∏ Trafilatura –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
        if not extracted_text:
            # self.logger.debug(f"  Trying Newspaper3k fallback for {url}...")
            try:
                article = Article(url=url, language='ru' if '.ru/' in url or '.—Ä—Ñ/' in url else 'en') # –ü–æ–º–æ–∂–µ–º —Å —è–∑—ã–∫–æ–º
                # –ü–µ—Ä–µ–¥–∞–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π HTML
                article.download(input_html=response.body.decode(response.encoding, errors='ignore'))
                article.parse()
                if article.text:
                     article_text = article.text.strip()
                     if len(article_text) >= MIN_CONTENT_LENGTH:
                        extracted_text = article_text
                        extraction_method = "newspaper3k"
                        # self.logger.debug(f"  Extracted ~{len(extracted_text)} chars using Newspaper3k (fallback).")
                        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ newspaper, –µ—Å–ª–∏ –Ω–∞—à –ø—É—Å—Ç
                        if not title and article.title:
                             title = article.title.strip()
                     else:
                         # self.logger.debug(f"  Newspaper3k extracted short text ({len(article_text)} chars). Discarding.")
                         extracted_text = None
                else:
                     # self.logger.debug(f"  Newspaper3k extracted no text.")
                     extracted_text = None
            except ArticleException as e:
                extracted_text = None
                self.logger.debug(f"  Newspaper3k ArticleException for {url}: {e}")
            except Exception as e:
                extracted_text = None
                self.logger.warning(f"  Newspaper3k failed unexpectedly for {url}: {e}")

        # 3. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞: –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å BeautifulSoup (–µ—Å–ª–∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ –Ω–µ —É–¥–∞–ª–æ—Å—å)
        if not extracted_text:
            # self.logger.debug(f"  Trying simple HTML parsing (BeautifulSoup) for {url}...")
            try:
                soup = BeautifulSoup(response.body, 'lxml') # –ò—Å–ø–æ–ª—å–∑—É–µ–º lxml –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–≥–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                main_content = soup.find('main') or soup.find('article') or soup.find('div', role='main') or soup.find('div', class_=re.compile(r'(content|main|body|post|entry)', re.I))

                if not main_content:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫, –±–µ—Ä–µ–º body —Ü–µ–ª–∏–∫–æ–º
                    main_content = soup.body

                if main_content:
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–Ω—É—Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –±–ª–æ–∫–∞
                    for element in main_content.select('script, style, nav, footer, header, aside, form, iframe, noscript, .sidebar, #sidebar, .comments, #comments, .related-posts, .social-links, .ad, [aria-hidden="true"]'):
                        element.extract()

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è –∞–±–∑–∞—Ü—ã
                    paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'pre', 'code', 'td', 'th'])
                    text_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)] # –ë–µ—Ä–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ
                    raw_text = '\n\n'.join(text_parts) # –°–æ–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏

                    # –ë–∞–∑–æ–≤–∞—è —á–∏—Å—Ç–∫–∞
                    clean_text = re.sub(r'\s{2,}', ' ', raw_text).strip() # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏
                    clean_text = re.sub(r'\n{3,}', '\n\n', clean_text)    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫

                    if len(clean_text) >= MIN_CONTENT_LENGTH:
                        extracted_text = clean_text
                        extraction_method = "simple_html"
                        # self.logger.debug(f"  Extracted ~{len(extracted_text)} chars using simple HTML parsing (last resort).")
                    else:
                        # self.logger.debug(f"  Simple HTML parsing extracted short/no text ({len(clean_text)} chars).")
                        pass # –ù–µ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º extracted_text –≤ None –∑–¥–µ—Å—å
                else:
                     # self.logger.debug(f"  Could not find <body> or main content block for simple parsing.")
                     pass

            except Exception as e:
                self.logger.warning(f"  Simple HTML parsing failed for {url}: {e}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
        if extracted_text:
            self.logger.info(f"‚úÖ Successfully extracted text from: {url} (Method: {extraction_method}, Length: {len(extracted_text)})")
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = re.sub(r'\s{2,}', ' ', extracted_text.strip())
            cleaned_text = re.sub(r'(\r\n|\r|\n){2,}', '\n\n', cleaned_text) # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫

            yield {
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏
                'query': task_info.get('query'),
                'plan_item': task_info.get('plan_item'),
                'plan_item_id': task_info.get('plan_item_id'),
                'query_id': task_info.get('query_id'),
                # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
                'url': url,
                'title': title or "No Title Found", # –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                'text': cleaned_text,
                'extraction_method': extraction_method,
                'content_length': len(cleaned_text)
            }
        else:
            self.logger.warning(f"‚ùå Failed to extract significant text content from: {url} after all attempts.")
            # –ú–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å item —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω–∏—á–µ–≥–æ
            # yield {
            #     'query': task_info.get('query'),
            #     'plan_item': task_info.get('plan_item'),
            #     'plan_item_id': task_info.get('plan_item_id'),
            #     'query_id': task_info.get('query_id'),
            #     'url': url,
            #     'title': title or "No Title Found",
            #     'text': "",
            #     'extraction_method': "failed",
            #     'content_length': 0
            # }

    def handle_error(self, failure):
        request = failure.request
        url = request.url
        self.visited_urls.add(url) # –û—Ç–º–µ—á–∞–µ–º URL –∫–∞–∫ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–π, –¥–∞–∂–µ –µ—Å–ª–∏ –æ—à–∏–±–∫–∞

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
        error_type = failure.type.__name__ if failure.type else 'Unknown Error'
        error_message = str(failure.value) # –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ

        self.logger.error(f"üï∑Ô∏è Request failed for URL: {url}")
        self.logger.error(f"  Error Type: {error_type}")
        self.logger.error(f"  Error Message: {error_message}")

        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if request.meta:
            task_info = request.meta.get('task_info', {})
            retry_times = request.meta.get('retry_times', 0)
            self.logger.error(f"  Associated Query: '{task_info.get('query', 'N/A')}'")
            self.logger.error(f"  Retry attempt: {retry_times}")
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–∞–¥–∞—é—â–∏—Ö URL –∏–ª–∏ –¥–æ–º–µ–Ω–æ–≤


# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Scrapy –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞ ---

def run_enhanced_scrape(search_tasks: List[Dict[str, Any]], results_per_query: int) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á.

    Args:
        search_tasks: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏.
                      –ö–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º 'query'.
                      –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ —Ç–∞–∫–∂–µ 'plan_item', 'plan_item_id', 'query_id' –¥–ª—è –ª—É—á—à–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.
        results_per_query: –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ *—É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö* —Å–∞–π—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å (—Ü–µ–ª—å, –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—è).

    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —Å–ø–∏—Å–∫–æ–≤:
        1. –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (list of dicts, –∫–∞–∫ yield –ø–∞—É–∫–∞).
        2. –ó–∞–¥–∞—á–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ URL (list of dicts, –∏—Å—Ö–æ–¥–Ω—ã–µ –∑–∞–¥–∞—á–∏).
    """
    if not search_tasks:
        logger.error("–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –∑–∞–ø—É—Å–∫–∞.")
        return [], []
    if not isinstance(search_tasks, list) or not all(isinstance(task, dict) and 'query' in task for task in search_tasks):
        logger.error("–û—à–∏–±–∫–∞: 'search_tasks' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤–∞—Ä–µ–π, –∫–∞–∂–¥—ã–π —Å –∫–ª—é—á–æ–º 'query'.")
        return [], []
    if not isinstance(results_per_query, int) or results_per_query <= 0:
        logger.error("–û—à–∏–±–∫–∞: 'results_per_query' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º.")
        return [], []

    logger.info(f"\n=== –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–ª—è {len(search_tasks)} –∑–∞–¥–∞—á ===")
    logger.info(f"–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –Ω–∞ –∑–∞–¥–∞—á—É: {results_per_query}")
    start_time = time.time()

    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    scraped_items = []
    final_failed_searches = [] # –ó–∞–¥–∞—á–∏, –≥–¥–µ –Ω–µ –Ω–∞—à–ª–∏ URL

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ item_scraped
    def item_scraped_handler(item, response, spider):
        if item and isinstance(item, dict):
            scraped_items.append(dict(item)) # –ö–æ–ø–∏—Ä—É–µ–º item
            spider.logger.info(f"Item collected: {item.get('url')} (Query: '{item.get('query')}')")

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ spider_closed
    def spider_closed_handler(spider, reason):
        nonlocal final_failed_searches
        logger.info(f"Spider closed. Reason: {reason}")
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –Ω–∞—à–ª–∏ URL, –∏–∑ —Å–∞–º–æ–≥–æ –ø–∞—É–∫–∞
        final_failed_searches = getattr(spider, 'failed_searches', [])
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–º URL
        visited = getattr(spider, 'visited_urls', set())
        processed = getattr(spider, 'processed_urls', set())
        logger.info(f"Spider stats: Processed {len(processed)} URL requests, Visited {len(visited)} URLs (includes errors/redirects).")


    # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Scrapy (–û–ë–ù–û–í–õ–ï–ù–û) ---
    settings = get_project_settings()
    # –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: INFO - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —à–∞–≥–∏ Scrapy, WARNING - —Ç–∏—à–µ
    settings.set('LOG_LEVEL', 'INFO')
    settings.set('LOG_FORMAT', '%(asctime)s [%(name)s] %(levelname)s: %(message)s')
    settings.set('LOG_DATEFORMAT', '%Y-%m-%d %H:%M:%S')

    settings.set('ROBOTSTXT_OBEY', False) # –ë—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã –∏ —ç—Ç–∏—á–Ω—ã! –°–æ–±–ª—é–¥–∞–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏.
    # –í–∫–ª—é—á–∞–µ–º AutoThrottle - –æ–Ω –±—É–¥–µ—Ç —É–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏!
    settings.set('AUTOTHROTTLE_ENABLED', True)
    # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è AutoThrottle (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π)
    settings.set('DOWNLOAD_DELAY', 1.0) # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—É—Ç—å –±–æ–ª—å—à–µ
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å AutoThrottle
    settings.set('AUTOTHROTTLE_MAX_DELAY', 15.0) # –ù–µ –∂–¥–∞—Ç—å —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∫ –∫–æ—Ç–æ—Ä–æ–º—É —Å—Ç—Ä–µ–º–∏—Ç—Å—è AutoThrottle (–±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ)
    settings.set('AUTOTHROTTLE_TARGET_CONCURRENCY', 1.0) # –°—Ç–∞—Ä–∞—Ç—å—Å—è –¥–µ–ª–∞—Ç—å ~1 –∑–∞–ø—Ä–æ—Å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–æ–≥–∏ AutoThrottle –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    settings.set('AUTOTHROTTLE_DEBUG', False) # –ü–æ—Å—Ç–∞–≤—å—Ç–µ True –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É (–≤–∞–∂–Ω–æ –¥–ª—è –≤–µ–∂–ª–∏–≤–æ—Å—Ç–∏)
    settings.set('CONCURRENT_REQUESTS_PER_DOMAIN', 1) # –ù–µ –±–æ–ª–µ–µ 1 –∑–∞–ø—Ä–æ—Å–∞ –∫ –æ–¥–Ω–æ–º—É —Å–∞–π—Ç—É –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (AutoThrottle –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)
    settings.set('CONCURRENT_REQUESTS', 8) # –£–º–µ–Ω—å—à–∏–º –æ–±—â–µ–µ —á–∏—Å–ª–æ

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ø–∞—É–∫ –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å)
    settings.set('USER_AGENT', random.choice(USER_AGENTS))
    settings.set('DOWNLOAD_TIMEOUT', 35) # –ß—É—Ç—å –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
    settings.set('DNS_TIMEOUT', 25)      # –ß—É—Ç—å –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ DNS

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
    settings.set('REDIRECT_ENABLED', True)
    settings.set('RETRY_ENABLED', True) # –í–∫–ª—é—á–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ (—Å–µ—Ç—å, 5xx)
    settings.set('RETRY_TIMES', 2)      # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å 2 —Ä–∞–∑–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ (–≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ)
    settings.set('RETRY_HTTP_CODES', [500, 502, 503, 504, 522, 524, 408, 429]) # –ö–æ–¥—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞

    # –û—Ç–∫–ª—é—á–∞–µ–º –∫—É–∫–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    settings.set('COOKIES_ENABLED', False)

    # --- –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ ---
    process = CrawlerProcess(settings)
    crawler = process.create_crawler(EnhancedArticleSpider)

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã
    crawler.signals.connect(item_scraped_handler, signal=signals.item_scraped)
    crawler.signals.connect(spider_closed_handler, signal=signals.spider_closed)


    logger.info("--- Starting CrawlerProcess ---")
    # –ü–µ—Ä–µ–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ __init__ –ø–∞—É–∫–∞
    process.crawl(crawler, search_tasks=search_tasks, results_per_query=results_per_query)

    try:
        process.start() # –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤, –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ä–µ–∞–∫—Ç–æ—Ä Twisted
        logger.info("--- CrawlerProcess finished successfully ---")
    except Exception as e:
        logger.error(f"--- CrawlerProcess encountered an error: {e} ---", exc_info=True)
    # –†–µ–∞–∫—Ç–æ—Ä –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ª–∏–±–æ —Å–∞–º –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã, –ª–∏–±–æ –ø–æ –æ—à–∏–±–∫–µ

    end_time = time.time()
    logger.info(f"\n=== Scrape Run Complete ===")
    logger.info(f"Total execution time: {end_time - start_time:.2f} seconds.")
    logger.info(f"Collected {len(scraped_items)} items (successfully parsed sources).")
    if final_failed_searches:
         logger.warning(f"Found {len(final_failed_searches)} tasks where no URLs could be found initially.")
         # logger.debug(f"Failed tasks details: {final_failed_searches}")


    return scraped_items, final_failed_searches


# --- –ü—Ä–∏–º–µ—Ä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ---
if __name__ == '__main__':
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    test_tasks = [
        {
            'query': "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ NLP",
            'plan_item': "–û–±–∑–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
            'plan_item_id': "plan_0",
            'query_id': "q_0_0"
        },
        {
            'query': "BERT model architecture explained", # –ü—Ä–∏–º–µ—Ä –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            'plan_item': "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BERT",
            'plan_item_id': "plan_1",
            'query_id': "q_1_0"
        },
        {
            'query': "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
            'plan_item': "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
            'plan_item_id': "plan_2",
            'query_id': "q_2_0"
        },
        {
            'query': "–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —á–µ–ø—É—Ö–∞ –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞", # –ó–∞–ø—Ä–æ—Å –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            'plan_item': "–¢–µ—Å—Ç –æ—à–∏–±–∫–∏",
            'plan_item_id': "plan_3",
            'query_id': "q_3_0"
        }
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∑–∞–¥–∞—á
    ]
    num_sites_to_parse_per_query = 2 # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º 2 —Å–∞–π—Ç–∞ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å

    # # –ü–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –∑–∞–ø—É—Å–∫–æ–º newspaper —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ nltk.download('punkt') –≤—ã–ø–æ–ª–Ω–µ–Ω
    # try:
    #     import nltk
    #     try: nltk.data.find('tokenizers/punkt')
    #     except nltk.downloader.DownloadError:
    #         print("NLTK 'punkt' not found. Downloading...")
    #         nltk.download('punkt')
    #         print("'punkt' downloaded.")
    #     except LookupError: # –î—Ä—É–≥–æ–π —Ç–∏–ø –æ—à–∏–±–∫–∏, –µ—Å–ª–∏ —Ä–µ—Å—É—Ä—Å –Ω–µ –Ω–∞–π–¥–µ–Ω
    #         print("NLTK 'punkt' not found (LookupError). Downloading...")
    #         nltk.download('punkt')
    #         print("'punkt' downloaded.")
    # except ImportError:
    #     print("nltk library not found. Newspaper3k might need it. Please install: pip install nltk")
    # except Exception as e:
    #     print(f"Could not check/download nltk punkt: {e}")

    print("\n--- Starting Test Scrape ---")
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ –∏ –ø–æ–ª—É—á–∞–µ–º –æ–±–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    scraped_results, failed_search_tasks = run_enhanced_scrape(
        search_tasks=test_tasks,
        results_per_query=num_sites_to_parse_per_query
    )

    # --- –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    print(f"\n--- Scraping Finished ---")

    if scraped_results:
        print(f"\n--- Scraped Content Summary ({len(scraped_results)} items) ---")
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–π –∑–∞–¥–∞—á–µ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        results_by_task = {}
        for item in scraped_results:
            task_key = (item.get('plan_item_id'), item.get('query_id'))
            if task_key not in results_by_task:
                 results_by_task[task_key] = {'query': item.get('query'), 'plan_item': item.get('plan_item'), 'items': []}
            results_by_task[task_key]['items'].append(item)

        task_counter = 0
        for task_key, task_data in results_by_task.items():
             task_counter += 1
             print(f"\n--- Task {task_counter} (ID: {task_key}) ---")
             print(f"  Query: {task_data['query']}")
             print(f"  Plan Item: {task_data['plan_item']}")
             print(f"  Scraped Items ({len(task_data['items'])}):")
             for i, item in enumerate(task_data['items']):
                  print(f"\n  Item {i+1}:")
                  print(f"    URL: {item.get('url')}")
                  print(f"    Title: {item.get('title', 'N/A')}")
                  print(f"    Method: {item.get('extraction_method')}")
                  text_preview = item.get('text', '')[:250].replace('\n', ' ') # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
                  print(f"    Text Preview ({len(item.get('text', ''))} chars): {text_preview}...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª
        try:
            with open("scraped_content_enhanced.json", "w", encoding="utf-8") as f:
                import json
                json.dump(scraped_results, f, ensure_ascii=False, indent=2)
            print("\nFull successful results saved to scraped_content_enhanced.json")
        except Exception as e:
            print(f"\nFailed to save successful results to JSON: {e}")
    else:
        print("\n--- No content was successfully scraped ---")

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–∞—Ö, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –Ω–∞—à–ª–∏ URL
    if failed_search_tasks:
        print(f"\n--- Tasks With No URLs Found ({len(failed_search_tasks)}) ---")
        for i, task in enumerate(failed_search_tasks):
             print(f"  Task {i+1}:")
             print(f"    Query: {task.get('query')}")
             print(f"    Plan Item: {task.get('plan_item', 'N/A')}")
             print(f"    Plan Item ID: {task.get('plan_item_id', 'N/A')}")
             print(f"    Query ID: {task.get('query_id', 'N/A')}")

        # –ú–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏ —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫
        try:
            with open("failed_search_tasks.json", "w", encoding="utf-8") as f:
                import json
                json.dump(failed_search_tasks, f, ensure_ascii=False, indent=2)
            print("\nList of tasks with no URLs found saved to failed_search_tasks.json")
        except Exception as e:
            print(f"\nFailed to save failed tasks list to JSON: {e}")
    else:
        print("\n--- All tasks had at least one URL found during the search phase ---")

    print("\n--- End of Script ---")