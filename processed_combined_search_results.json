[
  {
    "query": "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ NLP",
    "plan_item": "–û–±–∑–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
    "plan_item_id": "plan_0",
    "query_id": "q_0_0",
    "source": "duckduckgo",
    "url": "https://huggingface.co/learn/nlp-course/ru/chapter1/4",
    "title": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã? - Hugging Face NLP Course",
    "text": "NLP Course documentation –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã? and get access to the augmented documentation experience –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã? –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –≤ –æ–±—â–∏—Ö —á–µ—Ä—Ç–∞—Ö –Ω–∞ —Ç–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã. –ó–¥–µ—Å—å –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ (–∫–æ—Ä–æ—Ç–∫—É—é) –∏—Å—Ç–æ—Ä–∏—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±—ã–ª–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –≤ –∏—é–Ω–µ 2017. –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞. –≠—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ–≤–ª–µ–∫–ª–∞ –∑–∞ —Å–æ–±–æ–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ò—é–Ω—å 2018 : GPT , –ø–µ—Ä–≤–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è (fine-tuning), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–Ω–æ–≥–∏—Ö NLP-–∑–∞–¥–∞—á. –ò—é–Ω—å 2018 : GPT , –ø–µ—Ä–≤–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è (fine-tuning), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–Ω–æ–≥–∏—Ö NLP-–∑–∞–¥–∞—á. –û–∫—Ç—è–±—Ä—å 2018 : BERT , –¥—Ä—É–≥–∞—è –±–æ–ª—å—à–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–±–æ–ª—å—à–µ –º—ã —É–∑–Ω–∞–µ–º –æ–± —ç—Ç–æ–º –≤ —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ!) –û–∫—Ç—è–±—Ä—å 2018 : BERT , –¥—Ä—É–≥–∞—è –±–æ–ª—å—à–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–±–æ–ª—å—à–µ –º—ã —É–∑–Ω–∞–µ–º –æ–± —ç—Ç–æ–º –≤ —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ!) –§–µ–≤—Ä–∞–ª—å 2019 : GPT-2 , —É–ª—É—á—à–µ–Ω–Ω–∞—è (–∏ –±–æ–ª–µ–µ –æ–±—ä–µ–º–Ω–∞—è) –≤–µ—Ä—Å–∏—è GPT, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –±—ã–ª–∞ —Å—Ä–∞–∑—É –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –ø–æ —ç—Ç–∏—á–µ—Å–∫–∏–º —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –§–µ–≤—Ä–∞–ª—å 2019 : GPT-2 , —É–ª—É—á—à–µ–Ω–Ω–∞—è (–∏ –±–æ–ª–µ–µ –æ–±—ä–µ–º–Ω–∞—è) –≤–µ—Ä—Å–∏—è GPT, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –±—ã–ª–∞ —Å—Ä–∞–∑—É –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –ø–æ —ç—Ç–∏—á–µ—Å–∫–∏–º —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –û–∫—Ç—è–±—Ä—å 2019 : DistilBERT , ¬´–¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è¬ª –≤–µ—Ä—Å–∏—è BERT, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ 60% –±—ã—Å—Ç—Ä–µ–µ –∏ –Ω–∞ 40% –º–µ–Ω–µ–µ –æ–±—ä–µ–º–Ω–∞—è, –æ–¥–Ω–∞–∫–æ —Å–æ—Ö—Ä–∞–Ω—è—é—â–∞—è 97% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ BERT –û–∫—Ç—è–±—Ä—å 2019 : DistilBERT , ¬´–¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è¬ª –≤–µ—Ä—Å–∏—è BERT, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ 60% –±—ã—Å—Ç—Ä–µ–µ –∏ –Ω–∞ 40% –º–µ–Ω–µ–µ –æ–±—ä–µ–º–Ω–∞—è, –æ–¥–Ω–∞–∫–æ —Å–æ—Ö—Ä–∞–Ω—è—é—â–∞—è 97% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ BERT –û–∫—Ç—è–±—Ä—å 2019 : BART and T5 , –¥–≤–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–∏, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –û–∫—Ç—è–±—Ä—å 2019 : BART and T5 , –¥–≤–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–∏, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ú–∞–π 2020 , GPT-3 , –µ—â–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-2, —Å–ø–æ—Å–æ–±–Ω–∞—è —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ fine-tuning (—Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ zero-shot learning ) –ú–∞–π 2020 , GPT-3 , –µ—â–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-2, —Å–ø–æ—Å–æ–±–Ω–∞—è —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ fine-tuning (—Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ zero-shot learning ) –≠—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –¥–∞–ª–µ–∫–æ –Ω–µ –ø–æ–ª–Ω—ã–π, –æ–Ω –≤—Å–µ–≥–æ –ª–∏—à—å –ø—Ä–∏–∑–≤–∞–Ω –≤—ã–¥–µ–ª–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –í —à–∏—Ä–æ–∫–æ–º —Å–º—ã—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ —Ç—Ä–∏ —Ç–∏–ø–∞: GPT-–ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Ç–∞–∫–∂–µ —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã) BERT-–ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Ç–∞–∫–∂–µ —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä—É—é—â–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã ( auto-encoding )) —Ç–∏–ø BART/T5 –º–æ–¥–µ–ª–∏ (—Ç–∞–∫–∂–µ —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ( sequence-to-sequence, seq2seq )) –ú—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —ç—Ç–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ –ø–æ–∑–∂–µ. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã - —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –í—Å–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤—ã—à–µ (GPT, BERT, BART, T5, etc.) –æ–±—É—á–µ–Ω—ã –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–∞–Ω–≥–ª. language models) . –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –æ–Ω–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫—É —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–∞–Ω–≥–ª. self-supervised learning). –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ —Ç–∞–∫–æ–π —Å–ø–æ—Å–æ–± –æ–±—É—á–µ–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ª—é–¥–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã —Ä–∞–∑–º–µ—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ! –¢–∞–∫–æ–π —Ç–∏–ø –º–æ–¥–µ–ª–µ–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –±—ã–ª –æ–±—É—á–µ–Ω, –Ω–æ –æ–Ω –Ω–µ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –±–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ—Ç–æ–º –ø–æ–¥–≤–µ—Ä–≥–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–¥—É—Ä–µ, –Ω–∞–∑—ã–≤–∞–µ–º–æ–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º (–∞–Ω–≥–ª. transfer learning) . –í —Ö–æ–¥–µ —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, —Ç.–µ. —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ —á–µ–ª–æ–≤–µ–∫–æ–º –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –º–æ–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ n –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ–≤. –≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞—É–∑–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º (–∞–Ω–≥–ª. causal language modeling) , –ø–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–æ—à–ª—ã—Ö –∏ —Ç–µ–∫—É—â–∏—Ö —Å–ª–æ–≤, –Ω–æ –Ω–µ –æ—Ç –±—É–¥—É—â–∏—Ö. –î—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–∞–Ω–≥–ª. masked language modeling) , –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã - –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ó–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, DistilBERT), –æ–±—â–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –±–æ–ª—å—à–æ–π, —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –≠—Ç–æ –≤–æ–∑–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–∞–∂–µ –Ω–∞ –æ–∫—Ä—É–∂–∞—é—â—É—é —Å—Ä–µ–¥—É, —á—Ç–æ –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –≥—Ä–∞—Ñ–∏–∫–µ –Ω–∏–∂–µ. –ò —ç—Ç–æ –Ω–∞–≥–ª—è–¥–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ (–æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π) –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç –∫–æ–º–∞–Ω–¥–∞, —Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ –ø—ã—Ç–∞—é—â–∞—è—Å—è —É–º–µ–Ω—å—à–∏—Ç—å –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–∫—Ä—É–∂–∞—é—â—É—é —Å—Ä–µ–¥—É. –£–≥–ª–µ—Ä–æ–¥–Ω—ã–π —Å–ª–µ–¥ –æ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ª—É—á—à–∏—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—É–¥–µ—Ç –µ—â–µ –≤—ã—à–µ. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–µ–±–µ, —á—Ç–æ –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –≥—Ä—É–ø–ø–∞, —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∏–ª–∏ –∫–æ–º–ø–∞–Ω–∏—è —Ö–æ—Ç–µ–ª–∏ –±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –æ–Ω–∏ –¥–µ–ª–∞–ª–∏ –±—ã —ç—Ç–æ —Å –Ω—É–ª—è. –≠—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –±—ã –∫ –æ–≥—Ä–æ–º–Ω—ã–º, –Ω–µ–Ω—É–∂–Ω—ã–º –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∑–∞—Ç—Ä–∞—Ç–∞–º! –í–æ—Ç –ø–æ—á–µ–º—É —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–º–µ–µ—Ç –ø–µ—Ä–≤–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Å–Ω–∏–∂–∞–µ—Ç –æ–±—â—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ —É–≥–ª–µ—Ä–æ–¥–Ω—ã–π —Å–ª–µ–¥ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –ö—Å—Ç–∞—Ç–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ—Ä–∏—Ç—å —É–≥–ª–µ—Ä–æ–¥–Ω—ã–π —Å–ª–µ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ—Å—Ç–∞–≤—è—Ç –≤–∞—à–∏ –º–æ–¥–µ–ª–∏, –ø—Ä–∏ –ø–æ–º–æ—â–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ ü§ó Transformers ML CO2 Impact –∏–ª–∏ Code Carbon . –ß—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ —ç—Ç–æ–º, –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–≥-–ø–æ—Å—Ç , –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª emissions.csv , —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—Ä–æ–≥–Ω–æ–∑ –æ–±—ä–µ–º–æ–≤ –≤—ã–±—Ä–æ—Å–∞ —É–≥–ª–µ—Ä–æ–¥–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é ü§ó Transformers, –≤ –∫–æ—Ç–æ—Ä–æ–π –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç—Å—è —ç—Ç–∞ —Ç–µ–º–∞. –¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ (–∞–Ω–≥–ª. pretraining) - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è: –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è, –ø–æ—Å–ª–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–∞–º –ø—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–¥–µ–ª—å. –î–æ–æ–±—É—á–µ–Ω–∏–µ (–∞–Ω–≥–ª. fine-tuning) , —Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, —ç—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞. –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –≤—ã —Å–Ω–∞—á–∞–ª–∞ –¥–æ–ª–∂–Ω—ã –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –∞ –ø–æ—Å–ª–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –µ–µ –æ–±—É—á–µ–Ω–∏–µ –µ–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏. –°—Ç–æ–π—Ç–µ ‚Äî –ø–æ—á–µ–º—É –Ω–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å—Ä–∞–∑—É –∂–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏? –≠—Ç–æ–º—É –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏—á–∏–Ω: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –º–æ–¥–µ–ª—å—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–∞—Ö NLP –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è—Ö —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –≤ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ). –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –º–æ–¥–µ–ª—å—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–∞—Ö NLP –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è—Ö —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –≤ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ). –¢–∞–∫ –∫–∞–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ ‚Äú–≤–∏–¥–µ–ª–∞‚Äù –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫ –∫–∞–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ ‚Äú–≤–∏–¥–µ–ª–∞‚Äù –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–æ —ç—Ç–æ–π –∂–µ –ø—Ä–∏—á–∏–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏ –Ω–∞–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–æ —ç—Ç–æ–π –∂–µ –ø—Ä–∏—á–∏–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏ –Ω–∞–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –º–æ–¥–µ–ª—å, –∞ –∑–∞—Ç–µ–º –¥–æ–æ–±—É—á–∏—Ç—å –µ–µ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ arXiv, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–≥–æ –ø–æ–ª—É—á–∏—Ç—Å—è –Ω–∞—É—á–Ω–æ-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –º–æ–¥–µ–ª—å. –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏—à—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: –∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–æ–±—Ä–µ–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, ¬´–ø–µ—Ä–µ–¥–∞—é—Ç—Å—è¬ª (–æ—Å—É—â–µ—Å—Ç–≤–ª—è—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä), –æ—Ç—Å—é–¥–∞ –∏ —Ç–µ—Ä–º–∏–Ω ¬´—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ¬ª. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –¥–∞–Ω–Ω—ã—Ö, —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏ —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç. –¢–∞–∫–∂–µ –±—ã—Å—Ç—Ä–µ–µ –∏ –ø—Ä–æ—â–µ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ö–µ–º—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–æ —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ —É—Å–∏–ª–∏–π, —á–µ–º –ø–æ–ª–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ç–∞–∫–∂–µ –¥–∞—Å—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è (–µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ —É –≤–∞—Å –Ω–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö), –ø–æ—ç—Ç–æ–º—É –≤—ã –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–Ω—ã –ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å ‚Äî –º–æ–¥–µ–ª—å, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—É—é –∫ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ, –∞ –ø–æ—Ç–æ–º –¥–æ–æ–±—É—á–∏—Ç—å –µ–µ. –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–±—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ù–µ –±–µ—Å–ø–æ–∫–æ–π—Ç–µ—Å—å, –µ—Å–ª–∏ –≤—ã –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–Ω—è—Ç–∏–π; –¥–∞–ª–µ–µ –µ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã, –ø–æ—Å–≤—è—â–µ–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–º—É –∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –±–ª–æ–∫–æ–≤: –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ (—Å–ª–µ–≤–∞) (–∞–Ω–≥–ª. encoder): –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç—Ä–æ–∏—Ç –∏—Ö —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é (—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏). –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, –º–æ–¥–µ–ª—å –Ω–∞—Ü–µ–ª–µ–Ω–∞ –Ω–∞ ‚Äú–ø–æ–Ω–∏–º–∞–Ω–∏–µ‚Äù –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ (—Å–ø—Ä–∞–≤–∞) (–∞–Ω–≥–ª. decoder): –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ (–ø—Ä–∏–∑–Ω–∞–∫–∏) –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ —Å –¥—Ä—É–≥–∏–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω—É–∂–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞—Ü–µ–ª–µ–Ω–∞ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ö–∞–∂–¥–∞—è –∏–∑ —ç—Ç–∏—Ö —á–∞—Å—Ç–µ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ, —ç—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏: –ú–æ–¥–µ–ª–∏-–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ : –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª–∏-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ : –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞. –ú–æ–¥–µ–ª–∏ —Ç–∏–ø–∞ ‚Äú–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫‚Äù –∏–ª–∏ seq2seq-–º–æ–¥–µ–ª–∏ : –ø–æ–ª–µ–∑–Ω—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä: –ø–µ—Ä–µ–≤–æ–¥ –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞. –ú—ã –∏–∑—É—á–∏–º —ç—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö. –ö–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ—è, –Ω–∞–∑—ã–≤–∞–µ–º–æ–≥–æ —Å–ª–æ–µ–º –≤–Ω–∏–º–∞–Ω–∏—è (–∞–Ω–≥–ª. attention layer) . –°—Ç–∞—Ç—å—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª–∞ –≤–ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è ‚ÄúAttention Is All You Need‚Äù (‚Äú–í–Ω–∏–º–∞–Ω–∏–µ - –≤—Å–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ‚Äù)! –ú—ã –∏–∑—É—á–∏–º –¥–µ—Ç–∞–ª–∏ —ç—Ç–æ–≥–æ —Å–ª–æ—è –ø–æ–∑–∂–µ. –ù–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç –º—ã —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º –º–µ—Ö–∞–Ω–∏–∑–º –µ–≥–æ —Ä–∞–±–æ—Ç—ã —Ç–∞–∫: —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ ‚Äú–æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ‚Äù –Ω–∞ –æ–¥–Ω–∏ —Å–ª–æ–≤–∞ –≤ –ø–æ–¥–∞–Ω–Ω–æ–º –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –∞ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–æ–π –∏–ª–∏ –∏–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å. –ò —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞. –ß—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏—Ç—å —ç—Ç–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π —è–∑—ã–∫. –î–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚ÄúYou like this course‚Äù, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—É–¥–µ—Ç —Ç–∞–∫–∂–µ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å–æ—Å–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ ‚ÄúYou‚Äù, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å–ª–æ–≤–∞ ‚Äúlike‚Äù, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ –≥–ª–∞–≥–æ–ª ‚Äúlike‚Äù —Å–ø—Ä—è–≥–∞–µ—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–¥–ª–µ–∂–∞—â–µ–≥–æ. –û–¥–Ω–∞–∫–æ –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞. –í —Ç–æ–º –∂–µ –¥—É—Ö–µ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ ‚Äúlike‚Äù —Ç–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—É–¥–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–æ ‚Äúcourse‚Äù, –ø–æ—Ç–æ–º—É —á—Ç–æ ‚Äúthis‚Äù –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, —Å—Ç–æ–∏—Ç –ª–∏ –∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –≤ –º—É–∂—Å–∫–æ–º –∏–ª–∏ –∂–µ–Ω—Å–∫–æ–º —Ä–æ–¥–µ. –û–ø—è—Ç—å –∂–µ, –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ ‚Äúthis‚Äù. –° –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ (–∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏) –º–æ–¥–µ–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è —É–¥–µ–ª—è—Ç—å –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–ª–æ–≤–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ. –¢–∞–∫–∞—è –∂–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∫ –ª—é–±–æ–π –∑–∞–¥–∞—á–µ, —Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞: —Å–ª–æ–≤–æ —Å–∞–º–æ –ø–æ —Å–µ–±–µ –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –æ–¥–Ω–∞–∫–æ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—á–µ–Ω—å —á–∞—Å—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–µ—Ç —è–≤–ª—è—Ç—å—Å—è —Å–ª–æ–≤–æ (–∏–ª–∏ —Å–ª–æ–≤–∞), —Å—Ç–æ—è—â–∏–µ –≤–æ–∫—Ä—É–≥ –∏—Å–∫–æ–º–æ–≥–æ —Å–ª–æ–≤–∞. –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –≤—ã –∑–Ω–∞–∫–æ–º—ã —Å –∏–¥–µ–µ–π –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ü–µ–ª–æ–º, –ø–æ—Å–º–æ—Ç—Ä–∏–º –ø–æ–±–ª–∏–∂–µ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –≤—Å–µ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ, –∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç —Ç–µ –∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∂–µ–ª–∞–µ–º–æ–º —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ. –í –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ —Å–ª–æ–∏ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ (–ø–æ—Å–∫–æ–ª—å–∫—É, –∫–∞–∫ –º—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –≤–∏–¥–µ–ª–∏, –ø–µ—Ä–µ–≤–æ–¥ –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –º–æ–∂–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ—Å–ª–µ –∏ –ø–µ—Ä–µ–¥ –Ω–∏–º). –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –º–æ–∂–µ—Ç –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω —É–∂–µ –ø–µ—Ä–µ–≤–µ–ª (—Ç–æ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å–ª–æ–≤–æ–º). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–π —Ü–µ–ª–∏, –º—ã –ø–µ—Ä–µ–¥–∞–µ–º –∏—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫—É, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞, —á—Ç–æ–±—ã –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —á–µ—Ç–≤–µ—Ä—Ç–æ–µ —Å–ª–æ–≤–æ. –ß—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Ü–µ–ª–µ–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º), –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–æ –µ–º—É –Ω–µ —Ä–∞–∑—Ä–µ—à–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—É–¥—É—â–∏–µ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –æ–Ω –∏–º–µ–ª –¥–æ—Å—Ç—É–ø –∫ —Å–ª–æ–≤—É –≤ –ø–æ–∑–∏—Ü–∏–∏ 2 –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–æ–≤–æ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ 2, –∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç —Å–ª–æ–∂–Ω–æ–π!). –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —á–µ—Ç–≤–µ—Ä—Ç–æ–µ —Å–ª–æ–≤–æ —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –±—É–¥–µ—Ç –∏–º–µ—Ç—å –¥–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –∫ —Å–ª–æ–≤–∞–º –≤ –ø–æ–∑–∏—Ü–∏—è—Ö —Å 1 –ø–æ 3. –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer –≤—ã–≥–ª—è–¥–µ–ª–∞ —Ç–∞–∫: –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Å–ª–µ–≤–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Å–ø—Ä–∞–≤–∞: –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–æ–∫–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ (–ø—Ä–æ—à–ª—ã–µ) –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞, –∞ –≤—Ç–æ—Ä–æ–π —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–Ω –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–º—É –≤—Ö–æ–¥–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é, —á—Ç–æ–±—ã –Ω–∞–∏–ª—É—á—à–∏–º –æ–±—Ä–∞–∑–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ. –≠—Ç–æ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –∏–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –¥–∞–ª–µ–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞. –ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–∏–ª—É—á—à–µ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞. –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–∞–Ω–≥–ª. attention mask) —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ/–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –æ–±—Ä–∞—â–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–ª–æ–≤–æ-–∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å (–∞–Ω–≥–ª. padding), –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–ª—è –ø—Ä–∏–¥–∞–Ω–∏—è –≤—Å–µ–º –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –ü–æ –º–µ—Ä–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, –≤—ã –±—É–¥–µ—Ç–µ –≤—Å—Ç—Ä–µ—á–∞—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã (–∞–Ω–≥–ª. checkpoints) –≤ —Å–º—ã—Å–ª–µ –º–æ–¥–µ–ª–∏ . –≠—Ç–∏ —Ç–µ—Ä–º–∏–Ω—ã –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–π —Å–º—ã—Å–ª: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - —Å–∫–µ–ª–µ—Ç –º–æ–¥–µ–ª–∏ ‚Äî —Å–ª–æ–∏, —Å–≤—è–∑–∏ –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –º–æ–¥–µ–ª–∏. –ß–µ–∫–ø–æ–∏–Ω—Ç - –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ú–æ–¥–µ–ª—å - –∑–æ–Ω—Ç–∏—á–Ω—ã–π —Ç–µ—Ä–º–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∏ –≤–µ—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –í —ç—Ç–æ–º –∫—É—Ä—Å–µ –º—ã –±—É–¥–µ–º —Ç–æ—á–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —á–µ–∫–ø–æ–∏–Ω—Ç , –µ—Å–ª–∏ —ç—Ç–æ –±—É–¥–µ—Ç –≤–∞–∂–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, BERT - —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∞ bert-base-cased - –Ω–∞–±–æ—Ä –≤–µ—Å–æ–≤, –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π Google –∫ –ø–µ—Ä–≤–æ–º—É –≤—ã–ø—É—Å–∫—É BERT‚Äô–∞, - —ç—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç. –û–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –∏ ‚Äú–º–æ–¥–µ–ª—å BERT‚Äù, –∏ ‚Äú–º–æ–¥–µ–ª—å bert-base-cased‚Äù.",
    "extraction_method": "requests+bs4",
    "content_length": 14556,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ NLP",
    "plan_item": "–û–±–∑–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
    "plan_item_id": "plan_0",
    "query_id": "q_0_0",
    "source": "duckduckgo",
    "url": "https://aiforprofi.ru/iskusstvennyj-intellekt-v-sovremennom-mire/neuroseti/transformery-arhitektura-izmenivshaya-mir-nlp",
    "title": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –∫–∞–∫ –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç?",
    "text": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–µ–ª–∏ –Ω–∞—Å—Ç–æ—è—â—É—é —Ä–µ–≤–æ–ª—é—Ü–∏—é –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –ü–æ—è–≤–ª–µ–Ω–∏–µ —ç—Ç–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤ 2017 –≥–æ–¥—É –æ–∑–Ω–∞–º–µ–Ω–æ–≤–∞–ª–æ –ø–µ—Ä–µ–ª–æ–º–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: –æ–Ω–∏ –æ—Ç–∫—Ä—ã–ª–∏ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–µ—á–∏ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –ª–∞–Ω–¥—à–∞—Ñ—Ç –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ß—Ç–æ —É–º–µ—é—Ç –¥–µ–ª–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç—ã —Å –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ—ç–∑–∏—é –≤–µ—Å—Ç–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –í—Å—ë —ç—Ç–æ —Å—Ç–∞–ª–æ –≤–æ–∑–º–æ–∂–Ω—ã–º –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º. –ù–æ —á—Ç–æ –∂–µ –¥–µ–ª–∞–µ—Ç —ç—Ç—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å—Ç–æ–ª—å –æ—Å–æ–±–µ–Ω–Ω–æ–π? –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã ‚Äì —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –æ–±–ª–∞–¥–∞—é—â–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, —É–¥–µ–ª—è—è –≤–Ω–∏–º–∞–Ω–∏–µ –≤—Å–µ–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤, —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (RNN), —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø–æ—Ä—è–¥–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º —É–ª–∞–≤–ª–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏. –ö–ª—é—á –∫ —É—Å–ø–µ—Ö—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∫—Ä–æ–µ—Ç—Å—è –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è (attention mechanism). –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —á–∏—Ç–∞–µ—Ç–µ –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –í–∞—à –º–æ–∑–≥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –Ω–∏–º–∏. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç –ø–æ —Å—Ö–æ–∂–µ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É, –Ω–æ –¥–µ–ª–∞—é—Ç —ç—Ç–æ —Å –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –¥–ª—è –æ–≥—Ä–æ–º–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±—ã—Å—Ç—Ä–æ –∑–∞–≤–æ–µ–≤–∞–ª–∞ –º–∏—Ä NLP. –ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ BERT –æ—Ç Google –∏ GPT –æ—Ç OpenAI, –ø—Ä–æ–∏–∑–≤–µ–ª–∏ –Ω–∞—Å—Ç–æ—è—â–∏–π —Ñ—É—Ä–æ—Ä, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Ä–µ–∫–æ—Ä–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –û—Ç –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã ‚Äì —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å. –û–¥–Ω–∞–∫–æ –≤–ª–∏—è–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤—ã—Ö–æ–¥–∏—Ç –¥–∞–ª–µ–∫–æ –∑–∞ —Ä–∞–º–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π. –≠—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –º–µ–Ω—è–µ—Ç –Ω–∞—à–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —É–º–Ω–µ–µ, –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã ‚Äì —Ç–æ—á–Ω–µ–µ, –∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º ‚Äì –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º–∏. –ú—ã —Å—Ç–æ–∏–º –Ω–∞ –ø–æ—Ä–æ–≥–µ –Ω–æ–≤–æ–π —ç—Ä—ã, –≥–¥–µ —è–∑—ã–∫ –±–æ–ª—å—à–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±–∞—Ä—å–µ—Ä–æ–º –¥–ª—è –æ–±—â–µ–Ω–∏—è –∏ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è, –∫–∞–∫ –∏ –ª—é–±–∞—è —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å—Ç–∞–≤—è—Ç –ø–µ—Ä–µ–¥ –æ–±—â–µ—Å—Ç–≤–æ–º –≤–∞–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—Å—è —Ä–æ–ª—å —á–µ–ª–æ–≤–µ–∫–∞ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞? –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —ç—Ç–∏—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç—Ç–∏—Ö –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π? –≠—Ç–∏ —Ç–µ–º—ã –º—ã –ø–æ–¥—Ä–æ–±–Ω–µ–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö –Ω–∞—à–µ–π —Å—Ç–∞—Ç—å–∏. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –Ω–æ–≤—É—é –≥–ª–∞–≤—É –≤ –∏—Å—Ç–æ—Ä–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –û–Ω–∏ –Ω–µ –ø—Ä–æ—Å—Ç–æ —É–ª—É—á—à–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞ ‚Äì –æ–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ —Å–∞–º–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –Ω–∞ —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω—ã –º–∞—à–∏–Ω—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –í —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö –º—ã –≥–ª—É–±–∂–µ –ø–æ–≥—Ä—É–∑–∏–º—Å—è –≤ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã —ç—Ç–æ–π —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –Ω–∞—à–µ–π –∂–∏–∑–Ω–∏. 1 –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ 2 –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ 3 –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP 4 –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ 5 –í–ª–∏—è–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –ò–ò –∏ –æ–±—â–µ—Å—Ç–≤–∞ 6 –ë—É–¥—É—â–µ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ NLP –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ß—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–≥–ª—è–Ω—É—Ç—å –ø–æ–¥ ¬´–∫–∞–ø–æ—Ç¬ª —ç—Ç–æ–π —Å–ª–æ–∂–Ω–æ–π, –Ω–æ —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –í –µ—ë –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∞—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞—é—Ç –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–º. –°–µ—Ä–¥—Ü–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ ‚Äì —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. (self-attention). –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –º–æ–∂–µ—Ç ¬´–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å¬ª –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–∏ –≤–∞–∂–Ω—ã –¥–ª—è –µ–≥–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏. –ò–º–µ–Ω–Ω–æ —ç—Ç–æ –∏ –¥–µ–ª–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ —É–ª–æ–≤–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–∞–∂–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö. –î—Ä—É–≥–æ–π –≤–∞–∂–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äì –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ. –ö–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ø–æ–Ω–∏–º–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, –µ—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ? –û—Ç–≤–µ—Ç –∫—Ä–æ–µ—Ç—Å—è –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (multi-head attention). –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—É –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ–¥ —Ä–∞–∑–Ω—ã–º–∏ —É–≥–ª–∞–º–∏. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —á–∏—Ç–∞–µ—Ç–µ —Ç–µ–∫—Å—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç–µ –µ–≥–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏, —Å–º—ã—Å–ª–∞ –∏ —Å—Ç–∏–ª—è ‚Äì –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É –ø–æ–¥—Ö–æ–¥—É, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —É–ª–æ–≤–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω—é–∞–Ω—Å—ã –∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã —É—Å–∫–æ–ª—å–∑–Ω—É—Ç—å –ø—Ä–∏ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –∞–Ω–∞–ª–∏–∑–µ. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–º–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º–∏ –∏–ª–∏ –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –≥–¥–µ –≤–∞–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞—Å–ø–µ–∫—Ç–æ–≤. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–≤–∏–¥–Ω–æ–π –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏. –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN), –¥–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–≤—à–∏–µ –≤ NLP, —Å—Ç—Ä–∞–¥–∞–ª–∏ –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã ¬´–∑–∞–±—ã–≤–∞–Ω–∏—è¬ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ —Ä–µ—à–∏–ª–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –û–Ω–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –æ–±—ä–µ–º–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (GPU) –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∏–ø—ã –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (TPU). –ï—â–µ –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ ‚Äì —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å. –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á NLP, –æ—Ç –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–Ω–∏—è –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö, –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π –∏–ª–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP –í–ª–∏—è–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –º–∏—Ä NLP —Ç—Ä—É–¥–Ω–æ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏—Ç—å. –û–Ω–∏ –ø—Ä–æ–∏–∑–≤–µ–ª–∏ —Ä–µ–≤–æ–ª—é—Ü–∏—é –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–æ –≤—Å–µ—Ö –æ–±–ª–∞—Å—Ç—è—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –í –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞. –ú–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ T5 (Text-to-Text Transfer Transformer), —Å–ø–æ—Å–æ–±–Ω—ã –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Å–º—ã—Å–ª, –Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –Ω—é–∞–Ω—Å—ã –æ—Ä–∏–≥–∏–Ω–∞–ª–∞. –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –Ω–∞—Å –∫ –∏–¥–µ–∞–ª—É ¬´–Ω–µ–≤–∏–¥–∏–º–æ–≥–æ¬ª –ø–µ—Ä–µ–≤–æ–¥–∞, –≥–¥–µ –±–∞—Ä—å–µ—Ä—ã –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏—Å—á–µ–∑–∞—é—Ç. –í –æ–±–ª–∞—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–Ω–∏ —É—Å–ø–µ—à–Ω–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ç–∞–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –≠—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö: –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏–∞ –¥–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –û—Å–æ–±–µ–Ω–Ω–æ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ú–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ GPT —Å–ø–æ—Å–æ–±–Ω—ã —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤—è–∑–Ω—ã–µ –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ –∑–∞–¥–∞–Ω–Ω—É—é —Ç–µ–º—É, –∏–º–∏—Ç–∏—Ä—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç–∏–ª–∏ –ø–∏—Å—å–º–∞. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤ –∏ –¥–∞–∂–µ –≤ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–º –ø–∏—Å—å–º–µ. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–æ–∏–∑–≤–µ–ª–∏ —Ä–µ–≤–æ–ª—é—Ü–∏—é –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —á–∞—Ç-–±–æ—Ç–æ–≤ –∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤. –ë–ª–∞–≥–æ–¥–∞—Ä—è –≥–ª—É–±–æ–∫–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —ç—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –º–æ–≥—É—Ç –≤–µ—Å—Ç–∏ –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ —É—Ä–æ–≤–Ω—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –æ–±—â–µ–Ω–∏—è. –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –£—Å–ø–µ—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤–æ–ø–ª–æ—Ç–∏–ª—Å—è –≤ —Ä—è–¥–µ –∑–Ω–∞–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤–Ω–µ—Å–ª–∞ —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ NLP. BERT (Bidirectional Encoder Representations from Transformers) –æ—Ç Google –ø—Ä–æ–∏–∑–≤–µ–ª–∞ —Ñ—É—Ä–æ—Ä —Å–≤–æ–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–æ–≤ –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö. –≠—Ç–∞ –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, –æ—Ç –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –¥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π. GPT (Generative Pre-trained Transformer) –æ—Ç OpenAI, –æ—Å–æ–±–µ–Ω–Ω–æ –µ–≥–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è GPT-4, –ø–æ—Ä–∞–∑–∏–ª–∞ –º–∏—Ä —Å–≤–æ–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –û—Ç –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–¥–∞ ‚Äì GPT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—É—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å, –∑–∞—Å—Ç–∞–≤–ª—è—è –∑–∞–¥—É–º–∞—Ç—å—Å—è –æ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –∏ –º–∞—à–∏–Ω–Ω—ã–º —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ–º. –ù–µ–ª—å–∑—è –Ω–µ —É–ø–æ–º—è–Ω—É—Ç—å –∏ –æ T5 (Text-to-Text Transfer Transformer) –æ—Ç Google AI, –∫–æ—Ç–æ—Ä–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º NLP, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –∏—Ö –≤—Å–µ –∫–∞–∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç. –≠—Ç–æ –æ—Ç–∫—Ä—ã–ª–æ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏ –∑–Ω–∞–Ω–∏–π. –í–ª–∏—è–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –ò–ò –∏ –æ–±—â–µ—Å—Ç–≤–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–µ –ø—Ä–æ—Å—Ç–æ —É–ª—É—á—à–∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤ NLP ‚Äì –æ–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ –Ω–∞—à–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –í–ø–µ—Ä–≤—ã–µ –º—ã —É–≤–∏–¥–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã–µ –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –æ—á–µ–≤–∏–¥–Ω–æ: –í —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ–Ω–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–±–æ—Ç. –í –±–∏–∑–Ω–µ—Å–µ ‚Äî —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É—é—Ç –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫—É. –í –Ω–∞—É–∫–µ ‚Äì –ø–æ–º–æ–≥–∞—é—Ç –≤ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑. –û–¥–Ω–∞–∫–æ, –≤–º–µ—Å—Ç–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç –∏ –≤—ã–∑–æ–≤—ã. –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –º–∏—Ä–µ, –≥–¥–µ –ò–ò –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —É–±–µ–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–µ–π–∫–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏? –ö–∞–∫ –∑–∞—â–∏—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä—Å–∫–∏–µ –ø—Ä–∞–≤–∞, –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω—ã –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã–π –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ? –≠—Ç–∏ –≤–æ–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—Ç —Å–µ—Ä—å–µ–∑–Ω–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ–±—â–µ—Å—Ç–≤–∞. –ë—É–¥—É—â–µ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ NLP –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–∞–ª–µ–∫–æ –Ω–µ –∏—Å—á–µ—Ä–ø–∞–Ω. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–¥ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π, —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ —É–ª—É—á—à–µ–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏—Ö —Ä–µ—à–µ–Ω–∏–π. –û–¥–Ω–æ –∏–∑ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π ‚Äì —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ —Å —Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–æ–º. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–æ—è–≤–ª–µ–Ω–∏—é –ò–ò-—Å–∏—Å—Ç–µ–º —Å –±–æ–ª–µ–µ —Ü–µ–ª–æ—Å—Ç–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –º–∏—Ä–∞. –î—Ä—É–≥–∞—è –≤–∞–∂–Ω–∞—è –æ–±–ª–∞—Å—Ç—å ‚Äì —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª–µ–µ —ç—Ç–∏—á–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—â—É—Ç —Å–ø–æ—Å–æ–±—ã –≤—Å—Ç—Ä–æ–∏—Ç—å –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –∏—Ö –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–µ–ª–∏ –Ω–∞—Å—Ç–æ—è—â—É—é —Ä–µ–≤–æ–ª—é—Ü–∏—é –≤ –º–∏—Ä–µ NLP, –æ—Ç–∫—Ä—ã–≤ –Ω–æ–≤—É—é —ç—Ä—É –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –û–Ω–∏ –Ω–µ –ø—Ä–æ—Å—Ç–æ —É–ª—É—á—à–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞ ‚Äì –æ–Ω–∏ –∏–∑–º–µ–Ω–∏–ª–∏ —Å–∞–º–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–∞—à–∏–Ω –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–µ—á–∏. –û—Ç –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–≤ ‚Äì —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–∞—Ö–æ–¥—è—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –º–µ–Ω—è—è —Å–ø–æ—Å–æ–±—ã –Ω–∞—à–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –û–¥–Ω–∞–∫–æ, –≤–º–µ—Å—Ç–µ —Å –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏—Ö–æ–¥–∏—Ç –∏ –±–æ–ª—å—à–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –í–∞–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –¥–∏–∞–ª–æ–≥ –æ–± –∏—Ö —ç—Ç–∏—á–Ω–æ–º –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏. –¢–æ–ª—å–∫–æ —Ç–∞–∫ –º—ã —Å–º–æ–∂–µ–º –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç—Ç–æ–π —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –Ω–æ–≤—É—é –≥–ª–∞–≤—É –≤ –∏—Å—Ç–æ—Ä–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –û—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é –Ω–∞—à–µ–π –∂–∏–∑–Ω–∏. –ò –Ω–∞–º –≤–∞–∂–Ω–æ –Ω–µ —É—Ç—Ä–∞—Ç–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∏—Ö —Ä–∞–∑–≤–∏—Ç–∏–µ–º. –ú—ã –Ω–∞—á–Ω–∞–µ–º –ø–æ–Ω–∏–º–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç—Ç–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π. –û–Ω–∏ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω—è—Ç –Ω–∞—à–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –±—É–¥—É—â–µ–µ!",
    "extraction_method": "requests+bs4",
    "content_length": 10632,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ NLP",
    "plan_item": "–û–±–∑–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
    "plan_item_id": "plan_0",
    "query_id": "q_0_0",
    "source": "duckduckgo",
    "url": "https://habr.com/ru/articles/558488/",
    "title": "–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å ...",
    "text": "–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–∏ÃÜ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–∏ÃÜ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ú–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Natural language processing, NLP) –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ , —Ç–∞–∫–∏–µ –∫–∞–∫ BERT , RoBERTa , T5 –∏–ª–∏ GPT3 , —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —è–≤–ª—è—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –ì–∏–±–∫–æ—Å—Ç—å (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å) –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞–ª–∏ –∏—Ö —à–∏—Ä–æ–∫–æ–º—É —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é, —á—Ç–æ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –ø–æ–∑–≤–æ–ª–∏–ª–æ –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–∞–∫ –≤ –∫–∞—á–µ—Å—Ç–≤–µ seq2seq –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ , —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ , –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —Ç.–¥., —Ç–∞–∫ –∏ –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ , —á–∞—Å—Ç–µ—Ä–µ—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ , –º–∞—à–∏–Ω–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ –¥—Ä. –ì–ª–∞–≤–Ω—ã–º –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ–º –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å—Ç–∞–ª –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–∞—Ä —Ç–æ–∫–µ–Ω–æ–≤ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –º–µ—Ö–∞–Ω–∏–∑–º—É –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Ç–∞–∫–∏–µ –∫–∞–∫ LSTM . –û–¥–Ω–∞–∫–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏–º–µ—é—Ç —Å–≤–æ–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –¢–∞–∫, –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –¥–ª–∏–Ω–µ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –° –Ω—ã–Ω–µ—à–Ω–∏–º–∏ –º–æ—â–Ω–æ—Å—Ç—è–º–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π —ç—Ç–æ –æ–±—ã—á–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –ø—Ä–∏–º–µ–Ω—è—Ç—å –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∫ –∑–∞–¥–∞—á–∞–º, —Ç—Ä–µ–±—É—é—â–∏–º –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ç–∞–∫–∏–º –∫–∞–∫ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã , —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–æ–º–∞ . –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –¥–≤–∞ –≤–æ–ø—Ä–æ—Å–∞: –ú–æ–∂–Ω–æ –ª–∏ –¥–æ—Å—Ç–∏—á—å —Ç–µ—Ö –∂–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ –∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ ¬´–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–µ¬ª –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –ø–∞–º—è—Ç–∏ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—Ç –ª–∏–Ω–µ–π–Ω–æ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏? –ú–æ–∂–Ω–æ –ª–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞—Ç—å —Ç–æ, —á—Ç–æ —ç—Ç–∏ ¬´–ª–∏–Ω–µ–π–Ω—ã–µ¬ª –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≥–∏–±–∫–æ—Å—Ç—å —Å–≤–æ–∏—Ö ¬´–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö¬ª —Å–æ–±—Ä–∞—Ç–æ–≤? –ù–∞ –æ–±–∞ —ç—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞ –∞–≤—Ç–æ—Ä—ã –ø–æ—Å—Ç–∞—Ä–∞–ª–∏—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —Å–≤–æ–∏—Ö –Ω–µ–¥–∞–≤–Ω–∏—Ö —Å—Ç–∞—Ç—å—è—Ö. –¢–∞–∫, –≤ ¬´ ETC: Encoding Long and Structured Inputs in Transformers ¬ª, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ EMNLP 2020 , –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (Extended Transformer Construction, ETC) ‚Äì –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –ø–∞—Ä –æ—Ü–µ–Ω–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞ (similarity score). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∫ –ª–∏–Ω–µ–π–Ω–æ–π –∏ –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ NLP –∑–∞–¥–∞—á–∞—Ö. –ó–∞—Ç–µ–º –≤ —Å—Ç–∞—Ç—å–µ ¬´ Big Bird: Transformers for Longer Sequences ¬ª, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ NeurIPS 2020 , –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì BigBird, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é ETC –¥–ª—è –±–æ–ª–µ–µ –æ–±—â–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–∏—Ö—Å—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–π –∏–ª–∏ –∏–Ω–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≥–∏–±–∫–æ—Å—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ ¬´–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ¬ª –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∑–∞–¥–∞—é—Ç –Ω–æ–≤—É—é –ø–ª–∞–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã , —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–æ–º–∞ . –í–Ω–∏–º–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –≥—Ä–∞—Ñ –ú–æ–¥—É–ª—å –≤–Ω–∏–º–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π—Å—è –≤ –º–æ–¥–µ–ª—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü–æ–ª–µ–∑–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Å–µ–±–µ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ , –≥–¥–µ –≤ —É–∑–ª–∞—Ö –≥—Ä–∞—Ñ–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è —Ç–æ–∫–µ–Ω—ã, –∞ –Ω–∞ –≥—Ä–∞–Ω—è—Ö ‚Äì –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ (similarity score). –í —ç—Ç–æ–º —Å–º—ã—Å–ª–µ –º–æ–¥–µ–ª—å –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–ª–Ω—ã–π –≥—Ä–∞—Ñ . –í –æ—Å–Ω–æ–≤–µ –ø–æ–¥—Ö–æ–¥–∞ –∞–≤—Ç–æ—Ä–æ–≤ —Å—Ç–∞—Ç—å–∏ –ª–µ–∂–∏—Ç –∏–¥–µ—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–∞–∫–∏—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω–æ–µ —á–∏—Å–ª–æ –æ—Ü–µ–Ω–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞. –ú–µ—Ö–∞–Ω–∏–∑–º –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ –ø–æ–ª–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞. Extended Transformer Construction (ETC) –î–ª—è NLP –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–ª–∏–Ω–Ω—ã—Ö –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Ç.–Ω. Extended Transformer Construction , ETC (–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞). –ß—Ç–æ–±—ã –¥–æ—Å—Ç–∏—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ö–∞–Ω–∏–∑–º –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –¢–∞–∫, –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏: –≥–ª–æ–±–∞–ª—å–Ω—ã–π –≤—Ö–æ–¥ (global input), –≥–¥–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∏ ¬´–¥–ª–∏–Ω–Ω—ã–π¬ª –≤—Ö–æ–¥ (long input), –≥–¥–µ —Ç–æ–∫–µ–Ω—ã –º–æ–≥—É—Ç –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∏–ª–∏ –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –≤—Ö–æ–¥—É, –∏–ª–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –æ–∫—Ä—É–∂–µ–Ω–∏—é. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç ETC –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–∞–∫–∂–µ ETC –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫, –∫–∞—Å–∞—é—â–∏—Ö—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π; –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ–º–∏–º–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (Masked Language Model, MLM) –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤—Ä–æ–¥–µ BERT; –≥–∏–±–∫–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–æ–æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è —Ç–æ–∫–µ–Ω–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—É—Å—Ç—å –¥–∞–Ω –¥–ª–∏–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞; –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é, —Å–æ–µ–¥–∏–Ω—è—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –∫ –∫–∞–∂–¥–æ–º—É –∞–±–∑–∞—Ü—É, –æ–±—ä–µ–¥–∏–Ω—è—è —Ç–æ–∫–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –∞–±–∑–∞—Ü–∞. –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ETC –º–æ–¥–µ–ª–∏. –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç—Å—è –∫–∞–∫ C (—Å–∏–Ω–∏–π) –¥–ª—è –∞–±–∑–∞—Ü–∞, S (–∂–µ–ª—Ç—ã–π) –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç—Å—è –∫–∞–∫ X (—Å–µ—Ä—ã–π) –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–ª–∏–Ω–Ω–æ–º—É –≤—Ö–æ–¥—É. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã–¥–∞—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø—è—Ç–∏ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–ª–∏–Ω–Ω—ã—Ö –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: TriviaQA , Natural Questions (NQ) , HotpotQA , WikiHop –∏ OpenKP . –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –ø–æ–¥–Ω–∞–±–æ—Ä–µ –¥–ª—è –∑–∞–¥–∞—á–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –î–ª—è –æ–±–æ–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö TriviaQA –∏ WikiHop, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ETC –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—É—é –ø–ª–∞–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞. –ü—Ä–æ–¥–æ–ª–∂–∞—è –Ω–∞—Ä–∞–±–æ—Ç–∫–∏ ETC, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ BigBird ‚Äì –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫–∂–µ –∏–º–µ—é—â–∏–π –ª–∏–Ω–µ–π–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ–Ω—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç ETC, –¥–ª—è BigBird –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ BigBird —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —á–∞—Å—Ç–µ–π: –Ω–∞–±–æ—Ä –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–æ—Ç–Ω–æ—Å–∏–º—ã—Ö —Å–æ –≤—Å–µ–º–∏ —á–∞—Å—Ç—è–º–∏ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏; –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å–æ–æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è; –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å–æ–æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —á–∏—Å–ª–æ–º —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ BigBird –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –≥—Ä–∞—Ñ –í–∞—Ç—Ü–∞-–°—Ç—Ä–æ–≥–∞—Ü–∞, –≤ –∫–æ—Ç–æ—Ä—ã–π –¥–æ–±–∞–≤–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –í —Å—Ç–∞—Ç—å–µ, –ø–æ—Å–≤—è—â–µ–Ω–Ω–æ–π BigBird , –∞–≤—Ç–æ—Ä—ã –ø–æ—è—Å–Ω—è—é—Ç, –ø–æ—á–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –æ–±—ä—è—Å–Ω—è—è –æ—Ç—á–∞—Å—Ç–∏ —É—Å–ø–µ—Ö ETC. –ü—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –≤–∞–∂–Ω—ã–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –æ—Ü–µ–Ω–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –ø–æ—Ç–æ–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º–æ–π –º–µ–∂–¥—É —É–∑–ª–∞–º–∏ (—Ç.–µ. —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–ª–∏—è—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–π). –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Å–ª—É–∂–∞—Ç —Å–≤–æ–µ–æ–±—Ä–∞–∑–Ω—ã–º –∫–∞–Ω–∞–ª–æ–º –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏, –∫–∞–∫ –¥–æ–∫–∞–∑–∞–ª–∏ –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏, –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç–æ–ª—å –∂–µ –º–æ—â–Ω—ã–º, –∫–∞–∫ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∞–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ BigBird –∏–º–µ–µ—Ç —Ç–∞–∫—É—é –∂–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –ø—Ä–∏ —ç—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º (—Å–º. —Ä–∞–±–æ—Ç—ã Yun et al. –∏ Perez et al. ), –∞ —Ç–∞–∫–∂–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä–æ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Å–ª—É—á–∞–π–Ω—ã–µ –≥—Ä–∞—Ñ—ã –º–æ–≥—É—Ç –µ—â–µ –±–æ–ª—å—à–µ –æ–±–ª–µ–≥—á–∏—Ç—å –ø–µ—Ä–µ–¥–∞—á—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á—Ç–æ —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –∫–∞–∫ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, —Ç–∞–∫ –∏ —Å –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –î–∞–ª—å–Ω–µ–π—à–µ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–Ω–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (gradient checkpointing) –ø—É—Ç–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–µ–º –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ç—Ä–µ–±—É—é—â–∏—Ö —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞. –¢–∞–∫, –Ω–∞ –∑–∞–¥–∞—á–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≤—Ç–æ—Ä—ã –¥–æ—Å—Ç–∏–≥–ª–∏ –Ω–∞–∏–ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ä–µ–¥–∏ –ø—Ä–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç. –ú–µ—Ç—Ä–∏–∫–∞ ROUGE –¥–ª—è –∑–∞–¥–∞—á–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ù–æ–≤–∞—è –ø–ª–∞–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö BigPatent –∏ ArXiv . –ë–æ–ª–µ–µ —Ç–æ–≥–æ, —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ BigBird —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª–µ–µ –æ–±—â–µ–π –∑–∞–º–µ–Ω–æ–π —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤ –Ω–æ–≤—ã—Ö –¥–æ–º–µ–Ω–∞—Ö, –¥–∞–∂–µ –Ω–µ –∏–º–µ—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –Ω–æ–≤–æ–π –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ ‚Äì –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–æ–º–∞ (–î–ù–ö). –ë–æ–ª–µ–µ –¥–æ–ª–≥–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç BigBird –¥–æ—Å—Ç–∏—á—å –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–º–æ—Ç–æ—Ä–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ (promoter-region prediction) –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è —Ö—Ä–æ–º–∞—Ç–∏–Ω–∞ (chromatin profile prediction). BigBird –±—å—ë—Ç –±–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–º–æ—Ç–æ—Ä–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è —Ö—Ä–æ–º–∞—Ç–∏–Ω–∞ –≤–∫–ª—é—á–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (transcription factors, TF), –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–∏—Å—Ç–æ–Ω–æ–≤ (HM) –∏ –≥–∏–ø–µ—Ä—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –î–ù–ö–∞–∑–µ (histone-mark (HM) and DNase I hypersensitive (DHS) detection) . –ë–æ–ª–µ–µ —Ç–æ–≥–æ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –≤ –≥–µ–Ω–µ—Ç–∏–∫–µ –¥–ª—è –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞ –µ—â–µ –º–∞–ª–æ–∏–∑—É—á–µ–Ω—ã. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –û–¥–Ω–∏–º –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π –¥–ª—è –ø–æ–≤—Å–µ–º–µ—Å—Ç–Ω–æ–≥–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏. –ö–∞–∫ –≤ —Å–ª—É—á–∞–µ —Å ETC, —Ç–∞–∫ –∏ —Å BigBird, –æ–¥–Ω–∏–º –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π –∞–≤—Ç–æ—Ä–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å–∫–æ—Ä–∏—Ç–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GPU –∏ TPU, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç –±–ª–æ–∫–∏ —Å–º–µ–∂–Ω—ã—Ö –±–∞–π—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ —Å–ø–æ—Ä–∞–¥–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –≤—ã–∑–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–º –æ–∫–Ω–æ–º (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è) –∏–ª–∏ –≤—ã–±–æ—Ä–∫–æ–π —Å–ª—É—á–∞–π–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (—Å–ª—É—á–∞–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ). –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∏ —Å–ª—É—á–∞–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –ø–ª–æ—Ç–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –∫–æ–º–∞–Ω–¥, –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö (single instruction, multiple data, SIMD). –î–ª—è —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä—ã —Å–Ω–∞—á–∞–ª–∞ ¬´–±–ª–æ–∫–∏—Ä—É—é—Ç¬ª –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ–±—ã –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU/TPU, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–ª–æ–∫–∞–º–∏. –ó–∞—Ç–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –ø–ª–æ—Ç–Ω–æ–µ —Ç–µ–Ω–∑–æ—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ä—è–¥–∞ –ø—Ä–æ—Å—Ç—ã—Ö –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã, –≤—Ä–∞—â–µ–Ω–∏–µ –∏ —Å–±–æ—Ä–∫–∞, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –Ω–∏–∂–µ. –ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ ¬´–æ–∫–Ω–µ¬ª —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º—ã –±–µ–∑ –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–ø–æ—Ä–∞–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ù–µ–¥–∞–≤–Ω–æ –≤ —Å—Ç–∞—Ç—å–µ ¬´ Long Range Arena: A Benchmark for Efficient Transformers ¬ª –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏–∑ —à–µ—Å—Ç–∏ –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å BigBird, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å–≤–æ–∏—Ö –∞–Ω–∞–ª–æ–≥–æ–≤, —è–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –í —Å–≤–æ–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∞–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç–æ–ª—å –∂–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–º –∏ –≥–∏–±–∫–∏–º, –∫–∞–∫ –∏—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –ù–∞—Ä—è–¥—É —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º, –∞–≤—Ç–æ—Ä—ã –æ–±–µ—Å–ø–µ—á–∏–ª–∏ –≤–µ—Å—å–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–ª—è –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–æ–º–∞. –£—á–∏—Ç—ã–≤–∞—è –æ–±—â–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–∏–º –∫–æ –º–Ω–æ–≥–∏–º –¥—Ä—É–≥–∏–º –∑–∞–¥–∞—á–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∫–æ–¥–∞ –∏ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤ –æ—Ç–∫—Ä—ã—Ç–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ . –ê–≤—Ç–æ—Ä—ã –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∫–∞–∫ –¥–ª—è ETC (github) , —Ç–∞–∫ –∏ –¥–ª—è BigBird (github) , —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–π –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ GPU –∏ TPU. –ê–≤—Ç–æ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ ‚Äì Avinava Dubey –ü–µ—Ä–µ–≤–æ–¥ ‚Äì –°–º–∏—Ä–Ω–æ–≤–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ë—Ä—Å—Ç–∫–∞ ‚Äì –®–∫–∞—Ä–∏–Ω –°–µ—Ä–≥–µ–π Sparse Attention –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    "extraction_method": "requests+bs4",
    "content_length": 12453,
    "status": "success",
    "error_message": null
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "duckduckgo",
    "url": "https://medium.com/@coderhack.com/fine-tuning-bert-for-text-classification-a-step-by-step-guide-1a1c5f8e8ae1",
    "title": "Fine-tuning BERT for Text Classification: A Step-by-Step Guide",
    "text": "Fine-tuning BERT for Text Classification: A Step-by-Step Guide BERT is a powerful pre-trained language model that can be fine-tuned for a variety of NLP tasks. In this article, I will provide a step-by-step guide to fine-tuning BERT for document classification and sentiment analysis. To follow along with this tutorial, you will need: Transformers library by Hugging Face Dataset for your text classification task We will be using the 20 Newsgroups dataset for document classification and the SST-2 dataset for sentiment analysis. Preparing the Data For the 20 Newsgroups dataset, download the data and extract the files. We will create a data.csv file with columns text and label . For SST-2, download the dataset and you will get train and test CSV files with columns sentence , sentiment and label . We will load the data into PyTorch Datasets and Dataloaders to feed into our model. Fine-tuning BERT Choose a BERT model: We will use bert-base-uncased for this tutorial. Load it with the Transformers library: Define a classifier head: We will add a classifier layer on top of BERT. For a single label, we use a Dense layer. For multiple labels, we use one output for each class. Concatenate the BERT encoder and classifier into a single model: Define an optimizer and loss function for your model. We will use cross-entropy loss for the classifiers: Create DataLoader objects from your datasets to feed data into the model. We use a batch size of 16: Define a training function to train your model. We train for 3‚Äì4 epochs: Define an evaluation function to evaluate your model on the test set: Train your model by calling the training function, and evaluate on the test set: Save your model: You have fine-tuned BERT for your text classification task! You can now use the saved model to make predictions on new data.",
    "extraction_method": "requests+bs4",
    "content_length": 1821,
    "status": "success",
    "error_message": null
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "duckduckgo",
    "url": "https://medium.com/researchify/fine-tuning-bert-for-text-classification-a-step-by-step-guide-with-code-examples-0dea8513bcf2",
    "title": "Fine-Tuning BERT for Text Classification: A Step-by-Step Guide with ...",
    "text": "Fine-Tuning BERT for Text Classification: A Step-by-Step Guide with Code Examples In our last blog, we explored how to choose the right transformer model , highlighting BERT‚Äôs strengths in classification tasks. Now, we dive deeper into fine-tuning BERT with real-world implementations and hands-on code. Text classification is a cornerstone of natural language processing (NLP), enabling tasks such as sentiment analysis, spam detection, and topic categorization. At the forefront of NLP advancements is BERT (Bidirectional Encoder Representations from Transformers) , a pre-trained transformer model renowned for its ability to understand context in text. Fine-tuning BERT for classification tasks not only leverages its contextual understanding but also allows for exceptional performance, even with smaller datasets, as long as they are clean and well-prepared. This blog will guide you through the process of fine-tuning BERT step-by-step, demonstrating its real-world applications with hands-on code and practical insights. Data Preparation Before fine-tuning BERT, it‚Äôs essential to prepare clean, balanced, and well-structured data to ensure the model learns meaningful patterns and generalizes effectively. In this tutorial, we‚Äôll use a real-world example of resume text chunks, each representing different sections such as Contact Information, Education, Work Experience, and Skills. To achieve this, ensure the data is free from irrelevant text or missing rows, balanced across classes, and diverse enough to improve model generalization. This will optimize the model‚Äôs performance during training and inference. Implementation: Fine Tuning Bert Model Importing Necessary Libraries This sets up our environment for the next steps. Prepare Data For Training Data preparation is critical for training an effective classification model. Here‚Äôs what we focus on: ‚Ä¢ Cleaning : Removing irrelevant or incomplete data ensures we work with a high-quality dataset. ‚Ä¢ Label Encoding : Converting categorical labels (like ‚ÄúEducation‚Äù or ‚ÄúSkills‚Äù) into numeric labels makes the data compatible with the model. ‚Ä¢ Train-Test Split : Separating the data into training and validation sets ensures the model can generalize well. For example, with a dataset of resume chunks labeled by section: This ensures the data is structured, labeled, and ready for tokenization. Tokenizer and Dataset Class After data preparation, we need to tokenize the text and format it for the BERT model: The BertTokenizer splits text into smaller subwords and tokens that BERT can process. It also adds special tokens like: ‚Ä¢ [CLS]: Indicates the start of the input. ‚Ä¢ [SEP]: Separates segments in input. The custom ClassificationDataset class takes raw text, tokenizes it, and prepares it for training. It also creates: ‚Ä¢ Input IDs : Tokenized text converted into integers. ‚Ä¢ Attention Masks : Flags to distinguish real tokens from padding. ‚Ä¢ Labels : Encoded numeric labels for each input. This class ensures the data is in the correct format for BERT. BERT is a pre-trained model that can be fine-tuned for specific tasks like classification: 1. Load Pre-trained BERT : We use bert-base-uncased, which is a lowercase English BERT model. 2. Specify the Number of Labels : In this case, the number of unique sections in the resume data. 3. Device Setup : Leverages GPU if available for faster training. The model is now ready for training. The training loop is where the model learns patterns in the data: 1. Forward Pass : The input data is fed through the model to calculate predictions. 2. Loss Calculation : The loss measures how far the predictions are from actual labels. 3. Backward Pass : The optimizer updates the model weights to minimize the loss. This trains the model to classify resume sections effectively. After training, the model is evaluated to measure its performance on the validation set. Metrics like accuracy, precision, recall, and F1 score are calculated. The fine-tuned BERT model, trained on 1,839 labeled data points, achieved the following validation metrics: Accuracy : 0.8553, F1 Score : 0.8572, Precision : 0.8617, Recall : 0.8553 These results demonstrate the model‚Äôs strong performance and generalization capability, even with a modest dataset size. Saving the Model Saving the trained model, tokenizer, and label encoder allows us to reuse them for predictions or further training. Finally, we use the saved model to classify new text data. This demonstrates how to classify new resume chunks with the fine-tuned model. To make this tutorial more accessible, I‚Äôve provided a Colab notebook that includes all the code and explanations discussed in this blog. You can run the notebook in your browser, explore the implementation hands-on, and adapt it to your datasets with ease. Access the notebook here: Colab Notebook In this blog, we explored how to fine-tune BERT for classification tasks, using real-world data and hands-on implementation. From data preparation to evaluation, each step was tailored to help you apply BERT effectively in your own projects. Next, we‚Äôll dive into advanced techniques like deploying models into production environments, scaling them for real-world applications, and understanding their capabilities and limitations. Stay tuned!",
    "extraction_method": "requests+bs4",
    "content_length": 5271,
    "status": "success",
    "error_message": null
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "duckduckgo",
    "url": "https://medium.com/@danushidk507/fine-tune-bert-for-text-classification-with-tensorflow-7175c4ab5475",
    "title": "Fine-Tune BERT for Text Classification with TensorFlow",
    "text": "Fine-Tune BERT for Text Classification with TensorFlow BERT (Bidirectional Encoder Representations from Transformers) is a natural language processing (NLP) model that has achieved state-of-the-art results on a variety of tasks, including text classification, question answering, and natural language inference. BERT is a transformer-based model, which means that it uses a self-attention mechanism to learn contextual representations of words in a text. Unlike previous NLP models, which could only learn contextual representations of words in a left-to-right fashion, BERT can learn contextual representations of words in both directions. This allows BERT to better understand the meaning of words in a text, even when they are far apart. BERT is trained on a massive dataset of text and code, which allows it to learn a general understanding of language. This makes BERT suitable for a wide variety of NLP tasks. Here is a more detailed explanation of the BERT model architecture: Input: The input to the BERT model is a sequence of text tokens. Each token is represented as a vector of numbers. Encoder: The encoder consists of a stack of transformer layers. Each transformer layer consists of a self-attention mechanism and a feedforward network. Output: The output of the encoder is a sequence of hidden states for each token in the input sequence. The BERT model can be fine-tuned for a variety of NLP tasks by adding a classification head to the output of the encoder. The classification head is a simple feedforward network that predicts the class label for each text input. BERT has been shown to be effective for a variety of NLP tasks, including: BERT is a powerful NLP model that can be used for a variety of tasks. It is easy to use and can be fine-tuned for specific tasks with minimal effort. Here are some of the advantages of using BERT: State-of-the-art performance on a variety of NLP tasks. Easy to use and fine-tune. Can be used for a variety of tasks, including text classification, question answering, and natural language inference. Some of the disadvantages of using BERT include: Requires a large amount of data to train. Can be computationally expensive to train and use. Can be difficult to interpret the results of BERT models. Fine-tuning is a machine learning technique where a pre-trained model is adapted to a specific task by adjusting the model‚Äôs parameters. This is done by training the model on a smaller dataset that is specific to the task. Fine-tuning is often used to train machine learning models for natural language processing (NLP) tasks, such as text classification, question answering, and machine translation. This is because pre-trained NLP models, such as BERT and RoBERTa, have been trained on massive datasets of text and code. This allows them to learn a general understanding of language, which can be helpful for a variety of NLP tasks. To fine-tune a pre-trained NLP model, you first need to load the model and then define a new classification head. The classification head is a simple feedforward network that predicts the class label for each text input. You then need to train the model on a smaller dataset of text and labels that are specific to the task. Once the model is trained, you can use it to predict the class label for new text inputs. Fine-tuning can be a very effective way to train machine learning models for NLP tasks, as it allows you to leverage the knowledge that has been learned by a pre-trained model. Here are some of the benefits of fine-tuning: It can be faster and easier than training a model from scratch. It can achieve better performance than training a model from scratch, especially on small datasets. It can be used to adapt a pre-trained model to a new task without losing the knowledge that the model has already learned. Here are some of the challenges of fine-tuning: It can be difficult to choose the right hyperparameters for the training process. It can be difficult to evaluate the performance of the model on a small dataset. It can be difficult to interpret the results of the model. Download and Import the Quora Insincere Questions Dataset Task 4: Create tf.data.Datasets for Training and Evaluation Task 5: Download a Pre-trained BERT Model from TensorFlow Hub Task 6: Tokenize and Preprocess Text for BERT We‚Äôll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExamples using classifier_data_lib's constructor InputExample provided in the BERT library. Task 7: Wrap a Python Function into a TensorFlow op for Eager Execution Task 8: Create a TensorFlow Input Pipeline with tf.data The resulting tf.data.datasets return (features,labels) pairs Task 9: Add a Classification Head to the BERT Layer Task 10: Fine-Tune BERT for Text Classification Task 11: Evaluate the BERT Text Classification Model",
    "extraction_method": "requests+bs4",
    "content_length": 4860,
    "status": "success",
    "error_message": null
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "semantic_scholar",
    "paperId": "d72eefba7ca103a4b520824c1e996469ca049aa3",
    "url": "https://www.semanticscholar.org/paper/d72eefba7ca103a4b520824c1e996469ca049aa3",
    "title": "Bert model fine-tuning for text classification in knee OA radiology reports",
    "year": 2020,
    "authors": [
      "L. Chen",
      "Rutwik Shah",
      "T. Link",
      "M. Bucknor",
      "S. Majumdar",
      "V. Pedoia"
    ],
    "abstract": null,
    "isOpenAccess": true,
    "externalIds": {
      "MAG": "3018968729",
      "DOI": "10.1016/j.joca.2020.02.488",
      "CorpusId": 219080329
    },
    "pdf_url": "http://www.oarsijournal.com/article/S1063458420305550/pdf",
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "pdf_fetch_failed",
    "error_message": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å PDF –∫–æ–Ω—Ç–µ–Ω—Ç."
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "semantic_scholar",
    "paperId": "2aa86b563e760b89e01f652224f24018eb9843e2",
    "url": "https://www.semanticscholar.org/paper/2aa86b563e760b89e01f652224f24018eb9843e2",
    "title": "Empirical Study of LLM Fine-Tuning for Text Classification in Legal Document Review",
    "year": 2023,
    "authors": [
      "Fusheng Wei",
      "R. Keeling",
      "Nathaniel Huber-Fliflet",
      "Jianping Zhang",
      "Adam Dabrowski",
      "Jingchao Yang",
      "Qiang Mao",
      "Han Qin"
    ],
    "abstract": "The increased integration of Large Language Models (LLMs) across industry sectors is enabling domain experts with new text classification optimization methods. These LLMs are pretrained on exceedingly large amounts of data; however, practitioners can perform additional training, or ‚Äúfine-tuning,‚Äù to improve their text classifier‚Äôs results for their own use cases. This paper presents a series of experiments comparing a standard, pretrained DistilBERT model and a fine-tuned DistilBERT model, both leveraged for the downstream NLP task of text classification. Tuning the model using domain-specific data from real-world legal matters suggests fine-tuning improves the performance of LLM text classifiers.To evaluate the performance of text classification models, using these two Large Language Models, we employed two distinct approaches that 1) score a whole document‚Äôs text for prediction and 2) score snippets (sentence-level components of a document) of text for prediction. When comparing the two approaches we found that one prediction method outperforms the other, depending on the use case.",
    "isOpenAccess": false,
    "externalIds": {
      "DBLP": "conf/bigdataconf/WeiKHZDYMQ23",
      "DOI": "10.1109/BigData59044.2023.10386911",
      "CorpusId": 267144695
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "BERT model fine-tuning for text classification",
    "plan_item": "Fine-tuning BERT",
    "plan_item_id": "plan_bert",
    "query_id": "q_bert_0",
    "source": "semantic_scholar",
    "paperId": "7fe99842172a21b866360dc6c5b14425e6a9af49",
    "url": "https://www.semanticscholar.org/paper/7fe99842172a21b866360dc6c5b14425e6a9af49",
    "title": "Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning",
    "year": 2024,
    "authors": [
      "Hang Zhao",
      "Qile P. Chen",
      "Yijing Barry Zhang",
      "Gang Yang"
    ],
    "abstract": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.",
    "isOpenAccess": false,
    "externalIds": {
      "DBLP": "journals/corr/abs-2412-08587",
      "ArXiv": "2412.08587",
      "DOI": "10.48550/arXiv.2412.08587",
      "CorpusId": 274638463
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "duckduckgo",
    "url": "https://www.freecodecamp.org/news/react-lifecycle-methods-and-hooks-for-beginners/",
    "title": "React Lifecycle Methods and Hooks - a Beginner's Guide",
    "text": "React is all about building user interfaces. And to do that effectively, React provides ways for components to manage their lifecycles. This means that components can perform specific tasks at different stages of their existence, from the moment they are created to the point they are removed from the user interface. Lifecycle methods have been a fundamental part of React for many years. But with the introduction of hooks, React's approach to managing state and side effects in functional components has become more intuitive and flexible. Just a quick note: although hooks generally replace class components, there are no plans to remove classes from React. In this tutorial, you will learn about class component lifecycle methods such as componentDidMount , componentDidUpdate , componentWillUnmount , and shouldComponentUpdate . You'll also explore React hooks like useState , useEffect , and useContext , and understand why they were introduced. This will make your React journey smoother and more enjoyable. Whether you're just getting started with React or looking to deepen your understanding, this guide will equip you with the knowledge you need to build responsive and interactive web applications using React's powerful tools. Let's dive in and uncover the magic of React lifecycle methods and hooks. How the Component Lifecycle Works In React, components go through a lifecycle composed of distinct stages. Each of these stages offers specific methods that you can customize to run code at various moments during a component's existence. These methods help you perform tasks such as initializing data, managing updates, and tidying up resources as needed. Class Component Lifecycle Methods Let's start by looking at the class component lifecycle methods. These were the primary way to manage component lifecycle before the introduction of hooks. How to use componentDidMount : This is called after a component has been inserted into the DOM. It's a great place to perform initial setup tasks, like fetching data from an API or setting up event listeners. In this example, we created a class component called MyComponent . In the constructor, the component's state is initialized with data set to null, and we use it to store the fetched data. In the componentDidMount method, we simulate fetching data from an API using setTimeout to mimic an asynchronous operation. After 2 seconds (2000 milliseconds), the component's state updates with the fetched data. In the render method, content is conditionally rendered based on the data state. If data is null, a Loading data... message is displayed. Otherwise, the fetched data is displayed. When you use this component in your application, you'll notice that the Loading data... message is shown initially, and after 2 seconds, the fetched data is displayed. This demonstrates how componentDidMount is useful for performing tasks after a component has been added to the DOM. How to use componentDidUpdate B: This is called after a component has re-rendered due to changes in its state or props. It's a great place to handle side effects or perform additional actions based on those changes. In this code example, we create a Counter class component with a constructor that initializes the count state to 0. The handleIncrement method updates the count state when the Increment button is clicked. Inside the componentDidUpdate lifecycle method, we log a message (Component updated) to the console. We also log both the previous state (prevState) and the current state (this.state). This demonstrates how you can access both the previous and current values during an update. The render method displays the current count and a button to increment it. Now, when you use this Counter component in your application, open the browser's console. Every time you click the Increment button, you'll see messages in the console indicating that the component has updated, along with the previous and current state values. You can use componentDidUpdate for various purposes, such as making network requests when props or state change, updating the DOM based on state changes, or interacting with third-party libraries after an update. It provides a way to perform actions that should occur specifically after a component has re-rendered. How to use componentWillUnmount This is called just before a component is removed from the DOM. It's a crucial place to perform cleanup tasks, such as clearing timers, unsubscribing from events, or releasing resources to prevent [memory leaks](https://en.wikipedia.org/wiki/Memory_leak#:~:text=In computer science%2C a memory,longer needed is not released.). Let's illustrate a simple React component that sets up a timer when it mounts, using componentDidMount method, and clears that timer when it unmounts using the componentWillUnmount method. In this example, we created the TimerComponent class. Inside the constructor, the component's state is initialized with a seconds property, which we'll use to keep track of the elapsed time. The timer variable is also set to null. In the componentDidMount lifecycle method, the timer is started by using setInterval . This timer increments the seconds state property every second. In the componentWillUnmount lifecycle method, the timer is cleared using clearInterval to ensure that it doesn't continue running after the component has been removed from the DOM. In the render method, the elapsed time is displayed based on the seconds state property. When you use this TimerComponent in your application and render it, you'll notice that the timer starts when the component is mounted and stops when the component is unmounted. This is thanks to the cleanup performed in the componentWillUnmount method. This prevents resource leaks and ensures that the timer is properly managed throughout the component's lifecycle. How to use shouldComponentUpdate We use this lifecycle method to control whether a component should re-render when its state or props change. It is particularly useful for optimizing performance by preventing unnecessary renders. Let's create a simple React class component and use the shouldComponentUpdate method to decide whether the component should re-render based on changes in its state. In this example, we created the Counter class component that maintains a count state, which starts at 0. In the shouldComponentUpdate method, we check whether the next state's count is even. If it is, we allow the component to re-render. Otherwise, we prevent the re-render. The incrementCount method is called when the Increment button is clicked. It updates the count state by incrementing it. In the render method, the current count and a button to increment it is displayed. If you click the Increment button and the count becomes an odd number, the component won't re-render. This behavior demonstrates how shouldComponentUpdate can be used to optimize rendering in situations where not all state changes should trigger a re-render. Introducing React Hooks React introduced hooks in version 16.8. They granted functional components access to state and various React features without writing class components. As a result, class components have become largely unnecessary. Hooks simplify component logic and make it more reusable. Hooks were introduced to address several issues and make React code easier to understand and maintain: Complexity ‚Äì class components can become complex when managing state and side effects. Complexity ‚Äì class components can become complex when managing state and side effects. Reusability ‚Äì logic in class components isn't easily shareable between components. Reusability ‚Äì logic in class components isn't easily shareable between components. Learning Curve ‚Äì class components introduce a steeper learning curve for newcomers to React. Learning Curve ‚Äì class components introduce a steeper learning curve for newcomers to React. Commonly used React Hooks useState lets you add state to functional components. It returns an array with the current state value and a function to update it. In this example, we used the useState hook to manage a counter's state. When the Increment button is clicked, setCount updates the count state, causing the component to re-render with the updated value. The useEffect hook useEffect is used for side effects in functional components, similar to componentDidMount and componentDidUpdate . It runs after rendering and can be controlled by specifying dependencies. In this example, useEffect is used to fetch data from an API when the component mounts. The empty dependency array [] ensures that the effect runs only once. When the data is fetched, setData updates the data state, causing a re-render with the fetched information. The useContext hook useContext allows functional components to access context values. It's a way to pass data down the component tree without explicitly passing props. In this example, we create a context called MyContext . The useContext hook allows MyComponent to access the value stored in this context. It's a powerful tool for managing global state in your application. Benefits of custom hooks Custom hooks are functions that use hooks internally and can be reused across multiple components. They help encapsulate and share complex logic. Here's an example of a custom hook called useLocalStorage that simplifies storing and retrieving data in the browser's local storage: In this custom hook, we import useState from React because we'll use it to manage the state. The useLocalStorage function takes two parameters: key : A string representing the key under which the data will be stored in local storage. key : A string representing the key under which the data will be stored in local storage. **initialValue** : The initial value for the state. **initialValue** : The initial value for the state. Inside the hook, we first attempted to retrieve the stored value from local storage using localStorage.getItem(key) . Then we initialized the state variable value using useState , using the storedValue if it exists or the initialValue if not. Next, we defined a function setStoredValue that updates both the state and the local storage when called. It sets the new value in local storage using localStorage.setItem(key, newValue) . Finally, we returned an array [value, setStoredValue] as the hook's return value, allowing components to access the stored value and update it as needed. Here's an example of how you can use the useLocalStorage hook in a component: In this example, we import the useLocalStorage custom hook and use it to manage a username value in local storage. The component initializes the username state using the hook and updates it when the input field changes. The value is stored and retrieved from local storage, allowing it to persist across page reloads. Custom hooks are a powerful way to encapsulate and reuse complex logic in React applications, making your code more modular and maintainable. React provides developers with powerful tools to manage the lifecycles of their components. These lifecycles allow components to perform specific tasks at different stages of their existence, from creation to removal. In this guide, we've explored React's class component lifecycle methods. These methods have been a fundamental part of React for many years and continue to be relevant in certain scenarios. You've also been introduced to React Hooks. These have become the preferred way to manage state and side effects in React applications. They offer a more intuitive and flexible approach to building components. While hooks have gained popularity and generally replace the need for class components, it's important to note that there are no plans to remove class components from React. Existing codebases and third-party libraries may still use class components, so understanding both class component lifecycles and hooks is valuable for React developers. In summary, React's lifecycle methods and hooks are crucial for building dynamic and efficient applications, and they offer developers a range of options to manage component behavior and state. As you continue to explore and work with React, you'll find that having a solid understanding of both lifecycles and hooks will make you a more versatile and capable React developer. If you found this guide helpful and enjoyable, please give it a like. For more insightful tutorials, follow me on X for updates üôè. Enjoy your coding! I am a Software Developer who is so passionate about teaching and writing. If you read this far, thank the author to show them you care. Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started",
    "extraction_method": "requests+bs4",
    "content_length": 12736,
    "status": "success",
    "error_message": null
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "duckduckgo",
    "url": "https://www.w3schools.com/react/react_lifecycle.asp",
    "title": "React Lifecycle - W3Schools",
    "text": "Each component in React has a lifecycle which you can monitor and manipulate during its three main phases. The three phases are:Mounting,Updating, andUnmounting. Mounting means putting elements into the DOM. React has four built-in methods that gets called, in this order, when mounting a component: Therender()method is required and will always be called, the others are optional and will be called if you define them. Theconstructor()method is called before anything else, when the component is initiated, and it is the natural place to set up the initialstateand other initial values. Theconstructor()method is called with theprops, as arguments, and you should always start by calling thesuper(props)before anything else, this will initiate the parent's constructor method and allows the component to inherit methods from its parent (React.Component). Theconstructormethod is called, by React, every time you make a component: ThegetDerivedStateFromProps()method is called right before rendering the element(s) in the DOM. This is the natural place to set thestateobject based on the initialprops. It takesstateas an argument, and returns an object with changes to thestate. The example below starts with the favorite color being \"red\", but thegetDerivedStateFromProps()method updates the favorite color based on thefavcolattribute: ThegetDerivedStateFromPropsmethod is called right before the render method: Therender()method is required, and is the method that actually outputs the HTML to the DOM. A simple component with a simplerender()method: ThecomponentDidMount()method is called after the component is rendered. This is where you run statements that requires that the component is already placed in the DOM. At first my favorite color is red, but give me a second, and it is yellow instead: The next phase in the lifecycle is when a component isupdated. A component is updated whenever there is a change in the component'sstateorprops. React has five built-in methods that gets called, in this order, when a component is updated: Therender()method is required and will always be called, the others are optional and will be called if you define them. Also atupdatesthegetDerivedStateFromPropsmethod is called. This is the first method that is called when a component gets updated. This is still the natural place to set thestateobject based on the initial props. The example below has a button that changes the favorite color to blue, but since thegetDerivedStateFromProps()method is called, which updates the state with the color from the favcol attribute, the favorite color is still rendered as yellow: If the component gets updated, thegetDerivedStateFromProps()method is called: In theshouldComponentUpdate()method you can return a Boolean value that specifies whether React should continue with the rendering or not. The default value istrue. The example below shows what happens when theshouldComponentUpdate()method returnsfalse: Stop the component from rendering at any update: Same example as above, but this time theshouldComponentUpdate()method returnstrueinstead: Therender()method is of course called when a component getsupdated, it has to re-render the HTML to the DOM, with the new changes. The example below has a button that changes the favorite color to blue: Click the button to make a change in the component's state: In thegetSnapshotBeforeUpdate()method you have access to thepropsandstatebeforethe update, meaning that even after the update, you can check what the values werebeforethe update. If thegetSnapshotBeforeUpdate()method is present, you should also include thecomponentDidUpdate()method, otherwise you will get an error. The example below might seem complicated, but all it does is this: When the component ismountingit is rendered with the favorite color \"red\". When the componenthas been mounted,a timer changes the state, and after one second, the favorite color becomes \"yellow\". This action triggers theupdatephase, and since this component has agetSnapshotBeforeUpdate()method, this method is executed, and writes a message to the empty DIV1 element. Then thecomponentDidUpdate()method is executed and writes a message in the empty DIV2 element: Use thegetSnapshotBeforeUpdate()method to find out what thestateobject looked like before the update: ThecomponentDidUpdatemethod is called after the component is updated in the DOM. The example below might seem complicated, but all it does is this: When the component ismountingit is rendered with the favorite color \"red\". When the componenthas been mounted,a timer changes the state, and the color becomes \"yellow\". This action triggers theupdatephase, and since this component has acomponentDidUpdatemethod, this method is executed and writes a message in the empty DIV element: ThecomponentDidUpdatemethod is called after the update has been rendered in the DOM: The next phase in the lifecycle is when a component is removed from the DOM, orunmountingas React likes to call it. React has only one built-in method that gets called when a component is unmounted: ThecomponentWillUnmountmethod is called when the component is about to be removed from the DOM. Click the button to delete the header: If you want to use W3Schools services as an educational institution, team or enterprise, send us an e-mail:sales@w3schools.com If you want to report an error, or if you want to make a suggestion, send us an e-mail:help@w3schools.com",
    "extraction_method": "requests+bs4",
    "content_length": 5434,
    "status": "success",
    "error_message": null
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "duckduckgo",
    "url": "https://www.freecodecamp.org/news/react-component-lifecycle-methods/",
    "title": "React Component Lifecycle Methods - Explained with Examples",
    "text": "In React, components have a lifecycle that consists of different phases. Each phase has a set of lifecycle methods that are called at specific points in the component's lifecycle. These methods allow you to control the component's behavior and perform specific actions at different stages of its lifecycle. A component's lifecycle has three main phases: the Mounting Phase, the Updating Phase, and the Unmounting Phase. The Mounting Phase begins when a component is first created and inserted into the DOM. The Updating Phase occurs when a component's state or props change. And the Unmounting Phase occurs when a component is removed from the DOM. Component Mounting Phase The mounting phase refers to the period when a component is being created and inserted into the DOM. During this phase, several lifecycle methods are invoked by React to enable the developer to configure the component, set up any necessary state or event listeners, and perform other initialization tasks. The mounting phase has three main lifecycle methods that are called in order: The constructor() lifecycle method The constructor() method is called when the component is first created. You use it to initialize the component's state and bind methods to the component's instance. Here's an example: In this example, the constructor() method sets the initial state of the component to an object with a count property set to 0 , and binds the handleClick method to the component's instance. The handleClick method increments the count state property when the button is clicked. The render() lifecycle method The render() method is responsible for generating the component's virtual DOM representation based on its current props and state. It is called every time the component needs to be re-rendered, either because its props or state have changed, or because a parent component has been re-rendered. In the above example in the constructor part: The render method displays the current count value and a button. When the button is clicked, the handleClick method is invoked. This triggers a state update, causing a re-render, and the updated count is displayed. The getDerivedStateFromProps() lifecycle method getDerivedStateFromProps() is a lifecycle method available in React 16.3 and later versions that is invoked during the mounting and updating phase of a component. During the mounting phase, getDerivedStateFromProps() is called after the constructor and before render() . This method is called for every render cycle and provides an opportunity to update the component's state based on changes in props before the initial render. The signature of getDerivedStateFromProps() is as follows: It takes two parameters: props : The updated props for the component. state : The current state of the component. The method should return an object that represents the updated state of the component, or null if no state update is necessary. It's important to note that getDerivedStateFromProps() is a static method, which means it does not have access to the this keyword and cannot interact with the component's instance methods or properties. This is a React component named Header that renders an <h1> element with a string that includes the value of this.state.favoritefood . The component has a constructor that sets the initial state of favoritefood to \"rice\". The componentDidMount() method is also defined, which is called when the component is mounted in the DOM. In this method, there's a setTimeout() function that updates the state of favoritefood to \"pizza\" after 1 second (1000 milliseconds). The component also has a static getDerivedStateFromProps() method that takes props and state as arguments and returns an object with a key of favoritefood and a value of props.favfod . This method is called during the mounting and updating phase of the component and is used to derive the state from props. Finally, the component's render() method returns the <h1> element with the value of this.state.favoritefood .When the component is rendered using ReactDOM.render() , it's mounted to the element with an ID of \"root\". The componentDidMount() lifecycle method The componentDidMount() method is called once the component has been mounted into the DOM. It is typically used to set up any necessary event listeners or timers, perform any necessary API calls or data fetching, and perform other initialization tasks that require access to the browser's DOM API. Here's an example: This code defines a React component called Header that extends the React.Component class. The component has a constructor that initializes the component state with a property called favoritefood set to the string \"rice\". The component also has a componentDidMount lifecycle method, which is called by React after the component has been mounted (that is, inserted into the DOM). In this method, a setTimeout function is used to delay the execution of a function that updates the favoritefood state property to \"pizza\" by 1 second. Finally, the component has a render method that returns a JSX expression containing an h1 element with the text \"My Favorite Food is\" and the current value of the favoritefood state property. Component Updating Phase This phase occurs when a component's props or state changes, and the component needs to be updated in the DOM. The shouldComponentUpdate() lifecycle method The shouldComponentUpdate() method is called before a component is updated. It takes two arguments: nextProps and nextState . This method returns a boolean value that determines whether the component should update or not. If this method returns true, the component will update, and if it returns false, the component will not update. Here's an example of how to use shouldComponentUpdate() : The above code defines a Header component that displays the user's favorite food and allows the user to change it by clicking a button. The shouldComponentUpdate() method is implemented to only re-render the component if the favoriteFood state has changed, which is a good way to optimize performance. The code uses ES6 syntax to define the component class and its methods, and imports Component from React to create the class. The ReactDOM.render() function is used to render the Header component to the DOM. The componentWillUpdate() lifecycle method componentWillUpdate() is a lifecycle method in React that gets called just before a component's update cycle starts. It receives the next prop and state as arguments and allows you to perform any necessary actions before the component updates. But this method is not recommended for updating the state, as it can cause an infinite loop of rendering. It is primarily used for tasks such as making API calls, updating the DOM, or preparing the component to receive new data. componentWillUpdate() is often used in conjunction with componentDidUpdate() to handle component updates. In this example, the componentWillUpdate method is called whenever the component is about to update. In this method, we can access the nextProps and nextState arguments to check if there are any changes to the component's state or props. If there are changes, we can perform some actions or log messages before the update happens. The componentDidUpdate lifecycle method The componentDidUpdate() method is a lifecycle method in React that is called after a component has been updated and re-rendered. It is useful for performing side effects or additional operations when the component's props or state have changed. Here's an example of how to use the componentDidUpdate() method: In this example, the componentDidUpdate() method is used to log a message to the console whenever the count state has been updated. It compares the previous state ( prevState.count ) with the current state ( this.state.count ) to check if there was a change. Whenever the handleClick() method is called, the count state is incremented, triggering a re-render of the component. After the re-render, componentDidUpdate() is called, and it logs the updated count value to the console. It's important to include a conditional check inside componentDidUpdate() to prevent infinite loops. If you want to update the state based on a prop change, make sure to compare the previous prop ( prevProps ) with the current prop ( this.props ) before updating the state. Remember that componentDidUpdate() is not called during the initial render of the component, only on subsequent updates. As of React 16.3, the componentDidUpdate() method can receive two additional arguments: prevProps and prevState . These arguments provide access to the previous props and state values, which can be useful for performing comparisons. But if you are using a more recent version of React, you can utilize the useEffect() hook with dependency array to achieve similar functionality. The getSnapshotBeforeUpdate lifecycle method The getSnapshotBeforeUpdate() method is called just before the component's UI is updated. It allows the component to capture some information about the current state of the UI, such as the scroll position before it changes. This method returns a value that is passed as the third parameter to the componentDidUpdate() method. Here's an example of how to use getSnapshotBeforeUpdate() to capture the scroll position of a component before it updates: This is a React component called Header that renders a heading and a button that, when clicked, shows the user's favorite food. The component also has a state that keeps track of the favorite food and whether or not to show it. The constructor method sets the component's initial state, including the default favorite food of \"rice\" and the showFavFood state variable to false . The componentDidMount method is called after the component has been mounted to the DOM. In this case, it sets a timeout that will change the favorite food to \"pizza\" after one second. The getSnapshotBeforeUpdate method is called right before the component is updated. It checks if the favoriteFood state variable has changed since the last update and returns an object with the previous favorite food if it has. Otherwise, it returns null . The componentDidUpdate method is called after the component has been updated. It receives the previous props, state, and snapshot as arguments. In this case, it checks if the snapshot is not null and logs the previous favorite food to the console. In the render method, the component renders a heading that displays the current favorite food state variable. When the button is clicked, the showFavFood state variable is set to true and a paragraph is rendered that displays the current favorite food state variable. Finally, the ReactDOM.render function is called to render the Header component inside an HTML element with the id of \"root\". Component Unmounting Phase The unmounting phase refers to the lifecycle stage when a component is being removed from the DOM (Document Object Model) and is no longer rendered or accessible. During this phase, React performs a series of cleanup operations to ensure that the component and its associated resources are properly disposed of. The unmounting phase is the last stage in the lifecycle of a React component and occurs when the component is being removed from the DOM tree. This can happen for various reasons, such as when the component is no longer needed, the parent component is re-rendered without including the child component, or when the application is navigating to a different page or view. The componentWillUnmount() lifecycle method During the unmounting phase, React calls the following lifecycle methods in order: componentWillUnmount() : This method is called just before the component is removed from the DOM. It allows you to perform any necessary cleanup, such as canceling timers, removing event listeners, or clearing any data structures that were set up during the mounting phase. After componentWillUnmount() is called, the component is removed from the DOM and all of its state and props are destroyed. It's important to note that once a component is unmounted, it cannot be mounted again. If you need to render the component again, you will need to create a new instance of it. Here's an example of how you might use the componentWillUnmount() method to perform cleanup: This is a React component that renders a MyComponent with a Child component that will be conditionally rendered based on the value of showChild state. When the user clicks the \"Delete Header\" button, the handleDelete function will be called which will update the showChild state to false . This causes the Child component to unmount and trigger the componentWillUnmount lifecycle method, which will display an alert message. The MyComponent class extends the Component class provided by React and defines a state variable showChild with an initial value of true . It also defines a function handleDelete that will be called when the user clicks the \"Delete Header\" button. This function updates the showChild state to false . In the render method of MyComponent , the showChild state is used to conditionally render the Child component using the logical && operator. If showChild is true , the Child component will be rendered. Otherwise, it will not be rendered. The Child class extends the Component class and defines a componentWillUnmount method that will be called just before the component is unmounted. In this case, it displays an alert message. Finally, the ReactDOM.render method is called to render the MyComponent component inside the element with the ID \"root\" in the HTML document. In summary, React components have a lifecycle consisting of three phases: Mounting, Updating, and Unmounting. Each phase has specific lifecycle methods that are called at different points in the component's lifecycle. Understanding these lifecycle methods can help developers to control the component's behavior and perform specific actions at different stages of its lifecycle. If this article was helpful, . Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started",
    "extraction_method": "requests+bs4",
    "content_length": 14125,
    "status": "success",
    "error_message": null
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "semantic_scholar",
    "paperId": "f0a4210c2f9ed0bfd7d3decf855775683dad0495",
    "url": "https://www.semanticscholar.org/paper/f0a4210c2f9ed0bfd7d3decf855775683dad0495",
    "title": "ReactAppScan: Mining React Application Vulnerabilities via Component Graph",
    "year": 2024,
    "authors": [
      "Zhiyong Guo",
      "Ming-Zhi Kang",
      "V. Venkatakrishnan",
      "Rigel Gjomemo",
      "Yinzhi Cao"
    ],
    "abstract": "React, a single-page application framework, has recently become popular among web developers due to its flexible and convenient management of web application states via a syntax extension to JavaScript, called JSX (JavaScript and XML). Despite its abundant functionalities, the security of React, especially vulnerability detection, still lags: many existing vulnerability detection works do not support JSX let alone React Data Flow introduced by React components. The only exception is CodeQL, which supports JSX syntax. However, CodeQL cannot properly track React Data Flow across different components for detecting vulnerabilities. In this paper, we design a novel framework, called ReactApp-Scan, which constructs a Component Graph (CoG) for tracking Re-actDataFlowand detectingvulnerabilitiesfollowingbothJavaScript and React data flows. Specifically, ReactAppScan relies on abstract interpretation to build such a component graph via tracking component lifecycles and then detects vulnerabilities via finding paths be-tween sources and sinks. Our evaluation shows that ReactAppScan detects 61 zero-day vulnerabilities in real-world React applications. We have responsibly reported all the vulnerabilities and so far six vulnerabilities have been fixed and two have been acknowledged.",
    "isOpenAccess": true,
    "externalIds": {
      "DBLP": "conf/ccs/GuoKVGC24",
      "DOI": "10.1145/3658644.3670331",
      "CorpusId": 273365689
    },
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670331",
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "pdf_fetch_failed",
    "error_message": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å PDF –∫–æ–Ω—Ç–µ–Ω—Ç."
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "semantic_scholar",
    "paperId": "00ffda26a76353f9b28b66f773115c11c733b7a9",
    "url": "https://www.semanticscholar.org/paper/00ffda26a76353f9b28b66f773115c11c733b7a9",
    "title": "React's Evolution and the Paradigm Shift of Hooks in Modern Web Development",
    "year": 2019,
    "authors": [
      "Vishnuvardhan Reddy Goli"
    ],
    "abstract": "The component-based framework within React devised a new approach to frontend development that enabled better code reuse alongside simplified maintenance procedures. Virtual DOM by React established itself as a library leader by minimizing browser communication because of its performance benefits. The Hooks feature from version 16.8 marked the most significant evolutionary addition, allowing functional components to independently manage both state and side effects without class components. Introducing Hooks like useState and useEffect brought state management improvements that produced more condensed and easier-to-understand code. This article investigates React‚Äôs development trajectory, the critical role of virtual DOM, and the fundamental changes from Hooks to modern web application development. The article explores distinct aspects to show how Hooks now controls software development methods and helps programmers use functional code practices to boost application performance.",
    "isOpenAccess": false,
    "externalIds": {
      "DOI": "10.61841/turcomat.v10i1.15127",
      "CorpusId": 277281907
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "React component lifecycle hooks",
    "plan_item": "React Lifecycle",
    "plan_item_id": "plan_1",
    "query_id": "q_1_0",
    "source": "semantic_scholar",
    "paperId": "0c172487d1b995c7f4d07c57a03fbbc5b18620d0",
    "url": "https://www.semanticscholar.org/paper/0c172487d1b995c7f4d07c57a03fbbc5b18620d0",
    "title": "Performance Comparison Between Development Approaches in React: Hooks, Functional and Classes",
    "year": 2020,
    "authors": [
      "Jason Aaron Ancona Reyes"
    ],
    "abstract": "Nowadays, due to constant innovations in technology, there are many available libraries to help developers in the process of designing and code visual and functional websites. Among all of them, React is currently one of the most popular in the developer community. When developers work with React, there are three common approaches used in the community. The purpose of this project is to provide a comparison between those approaches and an insight of how they perform in a real world situation. For this task, chrome developer tools was used for debugging. The three different approaches (Classes, functional and Hooks components) were tested using 4 different projects. Each project had 3 different versions. One solely relies on state management, the second one on an API response and the third one an API call and the use of Redux. After performing all tests, Hooks was the clear winner overall but still there are developers which use the other two approaches, classes approach performed better than functional but it can lead to misuse of class lifecycle which can result in a performance downgrade, that is why if possible, the use of functional when the manage of state is not required is still recommended instead of class based components for that work. React is being updated regularly and further improvement may be expected. Further studies may be needed to cover new incoming features, optimizations and improvements.",
    "isOpenAccess": false,
    "externalIds": {
      "MAG": "3107480603",
      "DOI": "10.46610/jocses.2020.v06i03.001",
      "CorpusId": 229507073
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "diffusion models for image generation survey",
    "plan_item": "Diffusion Models Review",
    "plan_item_id": "plan_diff",
    "query_id": "q_diff_0",
    "source": "duckduckgo",
    "url": "https://arxiv.org/abs/2308.13142",
    "title": "[2308.13142] A Survey of Diffusion Based Image Generation Models ...",
    "text": "Computer Science > Computer Vision and Pattern Recognition Title: A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions Submission history References & Citations Semantic Scholar BibTeX formatted citation Bibliographic and Citation Tools Code, Data and Media Associated with this Article Recommenders and Search Tools arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .",
    "extraction_method": "requests+bs4",
    "content_length": 862,
    "status": "success",
    "error_message": null
  },
  {
    "query": "diffusion models for image generation survey",
    "plan_item": "Diffusion Models Review",
    "plan_item_id": "plan_diff",
    "query_id": "q_diff_0",
    "source": "duckduckgo",
    "url": "https://link.springer.com/article/10.1007/s10462-025-11110-3",
    "title": "Comprehensive exploration of diffusion models in image generation: a survey",
    "text": "Comprehensive exploration of diffusion models in image generation: a survey Published: 25 January 2025 Volume 58 , article number 99 , ( 2025 ) Cite this article You have full access to this open access article The rapid development of deep learning technology has led to the emergence of diffusion models as a promising generative model with diverse applications. These include image generation, audio and video synthesis, molecular design, and text generation. The distinctive generation mechanism and exceptional generation quality of diffusion models have made them a valuable tool in these diverse fields. However, with the extensive deployment of diffusion models in the domain of image generation, concerns pertaining to data privacy, data security, and artistic ethics have emerged with increasing prominence. Given the accelerated pace of development in the field of diffusion models, the majority of extant surveys are deficient in two respects: firstly, they fail to encompass the latest advances in diffusion-based image synthesis; and secondly, they seldom consider the potential social implications of diffusion models. In order to address these issues, this paper presents a comprehensive survey of the most recent applications of diffusion models in the field of image generation. Furthermore, it provides an in-depth analysis of the potential social impacts that may result from their use. Firstly, this paper presents a systematic survey of the background principles and theoretical foundations of diffusion models. Subsequently, this paper provides a detailed examination of the most recent applications of diffusion models across a range of image generation subfields, including style transfer, image completion, image editing, super-resolution, and beyond. Finally, we present a comprehensive examination of these social issues, addressing data privacy concerns, such as the potential for data leakage and the implementation of protective measures during model training. We also analyse the risk of malicious exploitation of the model and the defensive strategies employed to mitigate such risks. Additionally, we examine the implications of the authenticity and originality of generated images on artistic creativity and copyright protection. Similar content being viewed by others Navigating the Realm of Generative Models: GANs, Diffusion, Limitations, and Future Prospects‚ÄîA Review Fundamentals of Encoders and Decoders in Generative AI ProCreate, Don‚Äôt Reproduce! Propulsive Energy Diffusion for Creative Generation Explore related subjects Artificial Intelligence Avoid common mistakes on your manuscript. Image generation task is one of the most important research topics in the field of artificial intelligence, which aims to generate realistic image content through algorithms and models. As shown in Fig. 1 , this task ranges from simple image restoration (Bao et al. 2024 ; Fei et al. 2023 ; et al. 2023 ) and style transfer (Huang et al. 2024 ; Li et al. 2024 ; Wang et al. 2023 ) to complex scene generation (Liu and Liu 2024 ; Scene generation with hierarchical latent diffusion models 2023 ; Yang et al. 2023 ) and human generation (Ju et al. 2023 ; Wang et al. 2024 ; Liu et al. 2024 ). The advent of sophisticated technologies, including Variational Autoencoders (VAE) (Kingma et al. 2014 ), Normalizing Flows (Papamakarios et al. 2021 ), Generative Adversarial Networks (GANs) (Goceri 2024 ), and wavelet-based augmentation methods (Goceri 2023 ), has enabled the creation of unprecedented and imaginative visual effects in image generation. Diffusion models, originally inspired by the process of diffusion in physics, have been adapted to describe the stochastic process of gradually transforming simple noise distributions into complex data distributions, such as images (Neal 2001 ; Jarzynski 1997 ). Diffusion models offer a distinctive approach to image generation, whereby the data distribution is conceptualised as a sequence of conditional distributions (Ho et al. 2020 ; Song and Ermon 2020 ; Song et al. 2021 ). In contrast to traditional generative models, such as GANs and VAEs, which sample directly from a learned latent space, diffusion models capture intricate data dependencies through the diffusion process (Yang et al. 2024 ; Luo 2022 , 2023 ). Despite their considerable successes, diffusion models remain constrained by certain limitations, including prolonged training periods, elevated computational requirements, and difficulties in scaling to high-resolution imagery (Cao et al. 2024 ; Yang et al. 2024 ; Chang et al. 2023 ). Diffusion models have recently become a prominent area of interest within the field of artificial intelligence, due to their remarkable ability to generate images of exceptional quality, which often rival those created by humans (Fan et al. 2023 ; Zhang et al. 2023 ; et al. 2024 ; Yang et al. 2024 ; Croitoru et al. 2023a ; Lin et al. 2024 ). Diffusion models have also been applied to various areas of image generation, including Style Transfer (Zhang et al. 2023 ; Wang et al. 2023 , 2024 ), Image Restoration (Luo et al. 2023 ; Qiu et al. 2023 ; Ren et al. 2023 ), Image Editing (Pang et al. 2023 ; Wang et al. 2023 ; Gu et al. 2024 ), Super-Resolution (Zhao et al. 2023 ; Wang et al. 2024 ; Gandikota et al. 2024 ), Text-to-image Generation (Nichol et al. 2022 ; Saharia et al. 2022 ; Structured prediction for efficient text-to-image generation 2024 ), and other tasks (Zeng et al. 2024 ; Zhao et al. 2024 ; Hudson et al. 2024 ). The application of diffusion modelling in the field of image generation has profoundly impacted many aspects of society, driving content innovation in industries such as advertising, gaming, film and television, reducing creation costs and increasing productivity by generating high-quality, photorealistic images. However, it also raises issues such as copyright and originality (Wang et al. 2024 ; Dubinski et al. 2024 ; Gu et al. 2024 ; Zhang et al. 2023 ), and poses challenges to traditional artistic creation and market patterns. Meanwhile, at the social and ethical level, images generated by diffusion models may be used for improper purposes such as misleading and false propaganda, posing a potential threat to public perception and information authenticity (Hu et al. 2023 ; Zhu et al. 2023 ; Linet al. 2023 ; Carlini et al. 2023 ; Ni et al. 2023 ; Seunghoo and Juhun 2024 ; Qu et al. 2023 ; Brack et al. 2023 ). In light of the above, this paper focuses on recent applications of diffusion models in the field of image generation and the socio-ethical implications of diffusion model-based image generation. The following key aspects are addressed in this survey: firstly, in-depth research has been conducted on the historical background and theoretical basis of diffusion models (Ho et al. 2020 ; Alexander et al. 2021 ; Song and Ermon 2019 ), thereby providing a solid foundation for subsequent discussions. Secondly, the practical applications of diffusion models in image generation tasks, including image inpainting (Zhang et al. 2023 ; Anciukevicius et al. 2023 ; Liu et al. 2024 ; Lee et al. 2024 ), style transfer (Huang et al. 2024 ; Li et al. 2024 ; Wang et al. 2023 ) and super-resolution (Luo et al. 2024 ; Ma et al. 2024 ; Gao et al. 2023 ; Metzger et al. 2023 ), will be investigated. Thirdly, this study examines the challenges and potential solutions to the social problems encountered by diffusion models when utilised for image generation. The aim of this work is to synthesise the extensive body of knowledge scattered across a wide range of publications, distil the key findings and present them in a coherent manner that will facilitate future research efforts. By providing a comprehensive overview of diffusion models in image generation, this survey aims to serve as a valuable resource for both novice researchers and experienced professionals wishing to deepen their knowledge or explore new avenues of research in this dynamic and evolving field. The paper is divided into seven sections to provide a structured exploration of diffusion models in image generation. The introduction (Sect. 1 ) outlines the aims and significance of the study. Sect. 2 , Related Works, places our research in the context of previous studies. Sect. 3 , Background on diffusion models, explains the theoretical basis of diffusion models. Sect. 4 , Diffusion Models in Image Generation, describes recent applications of these models. Ethical and social implications (Sect. 5 ) discusses the potential impact on society. Sect. 6 , challenges and future directions, identifies current obstacles and suggests avenues for future research. The conclusion (Sect. 7 ) summarises the main findings and contributions of the paper. In recent years, diffusion models have emerged as a promising approach to image generation. These models, inspired by the principles of non-equilibrium thermodynamics, employ an iterative process to refine the noise, ultimately resulting in the generation of coherent images. Notably, studies such as those conducted by Sohl-Dickstein et al. ( 2015 ) and Song and Ermon ( 2019 ) have illustrated the efficacy of diffusion models in producing high-quality images. The advent of diffusion models has significantly accelerated the advancement of the field of image generation. A considerable number of researchers have made significant contributions to the advancement of knowledge in the various subfields of image generation. Additionally, comprehensive survey papers on the application of diffusion generation models have also emerged, providing valuable synthesis and summarisation for the continuous progress in this field (Po et al. 2024 ; Cao et al. 2024 ; Zhang et al. 2023 ; Fan et al. 2024 ; et al. 2024 ; Yang et al. 2024 ; Li et al. 2023 ; Croitoru et al. 2023b ; Moser et al. 2024 ; Ulhaq et al. 2022 ; Fan et al. 2024 ). The objective of text-to-image (T2I) generation is to transform natural language text descriptions into corresponding visual images. This task requires the model to have both language understanding and visual representation in order to generate images that match the textual description. The previous survey literature (Cao et al. 2024 ; Zhang et al. 2023 ) provides a comprehensive review of T2I using the diffusion model, covering both the theoretical foundations and practical progress in the field. In addition to natural images, magnetic resonance imaging (MRI), as an important medical imaging modality, also offers unique application opportunities for diffusion models (Fan et al. 2024 ). Other survey literature (Cao et al. 2024 ; Yang et al. 2024 ) aims to provide a comprehensive and in-depth understanding of diffusion models, from basic formulae and algorithmic improvements to a variety of applications, revealing their development history and future trends. In the field of computer vision, there is a substantial body of survey literature (Ulhaq et al. 2022 ; Croitoru et al. 2023b ) that provides a comprehensive review of denoising diffusion models. This includes theoretical and practical contributions to the field. Additionally, there is a significant corpus of survey literature that addresses more specific visual subfields, such as image restoration and enhancement (Li et al. 2023 ), video generation ( et al. 2024 ), and image super-resolution (Moser et al. 2024 ). The field of diffusion models is undergoing a period of rapid development, with a considerable amount of new research emerging in the area of image generation. It is therefore of great importance to conduct a comprehensive literature survey on the application of the latest diffusion model in image generation. However, the majority of existing surveys are flawed in two ways. Firstly, due to time constraints, they fail to cover the latest advances in diffusion-based image generation (Ulhaq et al. 2022 ; Li et al. 2023 ). Secondly, they rarely consider the potential social impact of diffusion models in the field of image generation (Croitoru et al. 2023b ; Moser et al. 2024 ). Therefore, the aim of this study is to provide insight into the development of diffusion models in the field of image generation by conducting a large number of surveys in the field of image generation and related social ethics literature. In order to guarantee that the research is both pioneering and pertinent, it is essential to document the most recent developments and discourses within the field. Consequently, our work has concentrated primarily on diffusion model methodologies for image generation over the past three years. In addition, we have investigated the ethical implications and potential mitigating strategies associated with these approaches. The selected papers not only demonstrate significant technological advances, but also address the broader impact of these technologies on society, thereby ensuring a comprehensive analysis of the technical and ethical dimensions of diffusion models in image generation. 3 Background on diffusion models The core idea of the diffusion model comes from sequential Monte Carlo (Neal 2001 ) and non-equilibrium statistical physics (Jarzynski 1997 ), which uses a Markov chain to transform one distribution into another. The diffusion model has two main components: the forward diffusion process and the backward denoising process. The forward process gradually adds noise to the original image and eventually converts the image into a pure noise image that conforms to a Gaussian distribution. The corresponding backward process is exactly the opposite, converting a pure noise image into a realistic image that conforms to the original distribution through several steps of denoising operations. To further demonstrate the intuition of diffusion models, we will discuss the three main formulaic representations of diffusion models currently being studied: denoised diffusion probability models (Ho et al. 2020 ; Alexander et al. 2021 ), score-based generation models (Song and Ermon 2019 , 2020 ), and stochastic differential equations (Song et al. 2020 , 2021 ). The following three subsections will independently elaborate on each formulaic representation, while discussing their connections and differences. 3.1 Denoising diffusion probilistic models (DDPMs) Suppose we sample the initial data \\(x_0\\sim q (x)\\) from a real data distribution q ( x ). By using the forward diffusion process to gradually add noise to the initial data \\(x_0\\) , a series of noisy data \\(x_1,..., x_T\\) are obtained. According to the properties of Markov processes and the chain rule of probability, we can represent the joint distribution of all data as \\(q(x_1,..., x_T|x_0)\\) , The transition kernel \\(q(x_t|x_{t-1})\\) is designed manually in DDPM (Ho et al. 2020 ), and noise is gradually added to the initial data at each step of the transition. Additionally, hyperparameters \\(\\beta\\) can be set to establish a schedule for noise introduction as a variance control term. where \\(\\beta _t \\in (0,1)\\) . By leveraging the properties of the Gaussian distribution and Eq.(2), we can streamline the calculation of the forward diffusion process, thereby directly obtaining the analytical form of \\(q (x_t | x_0)\\) , As t approaches infinity and \\(\\bar{\\alpha }_t\\) approaches 0, there is \\(x_ T\\sim \\mathcal {N}(0,1)\\) . By employing initial data \\(x_0\\) and the sampled Gaussian vector \\(\\epsilon\\) , it is a straightforward process to utilise Eq.(3) to calculate sample \\(x_t\\) , A new sample can be generated from the initial distribution by starting from a sample \\(x_T\\sim \\mathcal {N}(0, I)\\) and employing the reverse denoising process. A learnable transfer kernel based on \\(x_0\\) for a reverse step can be expressed in the following form: It can be observed that the \\(x_{t-1}\\) obtained from each reverse step follows a normal distribution, with the mean \\(\\mu _q(x_t,x_0)\\) being a function of \\(x_t\\) and \\(x_0\\) , and the variance \\(\\Sigma _q(t)\\) being a function of the coefficient \\(\\alpha\\) . The coefficient \\(\\alpha\\) is fixed and known at each step. Therefore, the key to learning the transfer kernel is to fit the distribution, which is to predict the mean \\(\\mu _q(x_t,x_0)\\) and variance \\(\\Sigma _q(t)\\) , where \\(\\theta\\) is a learnable neural network parameter. During the training process, we need to make the approximate denoising transformation \\(p_{\\theta }(x_{t-1}|x_t)\\) as closely as possible to the actual denoising transformation \\(q(x_{t-1}|x_t,x_0)\\) . According to Eq.(5) and Eq.(6), it is known that their variance terms are fixed and can be accurately matched. Therefore, when using KL-divergence to calculate the difference between two distributions, we only need to consider their means. Due to the difficulty in calculating \\(p_{\\theta }(x_0)\\) , it can be processed by minimizing the variational lower bound of negative log-likelihood. For the matching of the entire trajectory, the objective function can be formulated as follows: where \\(L_0=-\\log {p_{\\theta }(x_0|x_1)}\\) , \\(L_T=KL(q(x_T|x_0)||\\pi (x_T))\\) , and \\(L_t=KL(q(x_{t-1}|x_t,x_0)||p_{\\theta }(x_{t-1}|x_t))\\) . KL () represents the KL-divergence of two distributions. Furthermore, since \\(q(x_{t-1}|x_t,x_0)\\) follows a normal distribution, we can calculate the closed form of KL-divergence. As proposed in DDPM (Ho et al. 2020 ), the optimization of the function \\(L_{vlb}\\) can be achieved by minimizing the deviation between the model‚Äôs true noise and approximate noise at random time steps in the trajectory: where \\(\\mathbb {E}\\) is the expected value calculation, \\(\\epsilon _{\\theta }(x_t, t)\\) is predict noise in step t by neural network, \\(x_0\\) is the initial data. 3.2 Score-based generative models (SGMs) The objective of an explicit generative model is to model the probability distribution of data, and then use this distribution to generate samples through sampling. Score-Based Models (Song and Ermon 2019 ) do not directly learn the probability distribution p ( x ) of the data, but rather learn score function, which is the logarithmic gradient of the probability distribution. It is defined as follows: The score function learned by Score-Based Models can be denoted as \\(s_{\\theta }(x)\\) , and by learning to approximate the logarithmic gradient of the probability distribution, \\(s_{\\theta }(x)\\approx \\nabla _x \\log {p(x)}\\) . Furthermore, \\(s_{\\theta }(x)\\) can be parameterized using an energy-based model, where \\(Z_{\\theta }=\\int e^{-f_{\\theta }(x)} \\textrm{d}x\\) is normalizing constant. This also ensures that \\(p_{\\theta }(x)\\) is a density function. Since \\(Z_{\\theta }\\) being a constant, thus \\(\\nabla _x \\log {Z_{\\theta }}\\) equals zero. Therefore, Score-Based Models are not related to normalization terms, which greatly expands the range of models or architectures that can be used. Similar to other explicit generative models, Score-Based Models can calculate the Fisher divergence between the predicted distribution of the model and the ground-truth data distribution to measure the difference between the two distributions. Then, the model is trained by minimizing Fisher divergence. Therefore, the objective function can be defined as: However, it is difficult to calculate Fisher‚Äôs divergence directly because the score of real data cannot be calculated. Fortunately, we can use the score matching method (Hyvarinen and Dayan 2005 ; Vincent 2011 ; Song et al. 2020 ) to bypass the calculation of p ( x ) and minimize divergence. The optimization objective function can be rewritten as Train the model using the above equation to make \\(s_{\\theta }(x)\\approx \\nabla _x \\log {p(x)}\\) , thus obtaining the trained Score-Based Models. Then, we can use Langevin dynamics (Parisi 1981 ; Ulf and Miller 1994 ) to iteratively generate samples from the predicted score model. The process of sampling through Langevin dynamics can be expressed as the following iterations: where \\(z_i \\sim \\mathcal {N}(0, I)\\) , \\(\\eta\\) is a sufficiently small quantity. As K approaches infinity, \\(x_K\\) converges to a sample that follows the data distribution p ( x ). Furthermore, it can be seen from the above equation that the unknown variable \\(\\nabla _x \\log {p(x)}\\) during the iterative calculation process has also been approximated through the prediction of the trained model \\(s_{\\theta }(x)\\) . We have discussed the training and sampling of Score-Based Models. However, the performance in practical applications is not ideal, mainly due to the limited amount of data used for score matching in low-density areas, which leads to inaccurate estimation of the learned model in low-density areas, thereby limiting the model‚Äôs ability of the model to generate high-quality samples. Noise Conditional Score-Based Models (NCSMs) (Song and Ermon 2020 ; Song et al. 2020 ) propose the use of multi-scale noise to perturb data points and fill low-density areas, thereby improving the accuracy of estimated scores. The distribution after noise disturbance can be expressed as where \\(\\sigma _1< \\sigma _2<...<\\sigma _L\\) are a set of increasing standard deviations. As a result, the goal of Score-Based Models also becomes to compute the score function \\(s_{\\theta }(x, i)\\) of each noise disturbance distribution. Therefore, we can use the score matching to train the parameterized NCSN. The overall optimization objective function is a weighted Fisher divergence: where \\(\\lambda (i)=\\sigma _i^2\\) is weighting term. The training process is consistent with \\(s_{\\theta }(x)\\) without disturbance. After training the model \\(s_{\\theta }(x,i)\\) , use the annealed Langevin dynamics (Song and Ermon 2019 , 2020 ; Jolicoeur-Martineau et al. 2020 ) to iteratively generate samples, where \\(i=L, L-1,..., 1\\) . 3.3 Stochastic differential equations (SDEs) As the noise scale and time step approach infinity, the noise disturbance process of Score-Based Models and the noise addition process of diffusion models (i.e. the forward diffusion process) can be summarized as a continuous time stochastic process. Many stochastic processes can be represented as solutions of stochastic differential equations (SDEs), and a stochastic process represented by SDEs can be expressed in the following form: where \\(f(\\centerdot ,t): \\mathcal {R}^d \\rightarrow \\mathcal {R}^d\\) and \\(g(t)\\in \\mathcal {R}\\) are vector-valued function and real-valued function respectively. \\(\\textrm{d}w\\) is an infinitesimal Gaussian white noise, and w is a standard Brownian motion. It should be noted that Anderson (Anderson 1982 ) proposes that there exists a corresponding reverse SDEs for any SDEs, and it has the following closed form: where \\(\\textrm{d}t\\) is an negative infinitesimal quantity of time. To compute the inverse SDEs, we need to estimate the score function of \\(p_t(x)\\) . To train \\(s_{\\theta }(x, i)\\) , the Fisher divergence under continuous time can be calculated as follows: where \\(\\mathcal {U}(0,T)\\) represents a uniform distribution over time [0, T ]. \\(\\lambda\\) is consistent with Score-Based Models and is a weight function. Specifically, DDPMs and SGMs correspond to two special SDEs, and their ways of adding noise in the forward process are different. As proposed by Song et al. (2020) (Song et al. 2020 ), the DDPM corresponds to the Variance Preserving SDE (VP SDE) and has the following form: where \\(\\beta (t)\\) is a predefined schedule function. SGMs correspond to Variance Exploring SDE (VE SDE) and have the following form: where \\(\\sigma (t)\\) represents the disturbance noise corresponding to time T approaching infinity. Both SGMs and DDPMs can be regarded as discretization of stochastic differential equations determined by fractional functions. Therefore, the score based generative model and the diffusion probability model can be summarized into a unified framework of SDEs. 3.4 Controllable generation for diffusion models The main purpose of the generative models we discussed earlier is to fit the data distribution p ( x ), to generate samples with the same distribution as the initial data. These models can be summarized as unconditional generative models. However, in practical applications, we prefer to generate samples with certain characteristics according to our ideas, and these corresponding models are called conditional controlled diffusion models. Therefore, at this point, the fitting target of the model also correspondingly becomes the conditional data distribution p ( x | y ). According to the Bayes theorem, we can express it as Similar to Score-Based Models, by taking the gradients of x on both sides of this expression, we can obtain the score function of the conditional data distribution From the above expression, it can be seen that the score function of the conditional data distribution consists of the score function of the initial data and the known forward process. Similarly, conditional controlled diffusion models can be divided into Classifier Guidance and Classifier Free based on whether existing unconditional generation models are used. For Classifier Guidance (Dhariwal and Nichol 2021 ; Liu et al. 2023 ), the conditional control generation model does not require retraining the diffusion model, and simple control can be achieved at a low cost by training a classifier. Therefore, in a conditional sampling process, the probability of state transition can be rewritten as (Dhariwal and Nichol 2021 ): where Z denotes the normalization term, \\(\\theta\\) is the diffusion model parameter, and \\(\\phi\\) is the classifier parameter. By taking the logarithm of both sides of the above equation and expanding it, we can obtain where \\(g=\\nabla _{x_t}\\log {p_{\\phi }(y|x_t)}|_{x_t=\\mu }\\) . It can be seen that compared to the unconditional transition distribution, the conditional transition distribution also follows a Gaussian distribution, and the variance \\(\\Sigma\\) is the same as the unconditional transition distribution \\(p_{\\theta }(x_{t}|x_{t+1})\\) . At the same time, the mean has a shift of \\(\\Sigma g\\) , which also includes gradient information from the classifier. By using a hyperparameter s to control the degree of classifier guidance, the sampling algorithm with classifier guidance is represented as where \\(\\mu = \\mu _{\\theta }(x_t), \\Sigma = \\Sigma _{\\theta }(x_t)\\) . In the guided diffusion model, the diffusion model was not retrained, but the guidance information of the classifier was added during the sampling process. This type of method makes the model training process cumbersome and complex, and cannot fully exploit the performance of the diffusion model. Jonathan and Salimans ( 2022 ) has improved DDPM and proposed a classifier-free guidance technique, also known as Classifier Free Guidance (CFG). Its core idea is very simple: the conditional input is fed into the training process of the diffusion model and fitted directly to the model. There is no need for a classifier. This is fundamentally different from Classifier Guidance, which did not involve the training process of dynamic diffusion models, but only guided the sampling process. In the training process of the CFG model, the conditional input y is introduced, and the input parameters for the noise prediction are \\(x_t\\) , t , and y . Therefore, the loss function becomes as follows: 3.5 Improvements of diffusion models Compared to VAEs (Higgins et al. 2017 ; Kingma et al. 2014 ; van et al. 2017 ) and GANs (Goodfellow et al. 2014 ; Arjovsky et al. 2017 ; Reed et al. 2016 ; Zhang et al. 2017 ), diffusion models theoretically require more time in the sampling process and may require thousands of evaluation steps to extract a single sample. This is because when using SDE or Markov processes to iteratively transform prior distributions into complex data distributions, a large number of function evaluations are involved in the reverse process. In addition, diffusion models also face the instability of the reverse process, as well as the computational requirements and constraint challenges required for training models in high-dimensional Euclidean space. Maximum likelihood estimation is not comparable to likelihood-based models, which is also a challenge for diffusion models. Researchers have proposed various methods to address these challenges. For example, to improve sampling efficiency, many advanced SDE solution methods have been applied to diffusion models (Lu et al. 2022 ; Zhang et al. 2023 ; et al. 2023 ; Huang et al. 2024 ; Zheng et al. 2023 ). Meanwhile, diffusion distillation can also be used, which trains a large model but uses a small model to accelerate the sampling algorithm (Salimans et al. 2022 ; Li et al. 2023 ; Wizadwongsa et al. 2023 ; Wenliang et al. 2023 ). In summary, these efficient sampling methods can be divided into two main categories: learning-free sampling and learning-based sampling, with the difference between them being whether additional learning processes are required after the training of the diffusion model has been completed (Yang et al. 2024 ; Luo 2023 ). In addition, the design of new forward processes can also be used to improve sampling stability and reduce dimensionality (Yilun et al. 2023 ; Vahdat et al. 2021 ; Rombach et al. 2022 ). When training a diffusion model, the (negative) variational lower bound (VLB) on the log-likelihood is used as the target, which may not be tight in many cases. This can lead to suboptimal log-likelihood in the diffusion model (Kingma et al. 2021 ; Maggiora et al. 2024 ). Zheng et al. ( 2023 ) proposed an improved diffusion ODE maximum likelihood estimation technique from both training and evaluation perspectives. In addition, many methods have been proposed to further maximise VLB and log-likelihood values from different aspects, including Noise Schedule Optimization (Kingma et al. 2021 ; Nichol et al. 2022 ), Reverse Variance Learning (Bao et al. 2022 ), Exact Likelihood Computation (Song et al. 2020 ; Cheng et al. 2022 ). To overcome the likelihood optimization ignored in diffusion models due to the intractability of log-likelihood, MLE training (Song et al. 2021 ; Kingma et al. 2021 ; Huang et al. 2021 ) and hybrid loss (Nichol et al. 2022 ; Cheng et al. 2022 ) are proposed to improve likelihood training. Almost all diffusion models use the Convolutional U-Net (Ronneberger et al. 2015 ) as their backbone, but Peebles and Xie ( 2023 ); Ma et al. ( 2024 ) introduced Transformer to diffusion models, further enhancing their generation capabilities and resulting in the recent high-performance video generation model Sora Liuet al. ( 2024 ). The diffusion model assumes that the data exists in Euclidean space, which initially only handles continuous data such as images. To improve performance on discrete data or other data types, Feature Space Unification (Vahdat et al. 2021 ) and Data Dependent Transition Kernels (Austin et al. 2021 ) have been proposed to extend the application scope of diffusion models. 4 Diffusion models in image generation 4.1 Style transfer The task of image style transfer has received widespread attention in the research community, to transform images of one style (source style) into another style (target style) (Gatys et al. 2015 ; Johnson et al. 2016 ; Dumoulin et al. 2017 ; Xun et al. 2017 ), as shown in Fig. 3 . This transformation can be achieved by training a model to learn the image features of the source and target styles and then generating new images based on these features. Recently, diffusion models have been introduced into this area, achieving better performance and high-fidelity style image generation (Wang et al. 2023 ; Yang et al. 2023 ; Wang et al. 2023 ; Qi et al. 2024 ). To solve the problem of preserving image content in the diffusion model, Huang et al. ( 2022 ) propose a text-driven image stylization framework based on dual diffusion to control the balance between content and style. This method integrates the multimodal style information as a guide into the step-by-step diffusion process, and performs the reverse denoising process on this basis, so that the styled results can better retain the structural information of the content image. Brack et al. ( 2022 ) propose the Stable Artist, an iterative approach to guiding the generated images to the desired output. It achieves control by allowing the artist to guide the diffusion process along a variable number of semantic directions. This semantic guidance (SEGA) provides fine-grained control over the image generation process by exploiting complex operations in the underlying space of the model, allowing for tiny edits to the image, changes in composition and style, and optimization of the overall artistic concept. Deng et al. ( 2023 ) propose a zero-shot (i.e. no training) training method through attention rearrangement, namely Z-STAR. This is a zero-shot image style conversion method that uses generative prior knowledge to transform image styles without retraining or adaptation. This approach can better solve the problem that the text prompt is too coarse to effectively express the required style details. Chung et al. ( 2023 ) propose a style transfer approach that uses large-scale pre-trained diffusion models to simply manipulate features of self-attention, replacing the key and value of content with style, without the need for optimization or supervision (such as text). This method can effectively solve the problem that the existing style transfer methods based on diffusion model need to optimize the inference stage (such as fine-tuning or style text inversion), or cannot take advantage of the generation ability of large-scale diffusion model. Zhang et al. ( 2023 ) propose a style transfer method based on inversion, namely InST. This method can effectively and accurately learn the key information of images to capture and transfer the style of painting art. This method is a good solution to the problems that specific artistic elements are difficult to transfer, the textual prompts of the target style can only be misdescribed, and it is difficult to reproduce the key ideas of specific paintings in the result. To address the high cost of fine-tuning the diffusion model or additional neural network, Yang et al. ( 2023 ) propose a zero-shot contrastive (ZeCon) loss of the diffusion model without additional fine-tuning or auxiliary network to transfer the style of a given image and preserve its semantic content in the way of zero-shot. In addition, this method not only preserves the content but also realizes texture modification. Wang et al. ( 2023 ) propose a new C-S disentanglement framework for style transfer. This framework can explicitly extract content information and implicitly learn complementary style information, which achieves interpretable and controllable C-S disentanglement and high-quality stylized results. They also further introduced the diffusion model into the C-S disentanglement framework, making the C-S disentanglement framework achieve SOTA results. Pan ( 2023 ) propose an innovative style guidance approach that can improve the existing text-to-image diffusion model, while also supporting the use of reference images to guide arbitrary styles of generated images. This method optimizes the style guidance function to reduce the influence of noise input and maximize the guidance efficiency. As a result, the supervised style guidance and self-style guidance achieve effective results in generating the desired style images while maintaining a high correlation between the generated images and the text input. Chen et al. ( 2023 ) propose a model called ArtFusion, which aims to provide a flexible balance between content and style for AST. The model utilizes a dual-condition latent diffusion probability model, which breaks the limitation of paired data in cDM training and promoting the progress of other multi-condition generation tasks. During the model training phase, the model transforms the style transfer task into a self-reconstruction task while maintaining robust stylization ability during inference. Wang et al. ( 2024 ) propose a novel AST method called Highly Controllable Arbitrary Style Transfer (HiAST) to address the demand for flexible and customized stylized results. This model introduces a Style Adapter that allows users to flexibly manipulate the output stylized results by aligning multi-level style information and intrinsic knowledge in LDM. 4.2 Image restoration Image Restoration (IR) is a long-standing problem due to its broad applicability and ill-defined nature. The goal of IR is to recover a high-quality (HQ) image from its low-quality (LQ) counterpart, which has been damaged by various degradation factors (e.g., blur, mask, downsampling), as shown in Fig. 4 . The role of different image restoration methods is shown below: Lugmayr et al. ( 2022 ) propose a redrawing method based on the denoising diffusion probability model (DDPM) that does not require specific mask training. The reverse diffusion iteration is modified by sampling the unmasked region with the given image information. Instead of learning the mask condition generation model, this model samples the condition generation process from a given pixel in the reverse diffusion iteration and is not trained for the internal spray-painting task itself. Luo et al. ( 2023 ) propose to use a latent diffusion model to achieve realistic image recovery at large scale. A U-net-based latent diffusion strategy is proposed, which allows image restoration to be performed in a compressed and low-resolution latent space, thus speeding up training and inference. Lin et al. ( 2023 ) propose DiffBIR, which applies a pre-trained text-to-image diffusion model to the problem of blind image restoration. This method outperforms state-of-the-art approaches in blind image super-resolution and face restoration tasks on both synthetic and real-world datasets. Furthermore, DiffBIR can effectively handle severe degradation and restore both realistic and vivid semantic content. Qiu et al. ( 2023 ) propose a bootstrap diffusion model, DiffBFR, for blind face recovery. DiffBRF effectively employs the diffusion model to solve the problem of blind face recovery, which not only reduces the training difficulty and training time of the whole model but also provides a less degraded input truncated sampling module with severe conditions. Moreover, this method outperforms GANs in terms of avoiding training collapse and generating long-tailed distributions. Ren et al. ( 2023 ) introduce a simple and effective multiscale structure guide as an implicit bias to inform the icDPM about the coarse structure of sharp images in the middle layer. This guide leads to significant improvements in deblurring results, especially in invisible regions. The model can recover clean images more accurately and effectively. Wang et al. ( 2023 ) propose a Coarse-to-Fine Diffusion Transformer (C2F-DFT). The C2F-DFT is built from a diffusion transformer block containing Diffusion Self-Attention (DFSA) and Diffusion Feedforward Network (DFN).C2F-DFT can well embed diffusion in transformers, allowing them not only to model long dependencies, but also to take full advantage of the generative power of diffusion models to facilitate better image recovery. Liu et al. ( 2023 ) propose a residual denoising diffusion model (RDDM). It decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. Residual diffusion represents directional diffusion from the target image to the degraded input image and explicitly guides the reverse generation process of image recovery, while noise diffusion represents random disturbances in the diffusion process. RDDM can solve the uninterpretability problem of a single denoising process and unify different tasks that require different levels of certainty or diversity. Chen et al. ( 2024 ) propose a hierarchical integrated diffusion model (HI-Diff) for real-world image deblurring. HI-Diff exploits the power of diffusion models to generate informative priors for better results and generalization in complex blurred scenes. 4.3 Image editing As shown in Fig. 5 , image editing is a technology that modifies, enhances, or synthesizes images, which can be used in a variety of application scenarios, such as artistic creation, entertainment, education, medical treatment, etc. The goal of image editing is to produce images of high quality, variety, and control while maintaining the naturalness and semantic correctness of the images. In recent years, image editing methods based on generative models have made remarkable progress, especially those based on diffusion models. Pang et al. ( 2023 ) propose a new initialization method called cross initialization, which can significantly reduce the gap between initial and learned embeddings, thereby improving the reconstruction quality and editability of images. It also introduces a regularization term to make the learned embedding stay close to the initial one, further improving editability. Choi et al. ( 2023 ) propose a new editing method called Custom-Edit, which consists of two steps: (i) using a small number of reference images to customize the diffusion model, and (ii) using effective text-guided editing methods to edit the images. They found that customizing only language-related parameters and using enhanced text prompts significantly improved the similarity of the reference image while maintaining the similarity of the source image. Li et al. ( 2023 ) propose two novel subject-driven image editing subtasks, namely subject replacement and subject addition, which can realize more refined and flexible image editing. A new iterative generation method, called DreamEditor, is designed to achieve high-quality theme replacement and addition by gradually adapting to changes from the source theme to the target theme. Wang et al. ( 2023 ) propose a general framework called MDP, which can explain various operations suitable for editing in diffusion models. They found five different operations, including intermediate latent variables, conditional embeddings, cross-attention graphs, guided and predictive noise, and analyzed the parameters and operation plans corresponding to these operations. They also demonstrated a new control method that can achieve higher-quality local and global editing than previous work by manipulating predictive noise. Huang et al. ( 2023 ) propose a new sampling method called KV Inversion, which enables high-quality image editing without the need for fine-tuning. This method can change the action of the object in the image according to the textual prompt while maintaining the texture and identity of the object in the image. The advantage of KV inversion is that there is no need to train the diffusion model itself, nor does it need to scan large data sets for time-consuming training, but only to use the pre-trained diffusion model and text encoder to achieve image action editing. Lin et al. ( 2023 ) propose an image editing method based on learnable regions that can specify the editing target and content through text prompts without requiring the user to provide masks or sketches. This method uses a pre-trained text-to-image model and introduces a bounding box generator to find the editing region aligned with the text prompt. Therefore, the mask-based text-to-image model can perform local image editing without masks or other user-provided guidance. Aiming at the problem of reconstruction failure in real image editing, Chen et al. ( 2023 ) propose three sampling methods: FEC-ref, FEC-noise, and FEC-kv-reuse for different editing types and settings. The goal of these three methods is to ensure the success of reconstruction, that is, the result of sampling can retain the texture and features of the original real image, and cooperate with multiple editing methods to improve the performance of the editing task. All three methods do not require fine-tuning of diffusion models or training on large datasets, thus saving time and computational resources. Nguyen et al. ( 2023 ) propose a new image editing method, Visual Instruction Inversion, which can guide the text-to-image diffusion model through visual prompts. A new visual cue generator is designed to generate a short text instruction to describe the changes between images based on the given source image and target image. Kim et al. ( 2023 ) propose a simple and effective way to make the process of writing text prompts more user-friendly by incorporating a text generation framework. Specifically, they first classify text prompts into three categories based on the level of semantic detail: simple, medium, and complex. They then use existing text generation frameworks, such as T5 (Raffel et al. 2020 ) and DALL-E (Ramesh et al. 2021 ), to generate medium or complex text prompts based on the target words entered by the user. Dong et al. ( 2023 ) propose a method for image editing by text prompt. Given an original image and a target text prompt, the goal is to generate an edited image that is similar to the original image but conforms to the text prompt. The method uses a pre-trained text-to-image diffusion model and achieves editing through the technique of prompting adjustment inversion. It can realize flexible and diverse image editing without losing the details and quality of the original image. Gu et al. ( 2024 ) propose a novel approach to an immersive image editing experience through personalized subject swapping. Photoswap first learns the visual concept of the subject from the reference image and then replaces it untrained into the target image using a pre-trained diffusion model. The effectiveness and controllability of Photoswap in personalized subject replacement show its wide application potential in entertainment and professional editing. Han et al. ( 2024 ) propose a new editing method called Proximal Negative-Prompt Inversion (ProxNPI), It is an extension of the concepts of Negative-prompt inversion (NPI) Miyake et al. ( 2023 ) and Null-text inversion (NTI) (Mokady et al. 2023 ). ProxNPI reduces artifacts by introducing a regularized term and reconstructing the boot while preserving the untrained property. 4.4 Super-resolution As shown in Fig. 6 , the super-resolution task aims to enhance a low-resolution image or video to a high resolution through algorithms and models, while preserving and recovering as much detail information as possible in the image and reducing blurring and distortion. Saharia et al. ( 2022 ) introduce a method called SR3 for image super-resolution. This method aims to achieve efficient and realistic image super-resolution processing by combining a denoising diffusion probability model and a U-Net model. The innovation of SR3 lies in the use of the diffusion probability model and the implementation of the reverse process using the U-Net architecture, effectively avoiding the complex intrinsic optimization problems of traditional autoregressive models. Li et al. ( 2022 ) investigate a single image super-resolution method based on diffusion models - SRDiff. This method adopts the U-Net structure and achieves a stable and reliable training process by introducing multi-scale skip connections and diffusion models, thus generating high-quality and diverse super-resolution images. SRDiff performs excellently on various datasets, avoiding the excessive smoothness and mode collapse problems of traditional methods, while also supporting flexible image operations. Zhao et al. ( 2023 ) introduce a novel image super-resolution method based on diffusion probability models - Partial Diffusion Models (PartDiff). This method gradually diffuses the low-resolution input image into an intermediate latent state of the high-resolution image and performs partial denoising operations, to achieve high-quality image super-resolution. PartDiff achieves good performance on both magnetic resonance imaging and natural images. The Diffusion Rectification and Estimation-Adaptive Models (DREAM) framework, proposed by Zhou et al. ( 2023 ), aims to address the problem of training and sampling inconsistency in conditional diffusion models for super-resolution tasks. By introducing two key components, diffusion rectification, and estimation adaptation, the DREAM framework effectively improves the quality of generated images while accelerating the training convergence speed and sampling efficiency. Wang et al. ( 2024 ) propose a simple and effective single-step SR generation method, SinSR, to overcome the limitation of the number of inferences faced by recent methods to improve the inference speed. This method reduces the number of inference steps for mapping between random noise at training time and generating high-resolution images by deriving deterministic sampling and proves that this deterministic mapping can use only one inference step to perform SR‚Äôs student model. Gandikota et al. ( 2024 ) introduce the zero-shot text guidance problem of an open-domain image super-resolution solution. This approach allows users to explore diverse, semantically accurate reconstructions to maintain data consistency with low-resolution inputs with different large downsampling factors, without the need for explicit training for these specific downgrades. To eliminate the problem of artifacts in the iterative process of traditional diffusion-based SR techniques,Qingping et al. ( 2024 ) propose a training-free method, Adaptive Reality-Guided Diffusion (SARGD), which can effectively identify and mitigate the propagation of artifacts in latent spaces. The method first uses an artifact detector to identify untrustworthy pixels to create a binary mask that highlights artifacts and then uses Reality Guided Optimization (RGR) to integrate this mask with a realistic latent representation to optimize artifacts to improve alignment with the original image. 4.5 Text-to-image generation As shown in Fig. 7 , the text-to-image generation task refers to the process of automatically converting the input text description into the corresponding visual image through natural language processing technology, aiming to achieve seamless conversion and fusion between language and image ( M√ºller et al. 2013 ; Yarom et al. 2023 ; Zhang et al. 2023 ). With the current development of deep learning technology (LeCun et al. 2015 ; Bengio et al. 2021 ), especially diffusion models, the quality of generated images has been greatly improved, and text-to-image generation has become the most attractive application in the field of computer vision (Reed et al. 2016 ; et al. 2018 ; Yu et al. 2022 ; Nichol et al. 2022 ; Jiayi et al. 2024 ). Nichol et al. ( 2022 ) explored the application of guided diffusion to textual conditional image synthesis problems and compared two different guidance strategies: CLIP guidance (Radford et al. 2021 ) and classifier-free guidance (Jonathan and Salimans 2022 ). This method is the first attempt to apply a diffusion model to text-to-image generation, and it intuitively replaces the class labels in the class-conditioned diffusion model (i.e., ADM (Dhariwal et al. 2021 )) with text, so that the sample generation is limited by text conditions. Compared to CLIP guidance, classifier-free guidance is preferred by human evaluators in terms of realism and text similarity. Saharia et al. ( 2022 ) proposed Imagen, which abandons the cumbersome steps of GLIDE that require pre-trained visual-language models, and directly uses large language models such as T5 (Raffel et al. 2020 ) as text encoders, combined with diffusion models, to complete the direct association mapping from text to images. More importantly, they found that the general large language model is a very efficient text encoder for text-to-image generation, and increasing the size of the text encoder can effectively improve the quality of the generated samples and the alignment between text and image compared to increasing the size of the image diffusion model. In addition, the latest Imagen 3 (Jason et al. 2024 ), based on the latent diffusion model (Rombach et al. 2022 ), not only greatly improves text-image alignment to produce high-quality images, but also discusses security and representation issues. Ramesh et al. ( 2022 ) proposes a two-stage model, in which a text is first given, an image embedding similar to CLIP is generated by the prior model, and then the image is generated by a decoder under the condition of image embedding. Both the prior and the decoder use the diffusion model. This method develops a method for training the diffusion prior in latent space and shows that it has comparable performance to the autoregressive Prior but with higher computational efficiency. [13] proposed DALL-E3 to address the difficulty of text-to-image models in following detailed image descriptions. By training the highly descriptive generated image titles, this method can greatly improve the prompt following ability of the text-to-image model. In addition, diffusion-based text-to-image technologies, such as Stable Diffusion and Midjourney [1], show great potential for commercial applications. These models can transform simple text descriptions into high-quality images, greatly accelerating up the content creation process. To solve the problem of high computational cost required by modern text-to-image models to generate high quality images, Jayasumana et al. propose a lightweight method Structured prediction for efficient text-to-image generation ( 2024 ) to optimize image region compatibility, reduce computational cost, and improve image quality by using Markov Random Field (MRF). Based on the Muse latent marker text-to-image model, the MarkovGen model combined with MRF speeds up the generation process and reduces artifacts to improve image quality. Due to the shortcomings of the current diffusion model used in image generation to identify abstract continuous attributes, Cheng et al. ( 2024 ) propose a continuous 3D words technology to realize the fine control of multiple attributes in the image by users of the text-to-image model and realize efficient and burden-free image generation adjustment. Inspired by the success of reinforcement learning with human feedback (RLHF) in large language models, Liang et al. ( 2024 ) proposed a variety of ways to enrich human feedback information and train a multimodal transformer to automatically predict these feedbacks to improve image generation. A considerable number of notable works have employed diffusion models to address a range of subtasks associated with image generation. This paper will examine these applications and contributions in detail. Low-light Image Enhancement Shang et al. ( 2024 ) propose a multi-domain multi-scale (MDMS) diffusion model for low-light image enhancement to address the limitations of the diffusion model and thus improve the quality of the generated images. Yi et al. ( 2023 ) proposed a physically interpretable diffusion model for low-light image enhancement, and solved various degradation problems in the image generation process by designing a multipath generative diffusion network, including noise, color bias, and dark illumination. Image Denoising Zeng et al. ( 2024 ) introduce the diffusion model into the hyperspectral image denoising scene, and proposed a method Diff-Unmix that uses the denoising diffusion model to perform self-supervised denoising, which solves the problem that the current supervised denoising method is limited by the dataset. Camouflaged Vision Perception Fan et al. ( 2022 , 2023 ). To solve the problem that the current camouflage image generation methods require humans to specify the background and lead to high cost, Zhao et al. ( 2024 ) propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) to generate camouflage images, to expand the diversity of camouflage image samples at low cost. Medical Generative Modeling Chenlu et al. ( 2024 ) unify the medical generation task and the unified generation task of the medical model, and proposed to align, extract and generate the multimodal medical model MedM2G in the unified model, which greatly enhanced the comprehensive diagnostic ability of the multimodal medical model. Monocular Depth Estimation Ke et al. ( 2024 ) explore the role of the diffusion model‚Äôs ability to capture a wide range of priors in the depth estimation task, and proposed an affine-invariant monocular depth estimation method based on the use of stable diffusion to retain prior knowledge, which improved the model‚Äôs ability to understand new scenarios. Patni et al. ( 2024 ) explored the use of a global prior of images generated by a pre-trained ViT-based diffusion model to provide richer contextual information. Representation Learning Hudson et al. ( 2024 ) propose a self-supervised diffusion model for representation learning. This method uses the diffusion model as a powerful representation learner to realize semantic learning in an unsupervised manner, and clarifies the potential of the diffusion model for learning rich representations. Texture Synthesis Yeh et al. ( 2024 ) introduce a new image-guided texture synthesis method based on the diffusion model, which overcomes the limitations of traditional methods by using densely sampled views and precisely aligned geometric images, and greatly improves the quality of texture generation. Causal Attribution Asnani et al. ( 2024 ) propose a causal attribution technique, ProMark, that allows the images generated by the generative model to be attributed to the model‚Äôs training data, such as images, objects, artists, etc.. This approach can transform creative workflows by enabling creators to generate relevant content for model training in order to earn rewards. Image Classification Wang et al. ( 2024 ) propose a new inter-class data augmentation method, Diff-Mix, which can generate images that conform to the diversity of foreground objects and backgrounds for specific concepts, thereby improving image classification performance. Image Morphing (Wolberg 1998 ) Zhang et al. ( 2024 ) propose DiffMorpher, a method for smoothing and interpolating images using prior knowledge of pre-trained diffusion models. This method uses two LoRA (Hu et al. 2022 ) fitting images to capture the semantics, and then interpolates between the LoRA parameters and the latent noise to achieve a smooth transition of the semantics, which solves the problem that the latent space of the current diffusion model is highly unstructured. Image Inpainting Liu et al. ( 2024 ) explore the problem of semantic differences between masked and unmasked regions in the diffusion model for image inpainting, and proposed StrDiffusion, an image inpainting diffusion model that reconstructs texture denoising under structural guidance. Scene Completion Nunes et al. ( 2024 ) explore the application of the diffusion model in 3D point cloud generation, and instead of the previous work directly using image-based diffusion methods Lee et al. ( 2023 ); Luo and Wei ( 2021 ); Lyu et al. ( 2022 ), it proposes to operate directly on the points, and redesigns the forward and backward processes of the diffusion model to make it work effectively within the 3D point cloud scene. Intrinsic Image Decomposition The ambiguity between illumination and material properties, as well as the lack of real-world data sets, make the appearance decomposition task quite challenging. Kocsis et al. ( 2024 )use the powerful prior knowledge of the latest diffusion models to sample from the solution space of the conditional generation model, which greatly improves the generalization of the model to the real image. Image Deblurring To construct an efficient training dataset based on the generated realistic blurred images, et al. ( 2024 ) propose reBLurring AUgmentation (ID-Blau), which uses clear images and controllable blur conditions to pair to generate corresponding blurred images, to realize the diversified generation of blurred images. From the above discussion, it can be seen that diffusion models are widely used in all corners of the field of image generation, and the above discussion is only a part of the application of tasks. In addition, the diffusion model is also applied to Image Rectangling (Zhou et al. 2024 ), Image Segmentation (Baranchuk et al. 2022 ; et al. 2023 ), Semantic Matching (Li et al. 2024 ), Visual Emotion Analysis (Yang et al. 2024 ), Face Recognition (Boutros et al. 2023 ), Anomaly Detection (Zhang et al. 2023 ), etc. 5 Ethical and social implications As technology progresses, it is imperative to contemplate the harmonization of artistic innovation with ethical responsibility. For instance, in the case of diffusion models used for image generation, it is imperative to adhere to the principles of copyright and refrain from encroaching upon the intellectual property rights of others. Concurrently, it is imperative to consider the influence of technology on societal culture and values in order to attain sustainable and responsible technological advancement. The generation of images by diffusion models has the potential to result in the dissemination of misleading information. Such occurrences have the potential to influence public perception and decision-making processes. It is, therefore, incumbent upon developers and users to ensure that generated images are accompanied by clear labels and instructions, in order to obviate any potential for misdirection on the part of the user. 5.1 User privacy data leakage Although diffusion model-based image generation technology has shown great potential in the fields of creativity, entertainment, and design, the problem of user privacy leakage cannot be ignored. If this technology is used improperly, it may unknowingly leak sensitive information such as personal identity and living habits by collecting and analyzing image data uploaded by users, posing a serious threat to individual privacy (Hu et al. 2023 ; Zhu et al. 2023 ; Linet al. 2023 ; Carlini et al. 2023 ; Ni et al. 2023 ).In addition, privacy leakage may also lead to public distrust of new technologies, hinder the healthy development of technological innovation and application, and affect the reputation and prospects of the entire industry. Therefore, strengthening data protection and ensuring users‚Äô privacy security are key issues that must be taken seriously in the promotion and application of such technologies. Carlini et al. ( 2023 ) study the memory capacity of image diffusion models (such as stable diffusion) in the training data and the privacy problems it causes. By proposing a new data extraction method, the paper successfully extracts a large number of training examples from the diffusion model and shows how to identify these memory-generating samples by generating and filtering them. The researchers also thoroughly analyzed the effects of different types of models and parameter settings on the degree of privacy leakage, and found that the diffusion model has a higher risk of privacy leakage than other generative models. Zhu et al. ( 2023 ) investigate the privacy leakage problem of diffusion model in image generation, propose a reconstruction-based member inference attack method, and conduct experimental verification on several pre-trained diffusion models. This method realizes member inference by reconstructing images and calculating reconstruction errors. Compared with traditional gradient-based attack methods, this method has higher efficiency and stability, and is more difficult to defend. The experimental results show that the attack method can effectively infer the information of the members in the training dataset, and reveal the potential risks of the diffusion model in privacy protection. Currently, many researchers have also conducted in-depth research and proposed solutions. Hu et al. ( 2023 ) explores the effectiveness of differential privacy technology as a potential defense measure, and points out that future research should focus on how to strike a balance between protecting privacy and improving model quality. Linet al. ( 2023 ) explores a solution for generating privacy-protected synthetic data via API interfaces using generative adversarial networks such as Stable Diffusion. They propose a framework called Private Evolution (PE), which uses evolutionary algorithms and models supported by existing APIs to generate synthetic data similar to the distribution of private data through an iterative process that ensures differential privacy protection. Ni et al. ( 2023 ) proposes a new method called Degeneration-Tuning (DT). The core idea of the method is to create a degraded dataset by disrupting low-frequency visual content and retuning the stable diffusion model to mask unwanted concepts when generating images, thus protecting certain concepts from attacks or leaks. 5.2 Copyright issues The rapid development of image generation technology based on diffusion models has greatly enriched the opportunities for creative expression and visual content production, but it also raises serious copyright issues (Wang et al. 2024 ; Dubinski et al. 2024 ). These technologies can imitate or even create original works with high fidelity, making copyright ownership ambiguous and affecting the protection of rights for original creators (Gu et al. 2024 ; Zhang et al. 2023 ). Unauthorized use or reproduction of other people‚Äôs work styles for creation may infringe the copyrights of original creators, dampen their enthusiasm for creativity and hinder the healthy development of cultural industries. At the same time, it also increases the difficulty of copyright supervision and rights protection, and poses new challenges to the legal system. In this regard, the solutions not only include improving laws and regulations, strengthening industry self-discipline, improving public awareness of copyright, and exploring new copyright protection modes, but researchers have also explored a large number of technical solutions from the technical aspect. For the copyright challenges of the diffusion model, Gandikota et al. ( 2024 ) proposes a method called Unified Concept Editing (UCE), which precisely edits models through closed-form solutions to eliminate copyrighted content, correct bias, and control inappropriate concepts. Kumari et al. ( 2023 ) propose a concept ablation method based on a diffusion model, including two schemes based on noise and anchor point, which avoid generating target concepts by adjusting the KL divergence between distributions. This method can effectively remove the target concept while retaining the related concept, which proves its practicality in copyright protection and privacy enhancement. Zhang et al. ( 2023 ) addressed a part of the infringement problem, namely the generation of infringing content using queries not directly related to the copyrighted subject matter. The authors develop a data generation pipeline for generating copyright investigation datasets for diffusion models, and through this pipeline generate datasets containing infringement examples for different diffusion models. Casper et al. ( 2023 ) proposed and implemented a simple and quantitative method to measure the performance of the model when imitating a particular artist by combining CLIP encoders and standard techniques. The results of the experiment showed that Stable Diffusion was able to successfully mimic the style of most professional digital artists, a finding that has important implications for addressing the correlation between AI-generated images and copyright law. In addition, the paper discusses how image classification techniques can be used to analyze legal claims and test defense strategies against AI imitations of copyrighted works. In addition, for the commercial application of the diffusion model, researchers also proposed an evaluation framework and an attack method to evaluate the copyright security of the model. Wang et al. ( 2024 ) proposed a data pollution attack method called SilentBadDiffusion. This method uses multimodal large language models and text-guided image-filling techniques to generate images with specific prompts, and then injects this ‚Äúpoisoned‚Äù data into the training process of the diffusion model. The experimental results show that only a small amount of poisoning data is needed to enable the fine-tuned diffusion model to generate infringing content under specific trigger prompts. Dubinski et al. ( 2024 ) propose a new evaluation framework and attack method to evaluate the effectiveness of members‚Äô inferred attacks. By designing a new dataset, LAION-mi, the authors find that previous evaluation schemes fail to fully reflect the true impact of members‚Äô inferred attacks, revealing the serious privacy and copyright issues that large diffusion models face when processing copyrighted images. At the same time, this paper also highlights the challenges of shadow model attacks, such as high computational cost and difficult sampling. 5.3 Bias and fairness problem The rapid development of the diffusion model in the field of image generation has greatly enriched the possibilities of content creation and visual presentation, but has also brought with it problems of prejudice and fairness that cannot be ignored (Luccioni et al. 2023 ; Naik and Nushi 2023 ; Jiang et al. 2023 ; De Simone et al. 2023 ). These issues can not only exacerbate existing inequalities in society, such as the automatic reproduction and propagation of gender, racial, or cultural stereotypes, but also limit the diversity of innovation and affect the inclusiveness and ethical acceptability of technology. In the long run, without effective governance, it will hinder the healthy development of technology, damage public trust, and pose a threat to the cultural diversity and inclusiveness of society. Bansal et al. ( 2022 ) study the effect of moral natural language interventions on text-to-image generation models, specifically the performance of the stable diffusion model. Using the ENTIGEN dataset, the authors evaluated the impact of ethical interventions on image-generating diversity across three social axes: gender, skin color, and culture. Naik and Nushi ( 2023 ) aim to systematically investigate and quantify the social biases in text-to-image generation models. Using the stable diffusion model as an example, this paper examines its performance in terms of gender, race, age, and geographic location. By designing a series of experiments, including the use of different prompt words and automated and human scoring methods, the study found that the Stable Diffusion model has a significant bias in image generation. Luccioni et al. ( 2023 ) aim to explore the problem of social bias in machine learning driven text-to-image (TTI) systems, specifically for the performance of stable diffusion models. By proposing an evaluation method based on social attributes, combined with an analysis of occupational and social attributes, this paper reveals the gender and racial biases that exist in TTI systems when generating images. Friedrich et al. ( 2023 ) mainly discuss the bias problem of artificial intelligence in text-to-image generation and proposes a new strategy called Fair Diffusion. It is designed to reduce or eliminate bias by controlling the direction and proportion of model output. The research focuses on the Stable Diffusion model, and reveals the model‚Äôs gender bias in image generation through a series of experiments. Schramowski et al. ( 2023 ) focus on solving the problems of bias and misbehavior in real-world applications of image generation models under textual conditions. Specifically for stable diffusion models. By introducing a ‚Äúsecure latent diffusion‚Äù (SLD) approach, the paper aims to filter and balance the training data to eliminate or suppress inappropriate parts of the image. Shen et al. ( 2023 ) focus on solving the fairness problem of text-to-image diffusion models, especially the bias of stable diffusion models. By introducing the distribution alignment loss function and fine-tuning the sampling process, this paper aims to control the distribution of certain attributes of the generated image to achieve fairness and diversity. Jiang et al. ( 2023 ) aim to correct for racial stereotyping in image generation models such as stable diffusion. By introducing a framework called RS-corrector, this method adjusts the hidden code in latent space to eliminate racial bias while maintaining the integrity of the original model. Li et al. ( 2023 ) focus on solving the fairness problem in text-to-image diffusion models, especially for the bias that can arise when generating human-related descriptions. To this end, the authors propose the Fair Mapping method, which is a lightweight, general-purpose solution that does not depend on a specific model. With well-designed prompts to control sensitive attributes and adjust offsets in embedded spaces to correct semantic features of the original language, Fair Mapping aims to achieve fairer image generation. De Simone et al. ( 2023 ) aim to solve the problem of bias in the generation of text-to-image (GTTI) models by designing and implementing a tool called the fair diffusion model to improve the fairness and transparency of the model. Through the interactive interface and editing options, the tool allows users to analyze and adjust the worldview of the model to ensure that the generated image meets the fairness standards expected by users. Gandikota et al. ( 2024 ) mainly study various security problems in text-to-image models and propose a solution called ‚ÄúUnified concept editing (UCE)‚Äù. The method uses a closed-form solution to accurately edit the model and supports simultaneous processing of multiple concepts such as bias, copyright, and offensive content without retraining the model. UCE enables targeted bias correction for multiple attributes while removing potentially copyrighted content and controlling inappropriate concepts. 5.4 Inappropriate image generation The problem of inappropriate image generation faced by image generation technology based on diffusion models cannot be ignored. This problem may not only lead to the generation of violent, pornographic, or discriminatory images, which seriously violate social ethics, laws, and regulations, but also mislead the public and affect the healthy dissemination of information (Seunghoo and Juhun 2024 ; Qu et al. 2023 ; Brack et al. 2023 , 2023 ; Rando et al. 2022 ). In addition, it can damage personal reputation and privacy, and aggravate the sense of insecurity and distrust in cyberspace. Therefore, how to effectively identify and prevent the generation of inappropriate images has become a key challenge to be solved in this field. Rando et al. ( 2022 ) mainly study the security problem of the stable diffusion model in natural language processing tasks. Through the simulation of attacker behavior and reverse engineering analysis, the vulnerabilities of the model‚Äôs security filter are revealed and the corresponding improvement measures are proposed. The researchers generated several types of images to test the performance of the security filter and successfully bypassed the filter to generate images with pornographic content. The conclusion points out that there are loopholes in the security filter of the current stable diffusion model, and it is necessary to strengthen the security through open documents and disclosure channels. Chin et al. ( 2023 ) propose an automated tool called Prompting4Debugging (P4D) to detect security vulnerabilities that can lead to inappropriate image generation by optimizing prompt words. P4D uses prompt engineering technology to find modified prompts that bypass security mechanisms through continuous/soft embedding optimization and discrete/hard embedding projection. The experiments show that even seemingly secure prompts can be vulnerable to manipulation, underscoring the importance of fully testing the security of T2I diffusion models. Qu et al. ( 2023 ) aim to comprehensively study the potential risks of text-to-image models in generating unsafe images and hateful messages. By constructing five kinds of unsafe image classification systems and using four advanced text-to-image models, it is found that these models have the risk of generating unsafe images, and the stable diffusion model is particularly prominent. Brack et al. ( 2023 ) conduct an in-depth study of the security of image generation models under text conditions in the application. The goal of the study was to uncover systemic security issues in existing imaging models and to assess the impact of counterattacks. Pham et al. ( 2023 ) conduct an in-depth study of concept erasure methods in text-to-image generation models. Seven different concept erasure methods are described in detail and their effects on pre-trained diffusion models are shown. The experimental results show that these concept erasure methods do not eliminate sensitive concepts, but reintroduce the ‚Äúerased‚Äù concepts by adjusting the embedding of input words. Zhang et al. ( 2023 ) study the problem of text inversion for concept censorship in the text-to-image generation model and proposed a solution based on backdoor technology. By selecting sensitive words as triggers during training and using these triggers in combination with personalized embedding in the generation phase, the model outputs predefined target images instead of images containing malicious concepts. Experimental results show that this method can effectively prevent the cooperation between text inversion technology and censored words, while maintaining the original function of the model. Brack et al. ( 2023 ) focus on the problem of text-to-image models in generating inappropriate content and propose two solutions: negative cuing and semantic guidance (SEGA). The study aims to make the images generated by the model consistent with human preferences by evaluating and guiding strategies. In practice, negative cuing reduces the generation of inappropriate content by avoiding specific cues, while SEGA manipulates the image generation process by adding additional cues while minimizing changes to the original image. These guidance methods can effectively reduce the probability of generating inappropriate content, and SEGA performs better. Zhang et al. ( 2023 ) focus on solving the privacy, copyright, and security problems in text-to-image generation models. In particular, the models can learn and generate unauthorized personal information, content, and potentially harmful content. To this end, the authors propose an efficient and cost-effective solution called ‚Äúselflessness,‚Äù which aims to remove a particular identity, object, or style from the model without affecting the model‚Äôs ability to generate other content. Heng et al. ( 2024 ) propose Selective Amnesia (SA) to solve the problem of selective forgetting in deep generation models. Combining Bayesian Continual Learning (BCL), the method integrates Elastic Weight Consolidation (EWC) and Generative Replay (GR) into a training loss function. It allows forgetting of specific concepts without access to the original training data set. This research provides a new solution to the problem that large text-to-image models can generate harmful, misleading, and inappropriate content. Seunghoo and Juhun ( 2024 ) propose a new algorithm called ‚ÄúConcept Eraser‚Äù. This method achieves the goal of removing or replacing specific concepts in the pre-trained model by modifying the drift of the classifier guide term and the unconditional score term. This algorithm can not only effectively erase the object concept, but also maintain the generative ability of the model. The proliferation of fake images, with their deceptive nature, has the potential to mislead the public, distort the truth, and contribute to misunderstanding and panic. It can harm trust, disrupt social stability, and have negative effects on individual reputation and mental health (Shen et al. 2019 ; Nash et al. 2009 ). Researchers have developed a number of detection and identification methods to address the threat of fake images. Among these, the method combining generative adversarial networks (GANs) and convolutional neural networks (CNNs) has demonstrated particularly promising results (Neves et al. 2020 ; Raza et al. 2024 ; Bhandari et al. 2023 ). The advent of diffusion models in image generation has introduced new challenges to the authenticity and integrity of digital images. Qiang et al. ( 2023 ) conducted a comprehensive investigation into the collection mechanism of images generated by diffusion models and developed a hybrid neural network model that integrates attention-guided feature extraction (AGFE) and vision transformers (ViTs) based feature extraction (ViTFE) modules to enhance the representation of fake trace features. Raza et al. ( 2024 ) have proposed an innovative framework, designated as Multi-Model GAN Guard (MMGANGuard), which employs transfer learning and multi-model fusion techniques to facilitate the automated identification of PAN-generated fake images, thereby enhancing the precision and scalability of the detection process. Tassone et al. ( 2024 ) have proposed an in-depth analysis of the application of two continual learning techniques in addressing the generalization challenges faced by deepfake detection technology. This research involves a comprehensive examination of continual learning techniques for both short and long sequence fake media. The efficacy of existing AI-generated image detection methods is contingent upon the availability of extensive training data, which often proves challenging to obtain when the number of samples is limited. et al. ( 2023 ) developed FAMSeC, a novel AI-generated image detection method that aims to train a general-purpose detector using a limited number of training samples while avoiding overfitting and maintaining the generalization capabilities of the pre-trained model. Chen et al. ( 2024 ) explored the intricacies of differentiating genuine images from those generated by the Stable Diffusion Model (SDM). They devised a novel approach, comprising a convolutional neural network (CNN) and a Transformer-based detection model, which effectively identifies artificially created images from SDMs. Furthermore, they were the first to assess the generalisation capacity of these detection models across diverse scenarios. Cazenavette et al. ( 2024 ) put forth a technique, designated as ‚ÄúFakeInversion, ‚Äù which employs features derived from open-source pre-trained stable diffusion models to identify synthetic images. A salient attribute of this technique is its capacity to generalise effectively to high-visual-fidelity invisible generators, even when trained exclusively on low-fidelity images generated by Stable Diffusion. 6 Challenges and future directions Dataset limitation The accelerated advancement of image generation techniques based on diffusion models can be attributed to the accessibility of extensive, high-quality datasets. For instance, the current text-to-image synthesis employs billions of high-quality (text, image) pairs (Ramesh et al. 2022 ; Nichol et al. 2022 ). However, some other subtasks continue to grapple with data scarcity. Furthermore, datasets also confront challenges related to data bias, encompassing aspects such as language, ethnicity, and gender. These issues can give rise to substantial biases and fairness concerns. High computational cost The principal challenges to diffusion models include the high cost of training and the number of steps in inference, which serve to exacerbate the disparity in access to resources between industry and academia (Blattmann et al. 2023 ; Ganguli et al. 2022 ). Despite efforts to reduce training costs, dataset size and time complexity remain significant obstacles. Furthermore, the model has difficulty generating readable text, and the computational requirements limit its deployment in real-world applications. Therefore, research should focus on improving the efficiency of the model, reducing the computational cost, and exploring further improvements in wavelet-based methods. Additionally, the success of deep learning relies on large amounts of labelled data, which poses a challenge for small companies and edge devices. Image evaluation The current methods for evaluating image generation are limited in their ability to comprehensively assess quality, rely on user testing and subjective scoring, and are susceptible to bias (Saharia et al. 2022 ; Parmar et al. 2022 ; Radford et al. 2021 ). To address these shortcomings, it is essential to develop more targeted evaluation benchmarks and indicators, as well as more reliable and diverse automatic evaluation criteria. Multimodal framework The generation of content from text to image, otherwise known as Artificial Intelligence Generated Content (AIGC), has attracted considerable interest from both academic and industrial perspectives. The current popular large language models (OpenAI 2023 ), based on autoregressive models, have achieved considerable success, particularly in terms of their capacity to generalise across legal domains and zero-shot tasks. Meanwhile, in the field of image generation, represented by models such as Stable Diffusion (Esser et al. 2024 ) and Sora (OpenAI 2023 ), diffusion models are widely adopted. As a crucial step towards general artificial intelligence, current research aims to integrate multiple tasks into a single model, thereby constructing multimodal models. Consequently, research interest has shifted towards analysing the emergence capability of diffusion models and developing versatile models capable of generating diverse outputs and handling various data types. This is considered a major challenge and research direction in image generation. Data Security and Social Ethics The advent of models such as Stable Diffusion and Midjourney has precipitated a period of rapid development in the field of image generation, resulting in a notable increase in the stylistic and diverse range of image creation. However, this technological advancement has also been accompanied by data privacy violations, copyright disputes, and the potential for misinformation and disinformation. The existence of these problems not only threatens the rights and interests of individuals, but also poses a challenge to social trust and moral standards. It is therefore recommended that future research should focus on strengthening data ethics and privacy protection research, developing more transparent and explainable models, and improving the controllability of generated content. We provide a multi-perspective for observing the development and impact of diffusion models in the field of image generation. We first introduce the development background of diffusion models from three basic theories: DDPM, SGMs, and SDEs, and explain some improvement methods of diffusion models in image generation. Second, we explore the wide application and high performance of diffusion models in various subfields of computer vision, including style transfer, image completion, image processing, super-resolution, 3D image generation, etc. Finally, we conduct a comprehensive analysis of the potential social and ethical implications and challenges of diffusion model-based image generation techniques. In summary, this paper provides an in-depth analysis and discussion of the application and potential social impact of diffusion models in the field of image generation. We hope that this survey can provide some guidance and inspiration for the future development of diffusion models in this field. Alexander Quinn Nichol and Prafulla Dhariwal (2021) Improved denoising diffusion probabilistic models. In International conference on machine learning, pp 8162‚Äì8171 Alexander Quinn Nichol and Prafulla Dhariwal (2021) Improved denoising diffusion probabilistic models. In International conference on machine learning, pp 8162‚Äì8171 Ali Raza Syed, Usman Habib, Muhammad Usman, Ashraf Cheema Adeel, Sajid Khan Muhammad (2024) MMGANGUARD: a robust approach for detecting fake images generated by GANS using multi-model techniques. IEEE Access 12:104153‚Äì104164 Article MATH Google Scholar Ali Raza Syed, Usman Habib, Muhammad Usman, Ashraf Cheema Adeel, Sajid Khan Muhammad (2024) MMGANGUARD: a robust approach for detecting fake images generated by GANS using multi-model techniques. IEEE Access 12:104153‚Äì104164 Article MATH Google Scholar Anciukevicius Titas, Xu Zexiang, Fisher Matthew, Henderson Paul, Bilen Hakan, Mitra Niloy J, Guerrero Paul (2023) Renderdiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 12608‚Äì12618 Anciukevicius Titas, Xu Zexiang, Fisher Matthew, Henderson Paul, Bilen Hakan, Mitra Niloy J, Guerrero Paul (2023) Renderdiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 12608‚Äì12618 Anderson Brian DO (1982) Reverse-time diffusion equation models. Stochast Process Appl 12(3):313‚Äì326 Article MathSciNet MATH Google Scholar Anderson Brian DO (1982) Reverse-time diffusion equation models. Stochast Process Appl 12(3):313‚Äì326 Article MathSciNet MATH Google Scholar Arjovsky Mart√≠n, Chintala Soumith, Bottou L√©on (2017) Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, vol 70, pp 214‚Äì223 Arjovsky Mart√≠n, Chintala Soumith, Bottou L√©on (2017) Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, vol 70, pp 214‚Äì223 Asnani Vishal, Collomosse John, Bui Tu, Liu Xiaoming, Agarwal Shruti (2024) Promark: Proactive diffusion watermarking for causal attribution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 10802‚Äì10811 Asnani Vishal, Collomosse John, Bui Tu, Liu Xiaoming, Agarwal Shruti (2024) Promark: Proactive diffusion watermarking for causal attribution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 10802‚Äì10811 Austin Jacob, Johnson Daniel D., Ho Jonathan, Tarlow Daniel, Berg Rianne van den (2021) Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, pp 17981‚Äì17993 Austin Jacob, Johnson Daniel D., Ho Jonathan, Tarlow Daniel, Berg Rianne van den (2021) Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, pp 17981‚Äì17993 Bansal Hritik, Yin Da, Monajatipoor Masoud, Chang Kai-Wei (2022) How well can text-to-image generative models understand ethical natural language interventions? Preprint at arXiv:2210.15230 Bansal Hritik, Yin Da, Monajatipoor Masoud, Chang Kai-Wei (2022) How well can text-to-image generative models understand ethical natural language interventions? Preprint at arXiv:2210.15230 Bao Qiqi, Hui Zheng, Zhu Rui, Ren Peiran, Xie Xuansong, Yang Wenming (2024) Improving diffusion-based image restoration with error contraction and error correction. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 756‚Äì764 Bao Qiqi, Hui Zheng, Zhu Rui, Ren Peiran, Xie Xuansong, Yang Wenming (2024) Improving diffusion-based image restoration with error contraction and error correction. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 756‚Äì764 Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, Bo Dai (2023) Generative diffusion prior for unified image restoration and enhancement. In IEEE/CVF Conference on Computer Vision and Pattern Recognition 55:9935‚Äì9946 Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, Bo Dai (2023) Generative diffusion prior for unified image restoration and enhancement. In IEEE/CVF Conference on Computer Vision and Pattern Recognition 55:9935‚Äì9946 Betker James, Goh Gabriel, Jing Li, et. al. Improving image generation with better captions. https://api.semanticscholar.org/CorpusID:264403242 Betker James, Goh Gabriel, Jing Li, et. al. Improving image generation with better captions. https://api.semanticscholar.org/CorpusID:264403242 Blattmann Andreas, Rombach Robin, Ling Huan, Dockhorn Tim, Kim Seung Wook, Fidler Sanja, Kreis Karsten (2023) Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp 22563‚Äì22575 Blattmann Andreas, Rombach Robin, Ling Huan, Dockhorn Tim, Kim Seung Wook, Fidler Sanja, Kreis Karsten (2023) Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp 22563‚Äì22575 Boutros Fadi, Grebe Jonas Henry, Kuijper Arjan, Damer Naser (2023) Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 19650‚Äì19661 Boutros Fadi, Grebe Jonas Henry, Kuijper Arjan, Damer Naser (2023) Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 19650‚Äì19661 Brack Manuel, Friedrich Felix, Schramowski Patrick, Kersting Kristian (2023) Mitigating inappropriateness in image generation: Can there be value in reflecting the world‚Äôs ugliness? Preprint at arXiv:2305.18398 Brack Manuel, Friedrich Felix, Schramowski Patrick, Kersting Kristian (2023) Mitigating inappropriateness in image generation: Can there be value in reflecting the world‚Äôs ugliness? Preprint at arXiv:2305.18398 Brack Manuel, Schramowski Patrick, Friedrich Felix, Hintersdorf Dominik, Kersting Kristian (2022) The stable artist: Steering semantics in diffusion latent space. Preprint at arXiv:2212.06013 Brack Manuel, Schramowski Patrick, Friedrich Felix, Hintersdorf Dominik, Kersting Kristian (2022) The stable artist: Steering semantics in diffusion latent space. Preprint at arXiv:2212.06013 Brack Manuel, Schramowski Patrick, Kersting Kristian (2023) Distilling adversarial prompts from safety benchmarks: Report for the adversarial nibbler challenge. Preprint at arXiv:2309.11575 Brack Manuel, Schramowski Patrick, Kersting Kristian (2023) Distilling adversarial prompts from safety benchmarks: Report for the adversarial nibbler challenge. Preprint at arXiv:2309.11575 Cao Pu, Zhou Feng, Song Qing, Yang Lu (2024) Controllable generation with text-to-image diffusion models: A survey. Preprint at arXiv:2403.04279 Cao Pu, Zhou Feng, Song Qing, Yang Lu (2024) Controllable generation with text-to-image diffusion models: A survey. Preprint at arXiv:2403.04279 Carlini Nicolas, Hayes Jamie, Nasr Milad, Jagielski Matthew, Sehwag Vikash, Tramer Florian, Balle Borja, Ippolito Daphne, Wallace Eric (2023) Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pp 5253‚Äì5270 Carlini Nicolas, Hayes Jamie, Nasr Milad, Jagielski Matthew, Sehwag Vikash, Tramer Florian, Balle Borja, Ippolito Daphne, Wallace Eric (2023) Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pp 5253‚Äì5270 Casper Stephen, Guo Zifan, Mogulothu Shreya, Marinov Zachary, Deshpande Chinmay, Yew Rui-Jie, Dai Zheng, Hadfield-Menell Dylan (2023) Measuring the success of diffusion models at imitating human artists. Preprint at arXiv:2307.04028 Casper Stephen, Guo Zifan, Mogulothu Shreya, Marinov Zachary, Deshpande Chinmay, Yew Rui-Jie, Dai Zheng, Hadfield-Menell Dylan (2023) Measuring the success of diffusion models at imitating human artists. Preprint at arXiv:2307.04028 Cazenavette George, Sud Avneesh, Leung Thomas, Usman Ben (2024) Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp 10759‚Äì10769 Cazenavette George, Sud Avneesh, Leung Thomas, Usman Ben (2024) Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp 10759‚Äì10769 Chang Ziyi, Koulieris George Alex, Shum Hubert P. H (2023) On the design fundamentals of diffusion models: A survey. Preprint at arXiv:2306.04542 Chang Ziyi, Koulieris George Alex, Shum Hubert P. H (2023) On the design fundamentals of diffusion models: A survey. Preprint at arXiv:2306.04542 Chen Dar-Yen (2023) Artfusion: Controllable arbitrary style transfer using dual conditional latent diffusion models. Preprint at arXiv:2306.09330 Chen Dar-Yen (2023) Artfusion: Controllable arbitrary style transfer using dual conditional latent diffusion models. Preprint at arXiv:2306.09330 Chen Jingyi , Wang Xiaolong, He Zhijian, Peng Xiaojiang (2024) A comprehensive exploration on detecting fake images generated by stable diffusion. In Pattern Recognition and Computer Vision - 7th Chinese Conference, PRCV 2024, Urumqi, China, October 18-20, 2024, Proceedings, Part I , volume 15031 of Lecture Notes in Computer Science, pp 461‚Äì475 Chen Jingyi , Wang Xiaolong, He Zhijian, Peng Xiaojiang (2024) A comprehensive exploration on detecting fake images generated by stable diffusion. In Pattern Recognition and Computer Vision - 7th Chinese Conference, PRCV 2024, Urumqi, China, October 18-20, 2024, Proceedings, Part I , volume 15031 of Lecture Notes in Computer Science, pp 461‚Äì475 Chen Songyan, Huang Jiancheng (2023) Fec: Three finetuning-free methods to enhance consistency for real image editing. In 2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML), pp 76‚Äì87 Chen Songyan, Huang Jiancheng (2023) Fec: Three finetuning-free methods to enhance consistency for real image editing. In 2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML), pp 76‚Äì87 Chen Zheng, Zhang Yulun, Liu Ding, Gu Jinjin, Kong Linghe, Yuan Xin et al (2024) Hierarchical integration diffusion model for realistic image deblurring. Adv Neural Inform Process Syst 36 Chen Zheng, Zhang Yulun, Liu Ding, Gu Jinjin, Kong Linghe, Yuan Xin et al (2024) Hierarchical integration diffusion model for realistic image deblurring. Adv Neural Inform Process Syst 36 Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu (2022) Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International Conference on Machine Learning vol 162, pp 14429‚Äì14460 Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu (2022) Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International Conference on Machine Learning vol 162, pp 14429‚Äì14460 Cheng Ta-Ying, Gadelha Matheus, Groueix Thibault, Fisher Matthew, Mech Radomir, Markham Andrew, Trigoni Niki (2024) Learning continuous 3d words for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 6753‚Äì6762 Cheng Ta-Ying, Gadelha Matheus, Groueix Thibault, Fisher Matthew, Mech Radomir, Markham Andrew, Trigoni Niki (2024) Learning continuous 3d words for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 6753‚Äì6762 Chenlu Zhan Yu, Lin Gaoang Wang, Wang Hongwei, Jian Wu (2024) Medm2g: Unifying medical multi-modal generation via cross-guided diffusion with visual invariant. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 11502‚Äì11512 Chenlu Zhan Yu, Lin Gaoang Wang, Wang Hongwei, Jian Wu (2024) Medm2g: Unifying medical multi-modal generation via cross-guided diffusion with visual invariant. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 11502‚Äì11512 Chin Zhi-Yi, Jiang Chieh-Ming, Huang Ching-Chun, Chen Pin-Yu, Chiu Wei-Chen (2023) Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. Preprint at arXiv:2309.06135 Chin Zhi-Yi, Jiang Chieh-Ming, Huang Ching-Chun, Chen Pin-Yu, Chiu Wei-Chen (2023) Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. Preprint at arXiv:2309.06135 Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, Fleet David J, Mohammad Norouzi (2022) Image super-resolution via iterative refinement. IEEE Trans Pattern Anal Mach Intell 45(4):4713‚Äì4726 MATH Google Scholar Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, Fleet David J, Mohammad Norouzi (2022) Image super-resolution via iterative refinement. IEEE Trans Pattern Anal Mach Intell 45(4):4713‚Äì4726 MATH Google Scholar Choi Jooyoung, Choi Yunjey, Kim Yunji, Kim Junho, Yoon Sungroh (2023) Custom-edit: Text-guided image editing with customized diffusion models. Preprint at arXiv:2305.15779 Choi Jooyoung, Choi Yunjey, Kim Yunji, Kim Junho, Yoon Sungroh (2023) Custom-edit: Text-guided image editing with customized diffusion models. Preprint at arXiv:2305.15779 Christopher Jarzynski (1997) Equilibrium free-energy differences from nonequilibrium measurements: a master-equation approach. Phys Rev E 56(5):5018 Article MATH Google Scholar Christopher Jarzynski (1997) Equilibrium free-energy differences from nonequilibrium measurements: a master-equation approach. Phys Rev E 56(5):5018 Article MATH Google Scholar Chung Jiwoo, Hyun Sangeek, Heo Jae-Pil (2023) Style injection in diffusion: A training-free approach for adapting large-scale diffusion models for style transfer. Preprint at arXiv:2312.09008 Chung Jiwoo, Hyun Sangeek, Heo Jae-Pil (2023) Style injection in diffusion: A training-free approach for adapting large-scale diffusion models for style transfer. Preprint at arXiv:2312.09008 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Liu Peter J (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res 21(140):1‚Äì67 MathSciNet Google Scholar Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Liu Peter J (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res 21(140):1‚Äì67 MathSciNet Google Scholar De Simone Zoe, Boggust Angie, Satyanarayan Arvind, Wilson Ashia (2023) What is a fair diffusion model? designing generative text-to-image models to incorporate various worldviews. Preprint at arXiv:2309.09944 De Simone Zoe, Boggust Angie, Satyanarayan Arvind, Wilson Ashia (2023) What is a fair diffusion model? designing generative text-to-image models to incorporate various worldviews. Preprint at arXiv:2309.09944 Deng Yingying, He Xiangyu, Tang Fan, Dong Weiming (2023) Z \\(^{\\text{*}}\\) : Zero-shot style transfer via attention rearrangement. Preprint at arXiv:2311.16491 Deng Yingying, He Xiangyu, Tang Fan, Dong Weiming (2023) Z \\(^{\\text{*}}\\) : Zero-shot style transfer via attention rearrangement. Preprint at arXiv:2311.16491 Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, Ling Shao (2022) Concealed object detection. IEEE Trans Pattern Anal Mach Intell 44(10):6024‚Äì6042 Article MATH Google Scholar Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, Ling Shao (2022) Concealed object detection. IEEE Trans Pattern Anal Mach Intell 44(10):6024‚Äì6042 Article MATH Google Scholar Dhariwal Prafulla, Nichol Alexander Quinn (2021) Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pp 8780‚Äì8794 Dhariwal Prafulla, Nichol Alexander Quinn (2021) Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pp 8780‚Äì8794 Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, Artem Babenko (2022) Label-efficient semantic segmentation with diffusion models Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, Artem Babenko (2022) Label-efficient semantic segmentation with diffusion models Dong Wenkai, Xue Song, Duan Xiaoyue, Han Shumin (2023) Prompt tuning inversion for text-driven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 7430‚Äì7440 Dong Wenkai, Xue Song, Duan Xiaoyue, Han Shumin (2023) Prompt tuning inversion for text-driven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 7430‚Äì7440 Dubinski Jan, Kowalczuk Antoni, Pawlak Stanisaw, Rokita Przemyslaw, Trzciski Tomasz, Morawiecki Pawe (2024) Towards more realistic membership inference attacks on large diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 4860‚Äì4869 Dubinski Jan, Kowalczuk Antoni, Pawlak Stanisaw, Rokita Przemyslaw, Trzciski Tomasz, Morawiecki Pawe (2024) Towards more realistic membership inference attacks on large diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 4860‚Äì4869 Dumoulin Vincent, Shlens Jonathon, Kudlur Manjunath (2017) A learned representation for artistic style. In 5th International Conference on Learning Representations Dumoulin Vincent, Shlens Jonathon, Kudlur Manjunath (2017) A learned representation for artistic style. In 5th International Conference on Learning Representations Esser Patrick, Kulal Sumith, Blattmann Andreas, Entezari Rahim,√ºller Jonas M, Saini Harry, Levi Yam, Lorenz Dominik, Sauer Axel, Boesel Frederic, Podell Dustin, Dockhorn Tim, English Zion, Rombach Robin (2024) Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 Esser Patrick, Kulal Sumith, Blattmann Andreas, Entezari Rahim,√ºller Jonas M, Saini Harry, Levi Yam, Lorenz Dominik, Sauer Axel, Boesel Frederic, Podell Dustin, Dockhorn Tim, English Zion, Rombach Robin (2024) Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 Evgin Goceri (2023) Comparison of the impacts of dermoscopy image augmentation methods on skin cancer classification and a new augmentation method with wavelet packets. Int J Imaging Syst Technol 33(5):1727‚Äì1744 Article MATH Google Scholar Evgin Goceri (2023) Comparison of the impacts of dermoscopy image augmentation methods on skin cancer classification and a new augmentation method with wavelet packets. Int J Imaging Syst Technol 33(5):1727‚Äì1744 Article MATH Google Scholar Evgin Goceri (2024) GAN based augmentation using a hybrid loss function for dermoscopy images. Artif Intell Rev 57(9):234 Article MATH Google Scholar Evgin Goceri (2024) GAN based augmentation using a hybrid loss function for dermoscopy images. Artif Intell Rev 57(9):234 Article MATH Google Scholar Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang (2022) Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang (2022) Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models Fan Deng-Ping, Ji Ge-Peng, Xu Peng, Cheng Ming-Ming, Sakaridis Christos, Gool Luc Van (2023) Advances in deep concealed scene understanding. Vis Intell 1(1) Fan Deng-Ping, Ji Ge-Peng, Xu Peng, Cheng Ming-Ming, Sakaridis Christos, Gool Luc Van (2023) Advances in deep concealed scene understanding. Vis Intell 1(1) Fan Mingyuan, Wang Chengyu, Chen Cen, Liu Yang, Huang Jun (2024) On the trustworthiness landscape of state-of-the-art generative models: a survey and outlook. Preprint at arXiv:2307.16680 Fan Mingyuan, Wang Chengyu, Chen Cen, Liu Yang, Huang Jun (2024) On the trustworthiness landscape of state-of-the-art generative models: a survey and outlook. Preprint at arXiv:2307.16680 Fan Yuheng, Liao Hanxi, Huang Shiqi, Luo Yimin, Fu Huazhu, Qi Haikun (2023) A survey of emerging applications of diffusion probabilistic models in MRI. Preprint at arXiv:2311.11383 Fan Yuheng, Liao Hanxi, Huang Shiqi, Luo Yimin, Fu Huazhu, Qi Haikun (2023) A survey of emerging applications of diffusion probabilistic models in MRI. Preprint at arXiv:2311.11383 Florinel-Alin Croitoru, Vlad Hondru, Tudor Ionescu Radu, Mubarak Shah (2023) Diffusion models in vision: a survey. IEEE Trans Pattern Anal Mach Intell 45(9):10850‚Äì10869 Article MATH Google Scholar Florinel-Alin Croitoru, Vlad Hondru, Tudor Ionescu Radu, Mubarak Shah (2023) Diffusion models in vision: a survey. IEEE Trans Pattern Anal Mach Intell 45(9):10850‚Äì10869 Article MATH Google Scholar Florinel-Alin Croitoru, Vlad Hondru, Tudor Ionescu Radu, Mubarak Shah (2023) Diffusion models in vision: a survey. IEEE Trans Pattern Anal Mach Intell 45(9):10850‚Äì10869 Article MATH Google Scholar Florinel-Alin Croitoru, Vlad Hondru, Tudor Ionescu Radu, Mubarak Shah (2023) Diffusion models in vision: a survey. IEEE Trans Pattern Anal Mach Intell 45(9):10850‚Äì10869 Article MATH Google Scholar Francesco Tassone, Luca Maiano, Irene Amerini (2024) Continuous fake media detection: adapting deepfake detectors to new generative techniques. Comput Vis Image Underst 249:104143 Article MATH Google Scholar Francesco Tassone, Luca Maiano, Irene Amerini (2024) Continuous fake media detection: adapting deepfake detectors to new generative techniques. Comput Vis Image Underst 249:104143 Article MATH Google Scholar Friedrich Felix, Brack Manuel, Struppek Lukas, Hintersdorf Dominik, Schramowski Patrick, Luccioni Sasha, Kersting Kristian (2023) Fair diffusion: Instructing text-to-image generation models on fairness. Preprint at arXiv:2302.10893 Friedrich Felix, Brack Manuel, Struppek Lukas, Hintersdorf Dominik, Schramowski Patrick, Luccioni Sasha, Kersting Kristian (2023) Fair diffusion: Instructing text-to-image generation models on fairness. Preprint at arXiv:2302.10893 Gandikota Kanchana Vaishnavi,Chandramouli Paramanand (2024) Text-guided explorable image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25900‚Äì25911 Gandikota Kanchana Vaishnavi,Chandramouli Paramanand (2024) Text-guided explorable image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25900‚Äì25911 Gandikota Rohit, Orgad Hadas, Belinkov Yonatan, Materzy≈Ñska Joanna, Bau David (2024) Unified concept editing in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 5111‚Äì5120 Gandikota Rohit, Orgad Hadas, Belinkov Yonatan, Materzy≈Ñska Joanna, Bau David (2024) Unified concept editing in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 5111‚Äì5120 Ganguli Deep, Hernandez Danny, Lovitt Liane, Askell Amanda, Bai Yuntao, Chen Anna, Conerly Tom, DasSarma Nova, Drain Dawn, Elhage Nelson, Showk Sheer El, Fort Stanislav, Hatfield-Dodds Zac, Henighan Tom, Johnston Scott, Jones Andy, Joseph Nicholas, Kernian Jackson, Kravec Shauna, Mann Ben, Nanda Neel, Ndousse Kamal, Olsson Catherine, Amodei Daniela, Brown Tom B., Kaplan Jared, McCandlish Sam, Olah Christopher, Amodei Dario, Clark Jack (2022) Predictability and surprise in large generative models. In FAccT ‚Äô22: 2022 ACM Conference on Fairness, Accountability, and Transparency, Seoul, Republic of Korea, June 21 - 24, 2022, pp 1747‚Äì1764 Ganguli Deep, Hernandez Danny, Lovitt Liane, Askell Amanda, Bai Yuntao, Chen Anna, Conerly Tom, DasSarma Nova, Drain Dawn, Elhage Nelson, Showk Sheer El, Fort Stanislav, Hatfield-Dodds Zac, Henighan Tom, Johnston Scott, Jones Andy, Joseph Nicholas, Kernian Jackson, Kravec Shauna, Mann Ben, Nanda Neel, Ndousse Kamal, Olsson Catherine, Amodei Daniela, Brown Tom B., Kaplan Jared, McCandlish Sam, Olah Christopher, Amodei Dario, Clark Jack (2022) Predictability and surprise in large generative models. In FAccT ‚Äô22: 2022 ACM Conference on Fairness, Accountability, and Transparency, Seoul, Republic of Korea, June 21 - 24, 2022, pp 1747‚Äì1764 Gao Sicheng, Liu Xuhui, Zeng Bohan, Xu Sheng, Li Yanjing, Luo Xiaoyan, Liu Jianzhuang, Zhen Xiantong, Zhang Baochang (2023) Implicit diffusion models for continuous super-resolution. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10021‚Äì10030 Gao Sicheng, Liu Xuhui, Zeng Bohan, Xu Sheng, Li Yanjing, Luo Xiaoyan, Liu Jianzhuang, Zhen Xiantong, Zhang Baochang (2023) Implicit diffusion models for continuous super-resolution. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10021‚Äì10030 Gatys Leon A, Ecker Alexander S, Bethge Matthias (c) A neural algorithm of artistic style. Preprint at arXiv:1508.06576 Gatys Leon A, Ecker Alexander S, Bethge Matthias (c) A neural algorithm of artistic style. Preprint at arXiv:1508.06576 George Wolberg (1998) Image morphing: a survey. Vis Comput 14(8/9):360‚Äì372 MATH Google Scholar George Wolberg (1998) Image morphing: a survey. Vis Comput 14(8/9):360‚Äì372 MATH Google Scholar George Papamakarios, Nalisnick Eric T, Jimenez Rezende Danilo, Shakir Mohamed, Balaji Lakshminarayanan (2021) Normalizing flows for probabilistic modeling and inference. J Mach Learn Res 57:1‚Äì57 MathSciNet MATH Google Scholar George Papamakarios, Nalisnick Eric T, Jimenez Rezende Danilo, Shakir Mohamed, Balaji Lakshminarayanan (2021) Normalizing flows for probabilistic modeling and inference. J Mach Learn Res 57:1‚Äì57 MathSciNet MATH Google Scholar Giorgio Parisi (1981) Correlation functions and computer simulations. Nucl Phys B 180(3):378‚Äì384 Article MathSciNet MATH Google Scholar Giorgio Parisi (1981) Correlation functions and computer simulations. Nucl Phys B 180(3):378‚Äì384 Article MathSciNet MATH Google Scholar Goodfellow Ian J, Pouget-Abadie Jean, Mirza Mehdi, Xu Bing, Warde-Farley David, Ozair Sherjil, Courville Aaron C, Bengio Yoshua (2014) Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, pp 2672‚Äì2680 Goodfellow Ian J, Pouget-Abadie Jean, Mirza Mehdi, Xu Bing, Warde-Farley David, Ozair Sherjil, Courville Aaron C, Bengio Yoshua (2014) Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, pp 2672‚Äì2680 Grenander Ulf, Miller Michael I (1994) Representations of knowledge in complex systems. J Roy Stat Soc: Ser B (Methodol) 56(4):549‚Äì581 Article MathSciNet MATH Google Scholar Grenander Ulf, Miller Michael I (1994) Representations of knowledge in complex systems. J Roy Stat Soc: Ser B (Methodol) 56(4):549‚Äì581 Article MathSciNet MATH Google Scholar Gu Jing, Wang Yilin, Zhao Nanxuan, Fu Tsu-Jui, Xiong Wei, Liu Qing, Zhang Zhifei, Zhang He, Zhang Jianming, Jung HyunJoon et al (2024) Photoswap: Personalized subject swapping in images. Adv Neural Inform Process Sys 36 Gu Jing, Wang Yilin, Zhao Nanxuan, Fu Tsu-Jui, Xiong Wei, Liu Qing, Zhang Zhifei, Zhang He, Zhang Jianming, Jung HyunJoon et al (2024) Photoswap: Personalized subject swapping in images. Adv Neural Inform Process Sys 36 Gu Xiangming, Du Chao, Pang Tianyu, Li Chongxuan, Lin Min, Wang Ye (2024) On memorization in diffusion models. Preprint at arXiv:2310.02664 Gu Xiangming, Du Chao, Pang Tianyu, Li Chongxuan, Lin Min, Wang Ye (2024) On memorization in diffusion models. Preprint at arXiv:2310.02664 Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Jaakkola Tommi S, Shiyu Chang (2023) Towards coherent image inpainting using denoising diffusion implicit models. In International Conference on Machine Learning vol 202, pp 41164‚Äì41193 Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Jaakkola Tommi S, Shiyu Chang (2023) Towards coherent image inpainting using denoising diffusion implicit models. In International Conference on Machine Learning vol 202, pp 41164‚Äì41193 Han Ligong, Wen Song, Chen Qi, Zhang Zhixing, Song Kunpeng, Ren Mengwei, Gao Ruijiang, Stathopoulos Anastasis, He Xiaoxiao, Chen Yuxiao et al (2024) Proxedit: Improving tuning-free real image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 4291‚Äì4301 Han Ligong, Wen Song, Chen Qi, Zhang Zhixing, Song Kunpeng, Ren Mengwei, Gao Ruijiang, Stathopoulos Anastasis, He Xiaoxiao, Chen Yuxiao et al (2024) Proxedit: Improving tuning-free real image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 4291‚Äì4301 Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li. (2024) A survey on generative diffusion models. IEEE Trans Knowl Data Eng 36(7):2814‚Äì2830 Article Google Scholar Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li. (2024) A survey on generative diffusion models. IEEE Trans Knowl Data Eng 36(7):2814‚Äì2830 Article Google Scholar Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen (2022) SRDIFF: Single image super-resolution with diffusion probabilistic models. Neurocomputing 479:47‚Äì59 Article Google Scholar Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen (2022) SRDIFF: Single image super-resolution with diffusion probabilistic models. Neurocomputing 479:47‚Äì59 Article Google Scholar Heng Alvin, Soh Harold (2024) Selective amnesia: A continual learning approach to forgetting in deep generative models. Adv Neural Inform Process Syst 36 Heng Alvin, Soh Harold (2024) Selective amnesia: A continual learning approach to forgetting in deep generative models. Adv Neural Inform Process Syst 36 Higgins Irina, Matthey Lo√Øc, Pal Arka, Burgess Christopher P., Glorot Xavier, Botvinick Matthew M, Mohamed Shakir, Lerchner Alexander (2017) beta-vae: Learning basic visual concepts with a constrained variational framework. In 5th International Conference on Learning Representations Higgins Irina, Matthey Lo√Øc, Pal Arka, Burgess Christopher P., Glorot Xavier, Botvinick Matthew M, Mohamed Shakir, Lerchner Alexander (2017) beta-vae: Learning basic visual concepts with a constrained variational framework. In 5th International Conference on Learning Representations Ho Jonathan, Salimans Tim (2022) Classifier-free diffusion guidance. Preprint at arXiv:2207.12598 Ho Jonathan, Salimans Tim (2022) Classifier-free diffusion guidance. Preprint at arXiv:2207.12598 Ho Jonathan, Salimans Tim (2022) Classifier-free diffusion guidance. Preprint at arXiv:2207.12598 Ho Jonathan, Salimans Tim (2022) Classifier-free diffusion guidance. Preprint at arXiv:2207.12598 Hong Seunghoo, Lee Juhun, Woo Simon S (2024) All but one: Surgical concept erasing with model preservation in text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence 38:21143‚Äì21151 Hong Seunghoo, Lee Juhun, Woo Simon S (2024) All but one: Surgical concept erasing with model preservation in text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence 38:21143‚Äì21151 Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima Anandkumar (2023) Fast sampling of diffusion models via operator learning. In International Conference on Machine Learning vol 202, pp 42390‚Äì42402 Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima Anandkumar (2023) Fast sampling of diffusion models via operator learning. In International Conference on Machine Learning vol 202, pp 42390‚Äì42402 Hu Edward J, Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu (2022) Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 Hu Edward J, Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu (2022) Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 Hu Hailong, Pang Jun (2023) Loss and likelihood based membership inference of diffusion models. In International Conference on Information Security, pp 121‚Äì141 Hu Hailong, Pang Jun (2023) Loss and likelihood based membership inference of diffusion models. In International Conference on Information Security, pp 121‚Äì141 Huang Chin-Wei, Lim Jae Hyun, Aaron C (2021) Courville. A variational perspective on diffusion-based generative models and score matching. In Advances in Neural Information Processing Systems, pp 22863‚Äì22876 Huang Chin-Wei, Lim Jae Hyun, Aaron C (2021) Courville. A variational perspective on diffusion-based generative models and score matching. In Advances in Neural Information Processing Systems, pp 22863‚Äì22876 Huang Jiancheng, Liu Yifan, Qin Jin, Chen Shifeng (2023). Kv inversion: Kv embeddings learning for text-conditioned real image action editing. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp 172‚Äì184. Springer Huang Jiancheng, Liu Yifan, Qin Jin, Chen Shifeng (2023). Kv inversion: Kv embeddings learning for text-conditioned real image action editing. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp 172‚Äì184. Springer Huang Nisha, Zhang Yuxin, Tang Fan, Ma Chongyang, Huang Haibin, Zhang Yong, Dong Weiming, Xu Changsheng (2022) Diffstyler: Controllable dual diffusion for text-driven image stylization. Preprint at arXiv:2211.10682 Huang Nisha, Zhang Yuxin, Tang Fan, Ma Chongyang, Huang Haibin, Zhang Yong, Dong Weiming, Xu Changsheng (2022) Diffstyler: Controllable dual diffusion for text-driven image stylization. Preprint at arXiv:2211.10682 Hudson Drew A, Zoran Daniel, Malinowski Mateusz, Lampinen Andrew K, Jaegle Andrew, McClelland James L, Matthey Loic, Hill Felix, Lerchner Alexander (2024) Soda: Bottleneck diffusion models for representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 23115‚Äì23127 Hudson Drew A, Zoran Daniel, Malinowski Mateusz, Lampinen Andrew K, Jaegle Andrew, McClelland James L, Matthey Loic, Hill Felix, Lerchner Alexander (2024) Soda: Bottleneck diffusion models for representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 23115‚Äì23127 Hyvarinen Aapo, Dayan Peter (2005) Estimation of non-normalized statistical models by score matching. J Mach Learn Res 6(4) Hyvarinen Aapo, Dayan Peter (2005) Estimation of non-normalized statistical models by score matching. J Mach Learn Res 6(4) Imagen 3 Team: Jason Baldridge, Jakob Bauer, Mukul Bhutani, and et. al. (2024) Imagen 3. Preprint at arXiv:2408.07009 Imagen 3 Team: Jason Baldridge, Jakob Bauer, Mukul Bhutani, and et. al. (2024) Imagen 3. Preprint at arXiv:2408.07009 Jia-Hao Wu, Tsai Fu-Jen, Peng Yan-Tsung, Tsai Chung-Chi, Lin Chia-Wen, Lin Yen-Yu (2024) Id-blau: Image deblurring by implicit diffusion-based reblurring augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25847‚Äì25856 Jia-Hao Wu, Tsai Fu-Jen, Peng Yan-Tsung, Tsai Chung-Chi, Lin Chia-Wen, Lin Yen-Yu (2024) Id-blau: Image deblurring by implicit diffusion-based reblurring augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25847‚Äì25856 Jiang Yue, Lyu Yueming, Ma Tianxiang, Peng Bo, Dong Jing (2023) Rs-corrector: Correcting the racial stereotypes in latent diffusion models. Preprint at arXiv:2312.04810 Jiang Yue, Lyu Yueming, Ma Tianxiang, Peng Bo, Dong Jing (2023) Rs-corrector: Correcting the racial stereotypes in latent diffusion models. Preprint at arXiv:2312.04810 Jiayi Liao Xu, Chen Qiang Fu, Lun Du, He Xiangnan, Wang Xiang, Han Shi, Zhang Dongmei (2024) Text-to-image generation for abstract concepts. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 3360‚Äì3368 Jiayi Liao Xu, Chen Qiang Fu, Lun Du, He Xiangnan, Wang Xiang, Han Shi, Zhang Dongmei (2024) Text-to-image generation for abstract concepts. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 3360‚Äì3368 Jolicoeur-Martineau Alexia, Piche-Taillefer Remi, Tachet des Combes Remi, Mitliagkas Ioannis (2020) Adversarial score matching and improved sampling for image generation. Preprint at arXiv:2009.05475 Jolicoeur-Martineau Alexia, Piche-Taillefer Remi, Tachet des Combes Remi, Mitliagkas Ioannis (2020) Adversarial score matching and improved sampling for image generation. Preprint at arXiv:2009.05475 Jonathan Ho, Ajay Jain, Pieter Abbeel (2020) Denoising diffusion probabilistic models. Adv Neural Inf Process Syst 33:6840‚Äì6851 Google Scholar Jonathan Ho, Ajay Jain, Pieter Abbeel (2020) Denoising diffusion probabilistic models. Adv Neural Inf Process Syst 33:6840‚Äì6851 Ju Xuan, Zeng Ailing, Zhao Chenchen, Wang Jianan, Zhang Lei, Xu Qiang (2023) Humansd: A native skeleton-guided diffusion model for human image generation. In IEEE/CVF International Conference on Computer Vision, pp 15942‚Äì15952 Ju Xuan, Zeng Ailing, Zhao Chenchen, Wang Jianan, Zhang Lei, Xu Qiang (2023) Humansd: A native skeleton-guided diffusion model for human image generation. In IEEE/CVF International Conference on Computer Vision, pp 15942‚Äì15952 Justin Johnson, Alexandre Alahi, Li Fei-Fei (2016) Perceptual losses for real-time style transfer and super-resolution. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016‚Äì14th European Conference, Amsterdam, The Netherlands, October 11‚Äì14, 2016. Proceedings, Part II vol 9906, pp 694‚Äì711 Justin Johnson, Alexandre Alahi, Li Fei-Fei (2016) Perceptual losses for real-time style transfer and super-resolution. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016‚Äì14th European Conference, Amsterdam, The Netherlands, October 11‚Äì14, 2016. Proceedings, Part II vol 9906, pp 694‚Äì711 Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu (2023) Improved techniques for maximum likelihood estimation for diffusion odes. In International Conference on Machine Learning vol 202, pp 42363‚Äì42389 Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu (2023) Improved techniques for maximum likelihood estimation for diffusion odes. In International Conference on Machine Learning vol 202, pp 42363‚Äì42389 Ke Bingxin, Obukhov Anton, Huang Shengyu, Metzger Nando, Daudt Rodrigo Caye, Schindler Konrad (2024) Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 9492‚Äì9502 Ke Bingxin, Obukhov Anton, Huang Shengyu, Metzger Nando, Daudt Rodrigo Caye, Schindler Konrad (2024) Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 9492‚Äì9502 Kim Sunwoo, Jang Wooseok, Kim Hyunsu, Kim Junho, Choi Yunjey, Kim Seungryong, Lee Gayeong (2023) User-friendly image editing with minimal text input: Leveraging captioning and injection techniques. Preprint at arXiv:2306.02717 , Kim Sunwoo, Jang Wooseok, Kim Hyunsu, Kim Junho, Choi Yunjey, Kim Seungryong, Lee Gayeong (2023) User-friendly image editing with minimal text input: Leveraging captioning and injection techniques. Preprint at arXiv:2306.02717 , Kingma Diederik P, Salimans Tim, Poole Ben, Ho Jonathan (2021) Variational diffusion models. Preprint at arXiv:2107.00630 Kingma Diederik P, Salimans Tim, Poole Ben, Ho Jonathan (2021) Variational diffusion models. Preprint at arXiv:2107.00630 Kingma Diederik P, Welling Max (2014) Auto-encoding variational bayes. In 2nd International Conference on Learning Representations Kingma Diederik P, Welling Max (2014) Auto-encoding variational bayes. In 2nd International Conference on Learning Representations Kingma Diederik P, Welling Max (2014) Auto-encoding variational bayes. In 2nd International Conference on Learning Representations Kingma Diederik P, Welling Max (2014) Auto-encoding variational bayes. In 2nd International Conference on Learning Representations Kocsis Peter, Sitzmann Vincent, Nie√üner Matthias (2024) Intrinsic image diffusion for indoor single-view material estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 5198‚Äì5208 Kocsis Peter, Sitzmann Vincent, Nie√üner Matthias (2024) Intrinsic image diffusion for indoor single-view material estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 5198‚Äì5208 Kumari Nupur, Zhang Bingliang, Wang Sheng-Yu, Shechtman Eli, Zhang Richard, Zhu Jun-Yan (2023) Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 22691‚Äì22702 Kumari Nupur, Zhang Bingliang, Wang Sheng-Yu, Shechtman Eli, Zhang Richard, Zhu Jun-Yan (2023) Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 22691‚Äì22702 Lee Jumin, Im Woobin, Lee Sebin, Yoon Sung-Eui (2023) Diffusion probabilistic models for scene-scale 3d categorical data. Preprint at arXiv:2301.00527 Lee Jumin, Im Woobin, Lee Sebin, Yoon Sung-Eui (2023) Diffusion probabilistic models for scene-scale 3d categorical data. Preprint at arXiv:2301.00527 Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, Junbin Gao (2024) Diffusion models for time-series applications: a survey. Front Inf Technol Electron Eng 25(1):19‚Äì41 Article MATH Google Scholar Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, Junbin Gao (2024) Diffusion models for time-series applications: a survey. Front Inf Technol Electron Eng 25(1):19‚Äì41 Article MATH Google Scholar Li Jia, Hu Lijie, Zhang Jingfeng, Zheng Tianhang, Zhang Hua, Wang Di (2023) Fair text-to-image diffusion via fair mapping. Preprint at arXiv:2311.17695 Li Jia, Hu Lijie, Zhang Jingfeng, Zheng Tianhang, Zhang Hua, Wang Di (2023) Fair text-to-image diffusion via fair mapping. Preprint at arXiv:2311.17695 Li Shengmeng, Liu Luping, Chai Zenghao, Li Runnan, Tan Xu (2023) Era-solver: Error-robust adams solver for fast sampling of diffusion probabilistic models. Preprint at arXiv:2301.12935 Li Shengmeng, Liu Luping, Chai Zenghao, Li Runnan, Tan Xu (2023) Era-solver: Error-robust adams solver for fast sampling of diffusion probabilistic models. Preprint at arXiv:2301.12935 Li Sifei, Zhang Yuxin, Tang Fan, Ma Chongyang, Dong Weiming, Xu Changsheng (2024) Music style transfer with time-varying inversion of diffusion models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 547‚Äì555 Li Sifei, Zhang Yuxin, Tang Fan, Ma Chongyang, Dong Weiming, Xu Changsheng (2024) Music style transfer with time-varying inversion of diffusion models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 547‚Äì555 Li Tianle, Ku Max, Wei Cong, Chen Wenhu (2023) Dreamedit: Subject-driven image editing. Preprint at arXiv:2306.12624 Li Tianle, Ku Max, Wei Cong, Chen Wenhu (2023) Dreamedit: Subject-driven image editing. Preprint at arXiv:2306.12624 Li Xin, Ren Yulin, Jin Xin, Lan Cuiling, Wang Xingrui, Zeng Wenjun, Wang Xinchao, Chen Zhibo (2023) Diffusion models for image restoration and enhancement‚Äîa comprehensive survey. Preprint at arXiv:2308.09388 Li Xin, Ren Yulin, Jin Xin, Lan Cuiling, Wang Xingrui, Zeng Wenjun, Wang Xinchao, Chen Zhibo (2023) Diffusion models for image restoration and enhancement‚Äîa comprehensive survey. Preprint at arXiv:2308.09388 Li Xinghui, Jingyi Lu, Han Kai, Prisacariu Victor Adrian (2024) Sd4match: Learning to prompt stable diffusion model for semantic matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 27558‚Äì27568 Li Xinghui, Jingyi Lu, Han Kai, Prisacariu Victor Adrian (2024) Sd4match: Learning to prompt stable diffusion model for semantic matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 27558‚Äì27568 Liang Youwei, He Junfeng, Li Gang, Li Peizhao, Klimovskiy Arseniy, Carolan Nicholas, Sun Jiao, Pont-Tuset Jordi, Young Sarah, Yang Feng, Ke Junjie, Dvijotham Krishnamurthy Dj, Collins Katherine M, Luo Yiwen, Li Yang, Kohlhoff Kai J, Ramachandran Deepak, Navalpakkam Vidhya (2024) Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 19401‚Äì19411 Liang Youwei, He Junfeng, Li Gang, Li Peizhao, Klimovskiy Arseniy, Carolan Nicholas, Sun Jiao, Pont-Tuset Jordi, Young Sarah, Yang Feng, Ke Junjie, Dvijotham Krishnamurthy Dj, Collins Katherine M, Luo Yiwen, Li Yang, Kohlhoff Kai J, Ramachandran Deepak, Navalpakkam Vidhya (2024) Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 19401‚Äì19411 Lin Xinqi, He Jingwen, Chen Ziyan, Lyu Zhaoyang, Fei Ben, Dai Bo, Ouyang Wanli, Qiao Yu, Dong Chao (2023) Diffbir: Towards blind image restoration with generative diffusion prior. Preprint at arXiv:2308.15070 Lin Xinqi, He Jingwen, Chen Ziyan, Lyu Zhaoyang, Fei Ben, Dai Bo, Ouyang Wanli, Qiao Yu, Dong Chao (2023) Diffbir: Towards blind image restoration with generative diffusion prior. Preprint at arXiv:2308.15070 Lin Yuanze, Chen Yi-Wen, Tsai Yi-Hsuan, Jiang Lu, Yang Ming-Hsuan (2023) Text-driven image editing via learnable regions. Preprint at arXiv:2311.16432 Lin Yuanze, Chen Yi-Wen, Tsai Yi-Hsuan, Jiang Lu, Yang Ming-Hsuan (2023) Text-driven image editing via learnable regions. Preprint at arXiv:2311.16432 Lin Zinan, Gopi Sivakanth, Kulkarni Janardhan, Nori Harsha, Yekhanin Sergey (2023) Differentially private synthetic data via foundation model apis 1: Images. Preprint at arXiv:2305.15560 Lin Zinan, Gopi Sivakanth, Kulkarni Janardhan, Nori Harsha, Yekhanin Sergey (2023) Differentially private synthetic data via foundation model apis 1: Images. Preprint at arXiv:2305.15560 Ling Pengyang, Chen Lin, Zhang Pan, Chen Huaian, Jin Yi, Zheng Jinjin (2024) Freedrag: Feature dragging for reliable point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 6860‚Äì6870 Ling Pengyang, Chen Lin, Zhang Pan, Chen Huaian, Jin Yi, Zheng Jinjin (2024) Freedrag: Feature dragging for reliable point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 6860‚Äì6870 Ling Yang, Zhilong Zhang, Yang Song, Hong Shenda Xu, Runsheng Zhao Yue, Wentao Zhang, Bin Cui, Ming-Hsuan Yang (2024) Diffusion models: a comprehensive survey of methods and applications. ACM Comput Surv 56(4):105‚Äì39 Google Scholar Ling Yang, Zhilong Zhang, Yang Song, Hong Shenda Xu, Runsheng Zhao Yue, Wentao Zhang, Bin Cui, Ming-Hsuan Yang (2024) Diffusion models: a comprehensive survey of methods and applications. ACM Comput Surv 56(4):105‚Äì39 Liu Anji, Niepert Mathias, den Broeck Guy Van (2024) Image inpainting via tractable steering of diffusion models. In The Twelfth International Conference on Learning Representations Liu Anji, Niepert Mathias, den Broeck Guy Van (2024) Image inpainting via tractable steering of diffusion models. In The Twelfth International Conference on Learning Representations Liu Haipeng, Wang Yang, Qian Biao, Wang Meng, Rui Yong (2024) Structure matters: Tackling the semantic discrepancy in diffusion models for image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 8038‚Äì8047 Liu Haipeng, Wang Yang, Qian Biao, Wang Meng, Rui Yong (2024) Structure matters: Tackling the semantic discrepancy in diffusion models for image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 8038‚Äì8047 Liu Jiawei, Wang Qiang, Fan Huijie, Wang Yinong, Tang Yandong, Qu Liangqiong (2023) Residual denoising diffusion models. Preprint at arXiv:2308.13712 Liu Jiawei, Wang Qiang, Fan Huijie, Wang Yinong, Tang Yandong, Qu Liangqiong (2023) Residual denoising diffusion models. Preprint at arXiv:2308.13712 Liu Jinxiu, Liu Qi (2024) R3CD: scene graph to image generation with relation-aware compositional contrastive control diffusion. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 3657‚Äì3665 Liu Jinxiu, Liu Qi (2024) R3CD: scene graph to image generation with relation-aware compositional contrastive control diffusion. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 3657‚Äì3665 Liu Xian, Ren Jian, Siarohin Aliaksandr, Skorokhodov Ivan, Li Yanyu, Lin Dahua, Liu Xihui, Liu Ziwei, Tulyakov Sergey (2024) Hyperhuman: Hyper-realistic human generation with latent structural diffusion. In The Twelfth International Conference on Learning Representations Liu Xian, Ren Jian, Siarohin Aliaksandr, Skorokhodov Ivan, Li Yanyu, Lin Dahua, Liu Xihui, Liu Ziwei, Tulyakov Sergey (2024) Hyperhuman: Hyper-realistic human generation with latent structural diffusion. In The Twelfth International Conference on Learning Representations Liu Xihui, Park Dong Huk, Azadi Samaneh, Zhang Gong, Chopikyan Arman, Yuxiao Hu, Shi Humphrey, Rohrbach Anna, Darrell Trevor (2023) More control for free! image synthesis with semantic diffusion guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 289‚Äì299 Liu Xihui, Park Dong Huk, Azadi Samaneh, Zhang Gong, Chopikyan Arman, Yuxiao Hu, Shi Humphrey, Rohrbach Anna, Darrell Trevor (2023) More control for free! image synthesis with semantic diffusion guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp 289‚Äì299 Liu Yixin, Zhang Kai, Li Yuan, Yan Zhiling, Gao Chujie, Chen Ruoxi, Yuan Zhengqing, Huang Yue, Sun Hanchi, Gao Jianfeng, He Lifang, Sun Lichao (2024) Sora: A review on background, technology, limitations, and opportunities of large vision models. Preprint at arXiv:2402.17177 Liu Yixin, Zhang Kai, Li Yuan, Yan Zhiling, Gao Chujie, Chen Ruoxi, Yuan Zhengqing, Huang Yue, Sun Hanchi, Gao Jianfeng, He Lifang, Sun Lichao (2024) Sora: A review on background, technology, limitations, and opportunities of large vision models. Preprint at arXiv:2402.17177 Lu Cheng, Zhou Yuhao, Bao Fan, Chen Jianfei, Li Chongxuan, Zhu Jun (2022) Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Preprint at arXiv:2211.01095 Lu Cheng, Zhou Yuhao, Bao Fan, Chen Jianfei, Li Chongxuan, Zhu Jun (2022) Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Preprint at arXiv:2211.01095 Luccioni Alexandra Sasha, Akiki Christopher, Mitchell Margaret, Jernite Yacine (2023) Stable bias: Analyzing societal representations in diffusion models. Preprint at arXiv:2303.11408 Luccioni Alexandra Sasha, Akiki Christopher, Mitchell Margaret, Jernite Yacine (2023) Stable bias: Analyzing societal representations in diffusion models. Preprint at arXiv:2303.11408 Lugmayr Andreas, Danelljan Martin, Romero Andres, Fisher Yu, Timofte Radu, Van Gool Luc (2022) Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 11461‚Äì11471 Lugmayr Andreas, Danelljan Martin, Romero Andres, Fisher Yu, Timofte Radu, Van Gool Luc (2022) Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 11461‚Äì11471 Luo Calvin (2022) Understanding diffusion models: a unified perspective. Preprint at arXiv:2208.11970 Luo Calvin (2022) Understanding diffusion models: a unified perspective. Preprint at arXiv:2208.11970 Luo Shitong, Wei Hu (2021) Diffusion probabilistic models for 3D point cloud generation. In IEEE Conference on Computer Vision and Pattern Recognition, pp 2837‚Äì2845 Luo Shitong, Wei Hu (2021) Diffusion probabilistic models for 3D point cloud generation. In IEEE Conference on Computer Vision and Pattern Recognition, pp 2837‚Äì2845 Luo Weijian (2023) A comprehensive survey on knowledge distillation of diffusion models. Preprint at arXiv:2304.04262 Luo Weijian (2023) A comprehensive survey on knowledge distillation of diffusion models. Preprint at arXiv:2304.04262 Luo Xiaotong, Xie Yuan, Yanyun Qu, Yun Fu (2024) Skipdiff: Adaptive skip diffusion model for high-fidelity perceptual image super-resolution. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 4017‚Äì4025 Luo Xiaotong, Xie Yuan, Yanyun Qu, Yun Fu (2024) Skipdiff: Adaptive skip diffusion model for high-fidelity perceptual image super-resolution. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 4017‚Äì4025 Luo Ziwei, Gustafsson Fredrik K, Zhao Zheng, Sj√∂lund Jens, Sch√∂n Thomas B (2023) Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 1680‚Äì1691 Luo Ziwei, Gustafsson Fredrik K, Zhao Zheng, Sj√∂lund Jens, Sch√∂n Thomas B (2023) Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 1680‚Äì1691 Ma Nanye, Goldstein Mark, Albergo Michael S, BoffiNicholas M, Vanden-Eijnden Eric, Xie Saining (2024) Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. Preprint at arXiv:2401.08740 Ma Nanye, Goldstein Mark, Albergo Michael S, BoffiNicholas M, Vanden-Eijnden Eric, Xie Saining (2024) Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. Preprint at arXiv:2401.08740 Maggiora Gabriel della, Croquevielle Luis Alberto, Deshpande Nikita, Horsley Harry, Heinis Thomas, Yakimovich Artur (2024) Conditional variational diffusion models. In The Twelfth International Conference on Learning Representations Maggiora Gabriel della, Croquevielle Luis Alberto, Deshpande Nikita, Horsley Harry, Heinis Thomas, Yakimovich Artur (2024) Conditional variational diffusion models. In The Twelfth International Conference on Learning Representations Metzger Nando, Daudt Rodrigo Caye, Schindler Konrad (2023) Guided depth super-resolution by deep anisotropic diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 18237‚Äì18246 Metzger Nando, Daudt Rodrigo Caye, Schindler Konrad (2023) Guided depth super-resolution by deep anisotropic diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 18237‚Äì18246 Midjourney. https://www.midjourney.com/home Midjourney. https://www.midjourney.com/home Miyake Daiki, Iohara Akihiro, Saito Yu, Tanaka Toshiyuki (2023) Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. Preprint at arXiv:2305.16807 Miyake Daiki, Iohara Akihiro, Saito Yu, Tanaka Toshiyuki (2023) Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. Preprint at arXiv:2305.16807 Mohan Bhandari, Arjun Neupane, Saurav Mallik, Loveleen Gaur, Hong Qin (2023) Auguring fake face images using dual input convolution neural network. J Imaging 9(1):3 Google Scholar Mohan Bhandari, Arjun Neupane, Saurav Mallik, Loveleen Gaur, Hong Qin (2023) Auguring fake face images using dual input convolution neural network. J Imaging 9(1):3 Mokady Ron, Hertz Amir, Aberman Kfir, Pritch Yael, Cohen-Or Daniel (2023) Null-text inversion for editing real images using guided diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6038‚Äì6047 Mokady Ron, Hertz Amir, Aberman Kfir, Pritch Yael, Cohen-Or Daniel (2023) Null-text inversion for editing real images using guided diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6038‚Äì6047 Moser Brian B, Shanbhag Arundhati S, Raue Federico, Frolov Stanislav, Palacio Sebastian, Dengel Andreas (2024) Diffusion models, image super-resolution and everything: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp 1‚Äì21 Moser Brian B, Shanbhag Arundhati S, Raue Federico, Frolov Stanislav, Palacio Sebastian, Dengel Andreas (2024) Diffusion models, image super-resolution and everything: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp 1‚Äì21 M√ºller Vincent C, Bostrom Nick (2013) Future progress in artificial intelligence: A survey of expert opinion. In Vincent C. M√ºller, (ed) Fundamental Issues of Artificial Intelligence - 2nd Conference on Philosophy and Theory of Artificial Intelligence, PT-AI 2013, Oxford, UK, September 21-22, 2013, selected and invited papers, vol 376 of Synthese Library, pp 555‚Äì572 M√ºller Vincent C, Bostrom Nick (2013) Future progress in artificial intelligence: A survey of expert opinion. In Vincent C. M√ºller, (ed) Fundamental Issues of Artificial Intelligence - 2nd Conference on Philosophy and Theory of Artificial Intelligence, PT-AI 2013, Oxford, UK, September 21-22, 2013, selected and invited papers, vol 376 of Synthese Library, pp 555‚Äì572 Naik Ranjita, Nushi Besmira (2023) Social biases through the text-to-image generation lens. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp 786‚Äì808 Naik Ranjita, Nushi Besmira (2023) Social biases through the text-to-image generation lens. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp 786‚Äì808 Nash Robert A, Wade Kimberley A, Brewer Rebecca J (2009) Why do doctored images distort memory? Conscious Cogn 18(3):773‚Äì780 Article Google Scholar Nash Robert A, Wade Kimberley A, Brewer Rebecca J (2009) Why do doctored images distort memory? Conscious Cogn 18(3):773‚Äì780 Article Google Scholar Neal Radford M (2001) Annealed importance sampling. Stat Comput 11:125‚Äì139 Neal Radford M (2001) Annealed importance sampling. Stat Comput 11:125‚Äì139 Neves Jo√£o C, Ruben Tolosana, Rub√©n Vera-Rodr√≠guez, Vasco Lopes, Hugo Proen√ßa, Julian Fi√©rrez (2020) Ganprintr: Improved fakes and evaluation of the state of the art in face manipulation detection. IEEE J Sel Top Signal Process 14(5):1038‚Äì1048 Article MATH Google Scholar Neves Jo√£o C, Ruben Tolosana, Rub√©n Vera-Rodr√≠guez, Vasco Lopes, Hugo Proen√ßa, Julian Fi√©rrez (2020) Ganprintr: Improved fakes and evaluation of the state of the art in face manipulation detection. IEEE J Sel Top Signal Process 14(5):1038‚Äì1048 Article MATH Google Scholar Nguyen Thao, Li Yuheng, Ojha Utkarsh, Lee Yong Jae (2023) Visual instruction inversion: Image editing via visual prompting. Preprint at arXiv:2307.14331 Nguyen Thao, Li Yuheng, Ojha Utkarsh, Lee Yong Jae (2023) Visual instruction inversion: Image editing via visual prompting. Preprint at arXiv:2307.14331 Nguyen Thao, Ojha Utkarsh, Li Yuheng, Liu Haotian, Lee Yong Jae (2024) Edit one for all: Interactive batch image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 8271‚Äì8280 Nguyen Thao, Ojha Utkarsh, Li Yuheng, Liu Haotian, Lee Yong Jae (2024) Edit one for all: Interactive batch image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 8271‚Äì8280 Ni Zixuan, Wei Longhui, Li Jiacheng, Tang Siliang, Zhuang Yueting, Tian Qi (2023) Degeneration-tuning: Using scrambled grid shield unwanted concepts from stable diffusion. In Proceedings of the 31st ACM International Conference on Multimedia, pp 8900‚Äì8909 Ni Zixuan, Wei Longhui, Li Jiacheng, Tang Siliang, Zhuang Yueting, Tian Qi (2023) Degeneration-tuning: Using scrambled grid shield unwanted concepts from stable diffusion. In Proceedings of the 31st ACM International Conference on Multimedia, pp 8900‚Äì8909 Nichol Alexander Quinn, Dhariwal Prafulla (2022) Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8162‚Äì8171 Nichol Alexander Quinn, Dhariwal Prafulla (2022) Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8162‚Äì8171 Nisha Huang, Yuxin Zhang, Weiming Dong (2024) Style-a-video: Agile diffusion for arbitrary text-based video style transfer. IEEE Signal Process Lett 31:1494‚Äì1498 Article Google Scholar Nisha Huang, Yuxin Zhang, Weiming Dong (2024) Style-a-video: Agile diffusion for arbitrary text-based video style transfer. IEEE Signal Process Lett 31:1494‚Äì1498 Article Google Scholar Nunes Lucas, Marcuzzi Rodrigo, Mersch Benedikt, Behley Jens, Stachniss Cyrill (2024) Scaling diffusion models to real-world 3d lidar scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 14770‚Äì14780 Nunes Lucas, Marcuzzi Rodrigo, Mersch Benedikt, Behley Jens, Stachniss Cyrill (2024) Scaling diffusion models to real-world 3d lidar scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 14770‚Äì14780 Olaf Ronneberger, Philipp Fischer, Thomas Brox (2015) U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention vol 9351, pp 234‚Äì241 Olaf Ronneberger, Philipp Fischer, Thomas Brox (2015) U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention vol 9351, pp 234‚Äì241 OpenAI (2023) GPT-4 technical report. Preprint at arXiv:2303.08774 OpenAI (2023) GPT-4 technical report. Preprint at arXiv:2303.08774 OpenAI (2023) Video generation models as world simulators. https://openai.com/index/video-generation-models-as-world-simulators/ OpenAI (2023) Video generation models as world simulators. https://openai.com/index/video-generation-models-as-world-simulators/ Pan Zhihong, Zhou Xin, Tian Hao (2023) Arbitrary style guidance for enhanced diffusion-based text-to-image generation. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023, pp 4450‚Äì4460 Pan Zhihong, Zhou Xin, Tian Hao (2023) Arbitrary style guidance for enhanced diffusion-based text-to-image generation. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023, pp 4450‚Äì4460 Pang Lianyu, Yin Jian, Xie Haoran, Wang Qiping, Li Qing, Mao Xudong (2023) Cross initialization for personalized text-to-image generation. Preprint at arXiv:2312.15905 Pang Lianyu, Yin Jian, Xie Haoran, Wang Qiping, Li Qing, Mao Xudong (2023) Cross initialization for personalized text-to-image generation. Preprint at arXiv:2312.15905 Parmar Gaurav, Zhang Richard, Zhu Jun-Yan (2022) On aliased resizing and surprising subtleties in GAN evaluation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp 11400‚Äì11410 Parmar Gaurav, Zhang Richard, Zhu Jun-Yan (2022) On aliased resizing and surprising subtleties in GAN evaluation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp 11400‚Äì11410 Pascal Vincent (2011) A connection between score matching and denoising autoencoders. Neural Comput 23(7):1661‚Äì1674 Article MathSciNet MATH Google Scholar Pascal Vincent (2011) A connection between score matching and denoising autoencoders. Neural Comput 23(7):1661‚Äì1674 Article MathSciNet MATH Google Scholar Patni Suraj, Agarwal Aradhye, Arora Chetan (2024) Ecodepth: Effective conditioning of diffusion models for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June, pp 28285‚Äì28295 Patni Suraj, Agarwal Aradhye, Arora Chetan (2024) Ecodepth: Effective conditioning of diffusion models for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June, pp 28285‚Äì28295 Peebles William, Xie Saining (2023) Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pp 4172‚Äì4182 Peebles William, Xie Saining (2023) Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pp 4172‚Äì4182 Pham Minh, Marshall Kelly O, Cohen Niv, Mittal Govind, Hegde Chinmay (2023) Circumventing concept erasure methods for text-to-image generative models. In The Twelfth International Conference on Learning Representations Pham Minh, Marshall Kelly O, Cohen Niv, Mittal Govind, Hegde Chinmay (2023) Circumventing concept erasure methods for text-to-image generative models. In The Twelfth International Conference on Learning Representations PNVR Koutilya, Singh Bharat, Ghosh Pallabi, Siddiquie Behjat, Jacobs David (2023) Ld-znet: A latent diffusion approach for text-based image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 4157‚Äì4168 PNVR Koutilya, Singh Bharat, Ghosh Pallabi, Siddiquie Behjat, Jacobs David (2023) Ld-znet: A latent diffusion approach for text-based image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 4157‚Äì4168 Po R, Yifan W, Golyanik V, Aberman K, Barron JT, Bermano A, Chan E, Dekel T, Holynski A, Kanazawa A, Liu CK, Liu L, Mildenhall B, Nie√É√øner M, Ommer B, Theobalt C, Wonka P, Wetzstein G (2024) State of the art on diffusion models for visual computing. Comput Graph Forum 43(2):e15063 Article Google Scholar Po R, Yifan W, Golyanik V, Aberman K, Barron JT, Bermano A, Chan E, Dekel T, Holynski A, Kanazawa A, Liu CK, Liu L, Mildenhall B, Nie√É√øner M, Ommer B, Theobalt C, Wonka P, Wetzstein G (2024) State of the art on diffusion models for visual computing. Comput Graph Forum 43(2):e15063 Article Google Scholar Prafulla Dhariwal, Alexander Nichol (2021) Diffusion models beat GANS on image synthesis. Adv Neural Inf Process Syst 34:8780‚Äì8794 MATH Google Scholar Prafulla Dhariwal, Alexander Nichol (2021) Diffusion models beat GANS on image synthesis. Adv Neural Inf Process Syst 34:8780‚Äì8794 MATH Google Scholar Qi Tianhao, Fang Shancheng, Wu Yanze, Xie Hongtao, Liu Jiawei, Chen Lang, He Qian, Zhang Yongdong (2024) Deadiff: An efficient stylization diffusion model with disentangled representations. Preprint at arXiv:2403.06951 Qi Tianhao, Fang Shancheng, Wu Yanze, Xie Hongtao, Liu Jiawei, Chen Lang, He Qian, Zhang Yongdong (2024) Deadiff: An efficient stylization diffusion model with disentangled representations. Preprint at arXiv:2403.06951 Qiang Xu, Hao Wang, Laijin Meng, Zhongjie Mi, Jianye Yuan, Hong Yan (2023) Exposing fake images generated by text-to-image diffusion models. Pattern Recognit Lett 176:76‚Äì82 Article Google Scholar Qiang Xu, Hao Wang, Laijin Meng, Zhongjie Mi, Jianye Yuan, Hong Yan (2023) Exposing fake images generated by text-to-image diffusion models. Pattern Recognit Lett 176:76‚Äì82 Article Google Scholar Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu (2024) Self-adaptive reality-guided diffusion for artifact-free super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25806‚Äì25816 Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu (2024) Self-adaptive reality-guided diffusion for artifact-free super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25806‚Äì25816 Qinsheng Zhang, Yongxin Chen (2023) Fast sampling of diffusion models with exponential integrator Qinsheng Zhang, Yongxin Chen (2023) Fast sampling of diffusion models with exponential integrator Qiu Xinmin, Han Congying, Zhang Zicheng, Li Bonan, Guo Tiande, Nie Xuecheng (2023) Diffbfr: Bootstrapping diffusion model for blind face restoration. In Proceedings of the 31st ACM International Conference on Multimedia, pp 7785‚Äì7795 Qiu Xinmin, Han Congying, Zhang Zicheng, Li Bonan, Guo Tiande, Nie Xuecheng (2023) Diffbfr: Bootstrapping diffusion model for blind face restoration. In Proceedings of the 31st ACM International Conference on Multimedia, pp 7785‚Äì7795 Qu Yiting, Shen Xinyue, He Xinlei, Backes Michael, Zannettou Savvas, Zhang Yang (2023) Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pp 3403‚Äì3417 Qu Yiting, Shen Xinyue, He Xinlei, Backes Michael, Zannettou Savvas, Zhang Yang (2023) Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pp 3403‚Äì3417 Quinn Nichol Alexander, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen (2022) GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning vol 162, pp 16784‚Äì16804 Quinn Nichol Alexander, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen (2022) GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning vol 162, pp 16784‚Äì16804 Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya (2021) Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8748‚Äì8763 Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya (2021) Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8748‚Äì8763 Ramesh Aditya, Dhariwal Prafulla, Nichol Alex, Chu Casey, Chen Mark (2022) Hierarchical text-conditional image generation with CLIP latents. Preprint at arXiv:2204.06125 Ramesh Aditya, Dhariwal Prafulla, Nichol Alex, Chu Casey, Chen Mark (2022) Hierarchical text-conditional image generation with CLIP latents. Preprint at arXiv:2204.06125 Ramesh Aditya, Dhariwal Prafulla, Nichol Alex, Chu Casey, Chen Mark (2022) Hierarchical text-conditional image generation with clip latents. Preprint at arXiv:2204.06125 Ramesh Aditya, Dhariwal Prafulla, Nichol Alex, Chu Casey, Chen Mark (2022) Hierarchical text-conditional image generation with clip latents. Preprint at arXiv:2204.06125 Ramesh Aditya, Pavlov Mikhail, Goh Gabriel, Gray Scott, Voss Chelsea, Radford Alec, Chen Mark, Sutskever Ilya (2021) Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds), Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8821‚Äì8831 Ramesh Aditya, Pavlov Mikhail, Goh Gabriel, Gray Scott, Voss Chelsea, Radford Alec, Chen Mark, Sutskever Ilya (2021) Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds), Proceedings of the 38th International Conference on Machine Learning, vol 139, pp 8821‚Äì8831 Rando Javier, Paleka Daniel, Lindner David, Heim Lennart, Tram√®r Florian (2022) Red-teaming the stable diffusion safety filter. Preprint at arXiv:2210.04610 Rando Javier, Paleka Daniel, Lindner David, Heim Lennart, Tram√®r Florian (2022) Red-teaming the stable diffusion safety filter. Preprint at arXiv:2210.04610 Reed Scott E, Akata Zeynep, Yan Xinchen, Logeswaran Lajanugen, Schiele Bernt, Lee Honglak (2016) Generative adversarial text to image synthesis. In Proceedings of the 33nd International Conference on Machine Learning, vol 48, pp 1060‚Äì1069 Reed Scott E, Akata Zeynep, Yan Xinchen, Logeswaran Lajanugen, Schiele Bernt, Lee Honglak (2016) Generative adversarial text to image synthesis. In Proceedings of the 33nd International Conference on Machine Learning, vol 48, pp 1060‚Äì1069 Ren Mengwei, Delbracio Mauricio, Talebi Hossein, Gerig Guido, Milanfar Peyman (2023) Multiscale structure guided diffusion for image deblurring. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 10721‚Äì10733 Ren Mengwei, Delbracio Mauricio, Talebi Hossein, Gerig Guido, Milanfar Peyman (2023) Multiscale structure guided diffusion for image deblurring. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 10721‚Äì10733 Rombach Robin, Blattmann Andreas, Lorenz Dominik, Esser Patrick, Ommer Bj√∂rn (2022) High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10674‚Äì10685 Rombach Robin, Blattmann Andreas, Lorenz Dominik, Esser Patrick, Ommer Bj√∂rn (2022) High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10674‚Äì10685 Saharia Chitwan, Chan William, Saxena Saurabh, Li Lala, Whang Jay, Denton Emily L., Kamyar Seyed Ghasemipour Seyed, Lopes Raphael Gontijo, Ayan Burcu Karagol, Salimans Tim, Ho Jonathan, Fleet David J, Norouzi Mohammad (2022) Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems Saharia Chitwan, Chan William, Saxena Saurabh, Li Lala, Whang Jay, Denton Emily L., Kamyar Seyed Ghasemipour Seyed, Lopes Raphael Gontijo, Ayan Burcu Karagol, Salimans Tim, Ho Jonathan, Fleet David J, Norouzi Mohammad (2022) Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems Scene generation with hierarchical latent diffusion models (2023) Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. Neuralfield-ldm. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 8496‚Äì8506 Scene generation with hierarchical latent diffusion models (2023) Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. Neuralfield-ldm. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 8496‚Äì8506 Schramowski Patrick, Brack Manuel, Deiseroth Bjorn, Kersting Kristian (2023) Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 22522‚Äì22531 Schramowski Patrick, Brack Manuel, Deiseroth Bjorn, Kersting Kristian (2023) Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 22522‚Äì22531 Seung-Lee Lee, Minjae Kang, Jong-Uk Hou (2024) Localization of diffusion model-based inpainting through the inter-intra similarity of frequency features. Image Vis Comput 148:105138 Article MATH Google Scholar Seung-Lee Lee, Minjae Kang, Jong-Uk Hou (2024) Localization of diffusion model-based inpainting through the inter-intra similarity of frequency features. Image Vis Comput 148:105138 Article MATH Google Scholar Shang Kai, Shao Mingwen, Wang Chao, Cheng Yuanshuo, Wang Shuigen (2024) Multi-domain multi-scale diffusion model for low-light image enhancement. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 4722‚Äì4730 Shang Kai, Shao Mingwen, Wang Chao, Cheng Yuanshuo, Wang Shuigen (2024) Multi-domain multi-scale diffusion model for low-light image enhancement. In Thirty-Eighth AAAI Conference on Artificial Intelligence, pp 4722‚Äì4730 Shen Xudong, Du Chao, Pang Tianyu, Lin Min, Wong Yongkang, Kankanhalli Mohan (2023) Finetuning text-to-image diffusion models for fairness. Preprint at arXiv:2311.07604 Shen Xudong, Du Chao, Pang Tianyu, Lin Min, Wong Yongkang, Kankanhalli Mohan (2023) Finetuning text-to-image diffusion models for fairness. Preprint at arXiv:2311.07604 Shen Cuihua, Kasra Mona, Pan Wenjing, Bassett Grace A, Malloch Yining, O‚ÄôBrien James F (2019) Fake images: The effects of source, intermediary, and digital media literacy on contextual assessment of image credibility online. New Media Soc 21(2):438‚Äì463. https://doi.org/10.1177/1461444818799526 Article Google Scholar Shen Cuihua, Kasra Mona, Pan Wenjing, Bassett Grace A, Malloch Yining, O‚ÄôBrien James F (2019) Fake images: The effects of source, intermediary, and digital media literacy on contextual assessment of image credibility online. New Media Soc 21(2):438‚Äì463. https://doi.org/10.1177/1461444818799526 Article Google Scholar Sohl-Dickstein Jascha, Weiss Eric A, Maheswaranathan Niru, Ganguli Surya (2015) Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , vol 37, pp 2256‚Äì2265 Sohl-Dickstein Jascha, Weiss Eric A, Maheswaranathan Niru, Ganguli Surya (2015) Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , vol 37, pp 2256‚Äì2265 Song Yang, Ermon Stefano (2019) Generative modeling by estimating gradients of the data distribution. Adv Neural Inform Process Syst 32 Song Yang, Ermon Stefano (2019) Generative modeling by estimating gradients of the data distribution. Adv Neural Inform Process Syst 32 Song Yang, Garg Sahaj, Shi Jiaxin, Ermon Stefano (2020) Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pp 574‚Äì584 Song Yang, Garg Sahaj, Shi Jiaxin, Ermon Stefano (2020) Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pp 574‚Äì584 Song Yang, Sohl-Dickstein Jascha, Kingma Diederik P, Kumar Abhishek, Ermon Stefano, Poole Ben (2020) Score-based generative modeling through stochastic differential equations. Preprint at arXiv:2011.13456 , Song Yang, Sohl-Dickstein Jascha, Kingma Diederik P, Kumar Abhishek, Ermon Stefano, Poole Ben (2020) Score-based generative modeling through stochastic differential equations. Preprint at arXiv:2011.13456 , Structured prediction for efficient text-to-image generation (2024) Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, and Sanjiv Kumar. Markovgen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 9316‚Äì9325 Structured prediction for efficient text-to-image generation (2024) Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, and Sanjiv Kumar. Markovgen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 9316‚Äì9325 Suttisak Wizadwongsa, Supasorn Suwajanakorn (2023) Accelerating guided diffusion sampling with splitting numerical methods Suttisak Wizadwongsa, Supasorn Suwajanakorn (2023) Accelerating guided diffusion sampling with splitting numerical methods Tao Xu, Zhang Pengchuan, Huang Qiuyuan, Zhang Han, Gan Zhe, Huang Xiaolei, He Xiaodong (2018) Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp 1316‚Äì1324 Tao Xu, Zhang Pengchuan, Huang Qiuyuan, Zhang Han, Gan Zhe, Huang Xiaolei, He Xiaodong (2018) Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp 1316‚Äì1324 Tim Salimans, Jonathan Ho (2022) Progressive distillation for fast sampling of diffusion models Tim Salimans, Jonathan Ho (2022) Progressive distillation for fast sampling of diffusion models Ulhaq Anwaar, Akhtar Naveed, Pogrebna Ganna (2022) Efficient diffusion models for vision: a survey. Preprint at arXiv:2210.09292 Ulhaq Anwaar, Akhtar Naveed, Pogrebna Ganna (2022) Efficient diffusion models for vision: a survey. Preprint at arXiv:2210.09292 Vahdat Arash, Kreis Karsten, Kautz Jan (2021) Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems, pp 11287‚Äì11302 Vahdat Arash, Kreis Karsten, Kautz Jan (2021) Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems, pp 11287‚Äì11302 van den Oord A√§ron, Vinyals Oriol, Kavukcuoglu Koray (2017) Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp 6306‚Äì6315 van den Oord A√§ron, Vinyals Oriol, Kavukcuoglu Koray (2017) Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp 6306‚Äì6315 Wang Hanzhang, Wang Haoran, Yang Jinze, Yu Zhongrui, Xie Zeke, Tian Lei, Xiao Xinyan, Jiang Junjun, Liu Xianming, Sun Mingming (2024) Hicast: Highly customized arbitrary style transfer with adapter enhanced diffusion models. Preprint at arXiv:2401.05870 Wang Hanzhang, Wang Haoran, Yang Jinze, Yu Zhongrui, Xie Zeke, Tian Lei, Xiao Xinyan, Jiang Junjun, Liu Xianming, Sun Mingming (2024) Hicast: Highly customized arbitrary style transfer with adapter enhanced diffusion models. Preprint at arXiv:2401.05870 Wang Haonan, Shen Qianli, Tong Yao, Zhang Yang, Kawaguchi Kenji (2024) The stronger the diffusion model, the easier the backdoor: Data poisoning to induce copyright breaches without adjusting finetuning pipeline. Preprint at arXiv:2401.04136 Wang Haonan, Shen Qianli, Tong Yao, Zhang Yang, Kawaguchi Kenji (2024) The stronger the diffusion model, the easier the backdoor: Data poisoning to induce copyright breaches without adjusting finetuning pipeline. Preprint at arXiv:2401.04136 Wang Liyan, Yang Qinyu, Wang Cong, Wang Wei, Pan Jinshan, Su Zhixun (2023) Learning a coarse-to-fine diffusion transformer for image restoration. Preprint at arXiv:2308.08730 Wang Liyan, Yang Qinyu, Wang Cong, Wang Wei, Pan Jinshan, Su Zhixun (2023) Learning a coarse-to-fine diffusion transformer for image restoration. Preprint at arXiv:2308.08730 Wang Qian, Zhang Biao, Birsak Michael, Wonka Peter (2023) Mdp: A generalized framework for text-guided image editing by manipulating the diffusion path. Preprint at arXiv:2303.16765 Wang Qian, Zhang Biao, Birsak Michael, Wonka Peter (2023) Mdp: A generalized framework for text-guided image editing by manipulating the diffusion path. Preprint at arXiv:2303.16765 Wang Yibin, Zhang Weizhong, Zheng Jianwei, Jin Cheng (2023) High-fidelity person-centric subject-to-image synthesis. Preprint at arXiv:2311.10329 Wang Yibin, Zhang Weizhong, Zheng Jianwei, Jin Cheng (2023) High-fidelity person-centric subject-to-image synthesis. Preprint at arXiv:2311.10329 Wang Yufei, Yang Wenhan, Chen Xinyuan, Wang Yaohui, Guo Lanqing, Chau Lap-Pui, Ziwei Liu Yu, Qiao Alex C, Kot, and Bihan Wen. (2024) Sinsr: Diffusion-based image super-resolution in a single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25796‚Äì25805 Wang Yufei, Yang Wenhan, Chen Xinyuan, Wang Yaohui, Guo Lanqing, Chau Lap-Pui, Ziwei Liu Yu, Qiao Alex C, Kot, and Bihan Wen. (2024) Sinsr: Diffusion-based image super-resolution in a single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 25796‚Äì25805 Wang Zhicai, Wei Longhui, Wang Tan, Chen Heyu, Hao Yanbin, Wang Xiang, He Xiangnan, Tian Qi (2024) Enhance image classification via inter-class image mixup with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 17223‚Äì17233 Wang Zhicai, Wei Longhui, Wang Tan, Chen Heyu, Hao Yanbin, Wang Xiang, He Xiangnan, Tian Qi (2024) Enhance image classification via inter-class image mixup with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 17223‚Äì17233 Wang Zhizhong, Zhao Lei, Xing Wei (2023) Stylediffusion: Controllable disentangled style transfer via diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1‚Äì6, 2023, pp 7643‚Äì7655 Wang Zhizhong, Zhao Lei, Xing Wei (2023) Stylediffusion: Controllable disentangled style transfer via diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1‚Äì6, 2023, pp 7643‚Äì7655 Wang Zhizhong, Zhao Lei, Xing Wei (2023) Stylediffusion: Controllable disentangled style transfer via diffusion models. In IEEE/CVF International Conference on Computer Vision, pp 7643‚Äì7655 Wang Zhizhong, Zhao Lei, Xing Wei (2023) Stylediffusion: Controllable disentangled style transfer via diffusion models. In IEEE/CVF International Conference on Computer Vision, pp 7643‚Äì7655 Weng Zhenzhen, S√°nchez Laura Bravo, Yeung-Levy Serena (2024) Diffusion-hpc: Synthetic data generation for human mesh recovery in challenging domains. In International Conference on 3D Vision, pp 257‚Äì267 Weng Zhenzhen, S√°nchez Laura Bravo, Yeung-Levy Serena (2024) Diffusion-hpc: Synthetic data generation for human mesh recovery in challenging domains. In International Conference on 3D Vision, pp 257‚Äì267 Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu (2023) Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. In Advances in Neural Information Processing Systems Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu (2023) Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. In Advances in Neural Information Processing Systems Xia Bin, Zhang Yulun, Wang Shiyin, Wang Yitong, Xinglong Wu, Tian Yapeng, Yang Wenming, Van Gool Luc (2023) Diffir: Efficient diffusion model for image restoration. In IEEE/CVF International Conference on Computer Vision, pp 13049‚Äì13059 Xia Bin, Zhang Yulun, Wang Shiyin, Wang Yitong, Xinglong Wu, Tian Yapeng, Yang Wenming, Van Gool Luc (2023) Diffir: Efficient diffusion model for image restoration. In IEEE/CVF International Conference on Computer Vision, pp 13049‚Äì13059 Xing Zhen, Feng Qijun, Chen Haoran, Dai Qi, Hu Han, Xu Hang, Wu Zuxuan, Jiang Yu-Gang (2024) A survey on video diffusion models. ACM Comput Surv 57(2) Xing Zhen, Feng Qijun, Chen Haoran, Dai Qi, Hu Han, Xu Hang, Wu Zuxuan, Jiang Yu-Gang (2024) A survey on video diffusion models. ACM Comput Surv 57(2) Xu Juncong, Yang Yang, Fang Han, Liu Honggu, Zhang Weiming (2024) Famsec: A few-shot-sample-based general ai-generated image detection method. Preprint at arXiv:2410.13156 Xu Juncong, Yang Yang, Fang Han, Liu Honggu, Zhang Weiming (2024) Famsec: A few-shot-sample-based general ai-generated image detection method. Preprint at arXiv:2410.13156 Xu Yilun, Deng Mingyang, Cheng Xiang, Tian Yonglong, Liu Ziming, Jaakkola Tommi S (2023) Restart sampling for improving generative processes. In Advances in Neural Information Processing Systems Xu Yilun, Deng Mingyang, Cheng Xiang, Tian Yonglong, Liu Ziming, Jaakkola Tommi S (2023) Restart sampling for improving generative processes. In Advances in Neural Information Processing Systems Xun Huang, Belongie Serge J (2017) Arbitrary style transfer in real-time with adaptive instance normalization. In IEEE International Conference on Computer Vision, pp 1510‚Äì1519 Xun Huang, Belongie Serge J (2017) Arbitrary style transfer in real-time with adaptive instance normalization. In IEEE International Conference on Computer Vision, pp 1510‚Äì1519 Xunpeng Huang, Difan Zou, Hanze Dong, Yi-An Ma, Tong Zhang (2024) Faster sampling without isoperimetry via diffusion-based Monte Carlo. In The Thirty Seventh Annual Conference on Learning Theory 247:2438‚Äì2493 Xunpeng Huang, Difan Zou, Hanze Dong, Yi-An Ma, Tong Zhang (2024) Faster sampling without isoperimetry via diffusion-based Monte Carlo. In The Thirty Seventh Annual Conference on Learning Theory 247:2438‚Äì2493 Yang Binbin, Luo Yi, Chen Ziliang, Wang Guangrun, Liang Xiaodan, Lin Liang (2023) Law-diffusion: Complex scene generation by diffusion with layouts. In IEEE/CVF International Conference on Computer Vision, pp 22612‚Äì22622 Yang Binbin, Luo Yi, Chen Ziliang, Wang Guangrun, Liang Xiaodan, Lin Liang (2023) Law-diffusion: Complex scene generation by diffusion with layouts. In IEEE/CVF International Conference on Computer Vision, pp 22612‚Äì22622 Yang Jingyuan, Feng Jiawei, Huang Hui (2024) Emogen: Emotional image content generation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 6358‚Äì6368 Yang Jingyuan, Feng Jiawei, Huang Hui (2024) Emogen: Emotional image content generation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 6358‚Äì6368 Yang Serin, Hwang Hyunmin, Ye Jong Chul (2023) Zero-shot contrastive loss for text-guided diffusion image style transfer. In IEEE/CVF International Conference on Computer Vision, pp 22816‚Äì22825 Yang Serin, Hwang Hyunmin, Ye Jong Chul (2023) Zero-shot contrastive loss for text-guided diffusion image style transfer. In IEEE/CVF International Conference on Computer Vision, pp 22816‚Äì22825 Yang Song, Stefano Ermon (2020) Improved techniques for training score-based generative models. Adv Neural Inf Process Syst 33:12438‚Äì12448 Google Scholar Yang Song, Stefano Ermon (2020) Improved techniques for training score-based generative models. Adv Neural Inf Process Syst 33:12438‚Äì12448 Yang Song, Conor Durkan, Iain Murray, Stefano Ermon (2021) Maximum likelihood training of score-based diffusion models. Adv Neural Inf Process Syst 34:1415‚Äì1428 MATH Google Scholar Yang Song, Conor Durkan, Iain Murray, Stefano Ermon (2021) Maximum likelihood training of score-based diffusion models. Adv Neural Inf Process Syst 34:1415‚Äì1428 MATH Google Scholar Yann LeCun, Yoshua Bengio, Hinton Geoffrey E (2015) Deep learning. Nat 521(7553):436‚Äì444 Article MATH Google Scholar Yann LeCun, Yoshua Bengio, Hinton Geoffrey E (2015) Deep learning. Nat 521(7553):436‚Äì444 Article MATH Google Scholar Yarom Michal, Bitton Yonatan, Changpinyo Soravit, Aharoni Roee, Herzig Jonathan, Lang Oran, Ofek Eran, Szpektor Idan (2023) What you see is what you read? improving text-image alignment evaluation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023 Yarom Michal, Bitton Yonatan, Changpinyo Soravit, Aharoni Roee, Herzig Jonathan, Lang Oran, Ofek Eran, Szpektor Idan (2023) What you see is what you read? improving text-image alignment evaluation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023 Yeh Yu-Ying, Huang Jia-Bin, Kim Changil, Xiao Lei, Nguyen-Phuoc Thu, Khan Numair, Zhang Cheng, Chandraker Manmohan, Marshall Carl S, Dong Zhao, Li Zhengqin (2024) Texturedreamer: Image-guided texture synthesis through geometry-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 4304‚Äì4314 Yeh Yu-Ying, Huang Jia-Bin, Kim Changil, Xiao Lei, Nguyen-Phuoc Thu, Khan Numair, Zhang Cheng, Chandraker Manmohan, Marshall Carl S, Dong Zhao, Li Zhengqin (2024) Texturedreamer: Image-guided texture synthesis through geometry-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 4304‚Äì4314 Yi Xunpeng, Han Xu, Zhang Hao, Tang Linfeng, Ma Jiayi (2023) Diff-retinex: Rethinking low-light image enhancement with A generative diffusion model. In IEEE/CVF International Conference on Computer Vision, pp 12268‚Äì12277 Yi Xunpeng, Han Xu, Zhang Hao, Tang Linfeng, Ma Jiayi (2023) Diff-retinex: Rethinking low-light image enhancement with A generative diffusion model. In IEEE/CVF International Conference on Computer Vision, pp 12268‚Äì12277 Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, Jaakkola Tommi S (2023) PFGM++: unlocking the potential of physics-inspired generative models. In International Conference on Machine Learning vol 202, pp 38566‚Äì38591 Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, Jaakkola Tommi S (2023) PFGM++: unlocking the potential of physics-inspired generative models. In International Conference on Machine Learning vol 202, pp 38566‚Äì38591 Yiyang Ma, Huan Yang, Yang Wenhan Fu, Jianlong Liu Jiaying (2024) Solving diffusion odes with optimal boundary conditions for better image super-resolution Yiyang Ma, Huan Yang, Yang Wenhan Fu, Jianlong Liu Jiaying (2024) Solving diffusion odes with optimal boundary conditions for better image super-resolution Yoshua Bengio, Yann LeCun, Hinton Geoffrey E (2021) Deep learning for AI. Commun ACM 64(7):58‚Äì65 Article MATH Google Scholar Yoshua Bengio, Yann LeCun, Hinton Geoffrey E (2021) Deep learning for AI. Commun ACM 64(7):58‚Äì65 Article MATH Google Scholar Yu Jiahui Xu, Yuanzhong Koh Jing, Yu Luong Thang, Gunjan Baid, Zirui Wang, Vasudevan Vijay Ku, Alexander Yang Yinfei, Karagol Ayan Burcu, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Baldridge Jason Wu, Yonghui, (2022) Scaling autoregressive models for content-rich text-to-image generation. Trans Mach Learn Res 2022 Yu Jiahui Xu, Yuanzhong Koh Jing, Yu Luong Thang, Gunjan Baid, Zirui Wang, Vasudevan Vijay Ku, Alexander Yang Yinfei, Karagol Ayan Burcu, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Baldridge Jason Wu, Yonghui, (2022) Scaling autoregressive models for content-rich text-to-image generation. Trans Mach Learn Res 2022 Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi (2024) A survey of emerging applications of diffusion probabilistic models in MRI. Meta-Radiol 2(2):100082 Article Google Scholar Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi (2024) A survey of emerging applications of diffusion probabilistic models in MRI. Meta-Radiol 2(2):100082 Article Google Scholar Zeng Haijin, Cao Jiezhang, Zhang Kai, Chen Yongyong, Luong Hiep, Philips Wilfried (2024) Unmixing diffusion for self-supervised hyperspectral image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 27820‚Äì27830 Zeng Haijin, Cao Jiezhang, Zhang Kai, Chen Yongyong, Luong Hiep, Philips Wilfried (2024) Unmixing diffusion for self-supervised hyperspectral image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 27820‚Äì27830 Zhang Chenshuang, Zhang Chaoning, Zhang Mengchun, Kweon In So (2023) Text-to-image diffusion models in generative AI: A survey. Preprint at arXiv:2303.07909 Zhang Chenshuang, Zhang Chaoning, Zhang Mengchun, Kweon In So (2023) Text-to-image diffusion models in generative AI: A survey. Preprint at arXiv:2303.07909 Zhang Chenshuang, Zhang Chaoning, Zhang Mengchun, Kweon In So (2023) Text-to-image diffusion models in generative AI: A survey. Preprint at arXiv:2303.07909 Zhang Chenshuang, Zhang Chaoning, Zhang Mengchun, Kweon In So (2023) Text-to-image diffusion models in generative AI: A survey. Preprint at arXiv:2303.07909 Zhang Chenshuang, Zhang Chaoning, Zheng Sheng, Zhang Mengchun, Qamar Maryam, Bae Sung-Ho, Kweon In So (2023) A survey on audio diffusion models: Text to speech synthesis and enhancement in generative AI. Preprint at arXiv:2303.13336 Zhang Chenshuang, Zhang Chaoning, Zheng Sheng, Zhang Mengchun, Qamar Maryam, Bae Sung-Ho, Kweon In So (2023) A survey on audio diffusion models: Text to speech synthesis and enhancement in generative AI. Preprint at arXiv:2303.13336 Zhang Eric, Wang Kai, Xu Xingqian, Wang Zhangyang, Shi Humphrey (2023) Forget-me-not: Learning to forget in text-to-image diffusion models. Preprint at arXiv:2303.17591 Zhang Eric, Wang Kai, Xu Xingqian, Wang Zhangyang, Shi Humphrey (2023) Forget-me-not: Learning to forget in text-to-image diffusion models. Preprint at arXiv:2303.17591 Zhang Han, Xu Tao, Li Hongsheng (2017) Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision , pp 5908‚Äì5916 Zhang Han, Xu Tao, Li Hongsheng (2017) Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision , pp 5908‚Äì5916 Zhang Jie, Kerschbaum Florian, Zhang Tianwei, et al (2023) Backdooring textual inversion for concept censorship. Preprint at arXiv:2308.10718 Zhang Jie, Kerschbaum Florian, Zhang Tianwei, et al (2023) Backdooring textual inversion for concept censorship. Preprint at arXiv:2308.10718 Zhang Kaiwen, Zhou Yifan, Xudong Xu, Dai Bo, Pan Xingang (2024) Diffmorpher: Unleashing the capability of diffusion models for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 7912‚Äì7921 Zhang Kaiwen, Zhou Yifan, Xudong Xu, Dai Bo, Pan Xingang (2024) Diffmorpher: Unleashing the capability of diffusion models for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 7912‚Äì7921 Zhang Xinyi, Li Naiqi, Li Jiawei, Dai Tao, Jiang Yong, Xia Shu-Tao (2023) Unsupervised surface anomaly detection with diffusion probabilistic model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 6782‚Äì6791 Zhang Xinyi, Li Naiqi, Li Jiawei, Dai Tao, Jiang Yong, Xia Shu-Tao (2023) Unsupervised surface anomaly detection with diffusion probabilistic model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 6782‚Äì6791 Zhang Yang, Tzun Teoh Tze, Hern Lim Wei, Wang Haonan, Kawaguchi Kenji (2023) Investigating copyright issues of diffusion models under practical scenarios. Preprint at arXiv:2311.12803 Zhang Yang, Tzun Teoh Tze, Hern Lim Wei, Wang Haonan, Kawaguchi Kenji (2023) Investigating copyright issues of diffusion models under practical scenarios. Preprint at arXiv:2311.12803 Zhang Yimeng, Jia Jinghan, Chen Xin, Chen Aochuan, Zhang Yihua, Liu Jiancheng, Ding Ke, Liu Sijia (2023) To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. Preprint at arXiv:2310.11868 Zhang Yimeng, Jia Jinghan, Chen Xin, Chen Aochuan, Zhang Yihua, Liu Jiancheng, Ding Ke, Liu Sijia (2023) To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. Preprint at arXiv:2310.11868 Zhang Yuxin, Huang Nisha, Tang Fan, Huang Haibin, Ma Chongyang, Dong Weiming, Xu Changsheng (2023) Inversion-based style transfer with diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17‚Äì24, 2023, pp 10146‚Äì10156 Zhang Yuxin, Huang Nisha, Tang Fan, Huang Haibin, Ma Chongyang, Dong Weiming, Xu Changsheng (2023) Inversion-based style transfer with diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17‚Äì24, 2023, pp 10146‚Äì10156 Zhao Kai, Hung Alex Ling Yu, Pang Kaifeng, Zheng Haoxin, Sung Kyunghyun (2023) Partdiff: Image super-resolution with partial diffusion models. Preprint at arXiv:2307.11926 Zhao Kai, Hung Alex Ling Yu, Pang Kaifeng, Zheng Haoxin, Sung Kyunghyun (2023) Partdiff: Image super-resolution with partial diffusion models. Preprint at arXiv:2307.11926 Zhao Pancheng, Peng Xu, Qin Pengda, Fan Deng-Ping, Zhang Zhicheng, Jia Guoli, Zhou Bowen, Yang Jufeng (2024) Lake-red: Camouflaged images generation by latent background knowledge retrieval-augmented diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 4092‚Äì4101 Zhao Pancheng, Peng Xu, Qin Pengda, Fan Deng-Ping, Zhang Zhicheng, Jia Guoli, Zhou Bowen, Yang Jufeng (2024) Lake-red: Camouflaged images generation by latent background knowledge retrieval-augmented diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 4092‚Äì4101 Zhaoyang Lyu, Kong Zhifeng Xu, Xudong Pan Liang, Dahua Lin (2022) A conditional point diffusion-refinement paradigm for 3D point cloud completion Zhaoyang Lyu, Kong Zhifeng Xu, Xudong Pan Liang, Dahua Lin (2022) A conditional point diffusion-refinement paradigm for 3D point cloud completion Zheng Sizhe, Gao Pan, Zhou Peng, Qin Jie (2024) Puff-net: Efficient style transfer with pure content and style feature fusion network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 8059‚Äì8068 Zheng Sizhe, Gao Pan, Zhou Peng, Qin Jie (2024) Puff-net: Efficient style transfer with pure content and style feature fusion network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp 8059‚Äì8068 Zhou Jinxin, Ding Tianyu, Chen Tianyi, Jiang Jiachen, Zharkov Ilya, Zhu Zhihui, Liang Luming (2023) Dream: Diffusion rectification and estimation-adaptive models. Preprint at arXiv:2312.00210 Zhou Jinxin, Ding Tianyu, Chen Tianyi, Jiang Jiachen, Zharkov Ilya, Zhu Zhihui, Liang Luming (2023) Dream: Diffusion rectification and estimation-adaptive models. Preprint at arXiv:2312.00210 Zhou Tianhao, Li Haipeng, Wang Ziyi, Luo Ao, Zhang Chen-Lin, Li Jiajun, Zeng Bing, Liu Shuaicheng (2024) Recdiffusion: Rectangling for image stitching with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June, p 2692‚Äì2701 Zhou Tianhao, Li Haipeng, Wang Ziyi, Luo Ao, Zhang Chen-Lin, Li Jiajun, Zeng Bing, Liu Shuaicheng (2024) Recdiffusion: Rectangling for image stitching with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June, p 2692‚Äì2701 Zhu Derui, Chen Dingfan, Grossklags Jens, Fritz Mario (2023) Data forensics in diffusion models: A systematic analysis of membership privacy. Preprint at arXiv:2302.07801 Zhu Derui, Chen Dingfan, Grossklags Jens, Fritz Mario (2023) Data forensics in diffusion models: A systematic analysis of membership privacy. Preprint at arXiv:2302.07801 Download references Acknowledgements The work is supported by Talent Introduction Program of Hubei Polytechnic University (23xjz14R) and 2023 Joint Open Fund for Hubei Key Laboratory of Mine Environmental Pollution Control & Remedition, Hubei Polytechnic University and Huangshi Daye Lake National Hi-tech Development Zone Science Park (23xjz08AK) and 2023 Hubei Provincial Department of Education Scientific Research Plan Guiding Project (B2023234) and Hubei Provincial Science and Technology Plan Project (KJRQ2023000110) and National Natural Science Foundation of China (61871178) and 2023 Hubei Polytechnic University-Wuhan Nanhua Industrial Equipments Engineering Co., Ltd. Joint Laboratory Open Fund. Author information Authors and Affiliations School of Electrical and Electronic Information Engineering, Hubei Polytechnic University, Huangshi, 435003, China Hang Chen, Jiaxin Hu, Meilin Ye, Chao Yu, Hao Cheng & Lei Zhang School of Electrical and Electronic Information Engineering, Hubei Polytechnic University, Huangshi, 435003, China Hang Chen, Jiaxin Hu, Meilin Ye, Chao Yu, Hao Cheng & Lei Zhang Wuchang Shouyi University College Information Science and Engineering, Wuhan, 430064, China Qian Xiang Wuchang Shouyi University College Information Science and Engineering, Wuhan, 430064, China GongQing Institute of Science and Technology, Jiujiang, 332020, China Qian Xiang GongQing Institute of Science and Technology, Jiujiang, 332020, China Wuhan Nanhua Industrial Equipments Engineering Co.,Ltd, Wuhan, 430200, China Qian Xiang Wuhan Nanhua Industrial Equipments Engineering Co.,Ltd, Wuhan, 430200, China Hang Chen View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Qian Xiang View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Jiaxin Hu View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Meilin Ye View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Chao Yu View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Hao Cheng View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Lei Zhang View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Corresponding author Correspondence to Lei Zhang . Additional information Publisher‚Äôs Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Reprints and permissions About this article Cite this article Chen, H., Xiang, Q., Hu, J. et al. Comprehensive exploration of diffusion models in image generation: a survey. Artif Intell Rev 58 , 99 (2025). https://doi.org/10.1007/s10462-025-11110-3 Download citation Accepted : 07 January 2025 Accepted : 07 January 2025 Published : 25 January 2025 Published : 25 January 2025 DOI : https://doi.org/10.1007/s10462-025-11110-3 DOI : https://doi.org/10.1007/s10462-025-11110-3 Share this article Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Image generation Diffusion models Generative models",
    "extraction_method": "requests+bs4",
    "content_length": 195418,
    "status": "success",
    "error_message": null
  },
  {
    "query": "diffusion models for image generation survey",
    "plan_item": "Diffusion Models Review",
    "plan_item_id": "plan_diff",
    "query_id": "q_diff_0",
    "source": "duckduckgo",
    "url": "https://arxiv.org/abs/2303.07909",
    "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
    "text": "Computer Science > Computer Vision and Pattern Recognition Title: Text-to-image Diffusion Models in Generative AI: A Survey Submission history HTML (experimental) References & Citations Semantic Scholar BibTeX formatted citation Bibliographic and Citation Tools Code, Data and Media Associated with this Article Recommenders and Search Tools arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .",
    "extraction_method": "requests+bs4",
    "content_length": 860,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "duckduckgo",
    "url": "https://practicum.yandex.ru/blog/chto-takoe-klasterizaciya-i-klasternyi-analiz/",
    "title": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: —á—Ç–æ —ç—Ç–æ, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è - —Ü–µ–ª–∏, –º–µ—Ç–æ–¥—ã, –∞–ª–≥–æ—Ä–∏—Ç–º ...",
    "text": "–í –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–º–∏—Å—è –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ –¥–≤—É–º –∏ –±–æ–ª–µ–µ –∫–ª–∞—Å—Ç–µ—Ä–∞–º, –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Å–æ–≤–ø–∞–¥–∞—é—Ç –Ω—É–∂–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π–ø–æ–¥—Ö–æ–¥–º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ç–∞–º, –≥–¥–µ –µ—Å—Ç—å –º–∞—Å—Å–∏–≤—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏. –ü—Ä–∏ —ç—Ç–æ–º –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∏ –æ–±—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏–Ω–∞—á–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è.–ù–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å:‚óè –ö–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –≥—Ä—É–ø–ø.‚óè –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –±–∏–∑–Ω–µ—Å–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä—ã–Ω–∫–∞.‚óè –ó–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—ã–∑–¥–æ—Ä–æ–≤–ª–µ–Ω–∏—è.‚óè –†–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤ –æ–ø—Ä–æ—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–µ–Ω–∏–π –≤ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø–∞—Ö –ª—é–¥–µ–π.‚óè SEO-–∫–ª—é—á–∏ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–º–∞—Ç–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞.‚óè –°–æ–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è –∏—Ö —É–¥–æ–±–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.–°—Ñ–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∫ –ª—é–±—ã–º –¥–∞–Ω–Ω—ã–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å. –£ –¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ü–µ–ª–∏:1. –ü–æ–Ω–∏–º–∞–Ω–∏–µ.–î–µ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ–º–æ–≥–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –¥–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã. –ü–æ—Ç–æ–º –∏—Ö –ø—Ä–æ—â–µ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ —Ä–∞–∑–Ω—ã–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞.2. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π.–ü–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø–æ—è–≤–∏—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ò—Ö –Ω—É–∂–Ω–æ –∏–∑—É—á–∏—Ç—å, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –æ—à–∏–±–∫–∞ —ç—Ç–æ –∏–ª–∏ –∫–∞–∫–æ–π-—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–µ–Ω–æ–º–µ–Ω.3. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ.–ò–Ω–æ–≥–¥–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É –∫–∞–∫–∏—Ö-—Ç–æ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —É –∫–∞–∫–∏—Ö-—Ç–æ –º–µ–Ω—å—à–µ. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–∂–µ—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ –∫–ª–∏–µ–Ω—Ç—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ ¬´–º—É–∂—á–∏–Ω—ã¬ª –ø—Ä–æ–≤–æ–¥—è—Ç –Ω–∞ —Å–∞–π—Ç–µ –≤ —Å—Ä–µ–¥–Ω–µ–º 15 –º–∏–Ω—É—Ç. –ï—Å–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –ø–æ—è–≤–∏—Ç—Å—è –Ω–æ–≤—ã–π —á–µ–ª–æ–≤–µ–∫ —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è –Ω–∞ —Å–∞–π—Ç–µ, –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å, —á—Ç–æ –¥–ª—è –Ω–µ–≥–æ –æ–Ω–æ —Ç–æ–∂–µ —Ä–∞–≤–Ω–æ 15 –º–∏–Ω—É—Ç–∞–º.4. –°–∂–∞—Ç–∏–µ.–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –º–æ–∂–Ω–æ –ø–æ–¥–µ–ª–∏—Ç—å –∏—Ö –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã, —É—Å—Ä–µ–¥–Ω–∏—Ç—å –∏ –æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É –Ω–∞ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ –º–æ—â–Ω–æ—Å—Ç–∏.–ß–∞—Å—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ‚Äî —ç—Ç–æ –Ω–µ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∞–Ω–∞–ª–∏–∑, –∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ç–∞–ø. –û–Ω–∞ –æ–±–ª–µ–≥—á–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ –Ω–∏—Ö –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –Ω–∞ –∫—É—Ä—Å–µ ¬´–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö¬ª. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–±—ã—á–Ω–æ –Ω–µ –≤—ã–¥–µ–ª—è—é—Ç. –ï–≥–æ –ø—Ä–æ–≤–æ–¥—è—Ç —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –≤—ã–±–æ—Ä –∫–æ—Ç–æ—Ä—ã—Ö —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —É—Å–ª–æ–≤–∏—è –∞–Ω–∞–ª–∏–∑–∞.–î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã:‚óè–ù–∏—Å—Ö–æ–¥—è—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.–û–±—ä–µ–∫—Ç—ã —Å–Ω–∞—á–∞–ª–∞ –ø–æ–º–µ—â–∞—é—Ç –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä, –∞ –ø–æ—Ç–æ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –≤—Å—ë –º–µ–Ω—å—à–µ –∏ –º–µ–Ω—å—à–µ.‚óè–í–æ—Å—Ö–æ–¥—è—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.–ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –Ω–∞–∑–Ω–∞—á–∞—é—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–º, –∞ –ø–æ—Ç–æ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –∏—Ö –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω—É–∂–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –¥—Ä–æ–±–ª–µ–Ω–∏—è.‚óè–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –æ—à–∏–±–∫–∏.–û–Ω–∏ —Å—Ç—Ä–æ—è—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –æ—à–∏–±–∫–∏. –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∏–∑ —Ç–∞–∫–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ‚Äî –º–µ—Ç–æ–¥ k-—Å—Ä–µ–¥–Ω–∏—Ö, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –Ω—É–∂–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞.‚óè–°–∏—Å—Ç–µ–º—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å –ø–æ–º–æ—â—å—é–Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ß–∞—â–µ –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è, –∫–æ–≥–¥–∞ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ.‚óè–õ–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –¥–µ–ª—è—Ç –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º —Å –ø–æ–º–æ—â—å—é–¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π.–¢–∞–∫–∂–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –≤—Ä—É—á–Ω—É—é. –ï—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–ª–∏—á–∞–µ—Ç—Å—è —É —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –≤—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –Ω–µ–º—É. –ö–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ —Å –≤–æ–∑—Ä–∞—Å—Ç–æ–º –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π, –∫–æ–≥–¥–∞ –≤–æ–∑—Ä–∞—Å—Ç –∏–∑–≤–µ—Å—Ç–µ–Ω –∏ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–¥–µ–ª–∏–ª–∏ –≤–µ—Å—å –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–º. –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°—É—à–∫–æ–≤–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ‚Äî –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∫–ª–∞–º—ã. –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –≤ –Ω—É–∂–Ω–æ–µ —Ä—É—Å–ª–æ, —á—Ç–æ–±—ã –∑–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–µ–Ω—å–≥–∏ –ø—Ä–∏–≤–ª–µ—á—å –º–∞–∫—Å–∏–º—É–º –∫–ª–∏–µ–Ω—Ç–æ–≤, –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ–º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —ç—Ç–æ —Å–∞–º–æ–µ ¬´–Ω—É–∂–Ω–æ–µ —Ä—É—Å–ª–æ¬ª. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫–ª–∏–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–º —Ä–µ–∫–ª–∞–º–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞, –∏ –ø—Ä–∏–≤–ª–µ—á—å –∏—Ö –∫–∞–∫ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π. –ï—Å–ª–∏ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å —Ä–µ–∫–ª–∞–º—É –≤—Å–µ–º –∫–ª–∏–µ–Ω—Ç–∞–º, —Ç–æ –∑–∞—Ç—Ä–∞—Ç—ã –±—É–¥—É—Ç –Ω–∞–º–Ω–æ–≥–æ –≤—ã—à–µ.",
    "extraction_method": "requests+bs4",
    "content_length": 3903,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "duckduckgo",
    "url": "https://habr.com/ru/articles/101338/",
    "title": "–û–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö / –•–∞–±—Ä",
    "text": "–û–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ü–æ–Ω—è—Ç–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –û—Ç–±–æ—Ä –≤—ã–±–æ—Ä–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥—É—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å—Å—è –æ–±—ä–µ–∫—Ç—ã –≤ –≤—ã–±–æ—Ä–∫–µ. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚Äì –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ä—ã —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä—É–ø–ø —Å—Ö–æ–¥–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–∫–ª–∞—Å—Ç–µ—Ä–æ–≤). –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞. –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ù–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: –ö–≤–∞–¥—Ä–∞—Ç –µ–≤–∫–ª–∏–¥–æ–≤–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –ø—Ä–∏–¥–∞–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –≤–µ—Å–∞ –±–æ–ª–µ–µ –æ—Ç–¥–∞–ª–µ–Ω–Ω—ã–º –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ –æ–±—ä–µ–∫—Ç–∞–º. –≠—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö –∫–≤–∞—Ä—Ç–∞–ª–æ–≤ (–º–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ) –≠—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–º —Ä–∞–∑–Ω–æ—Å—Ç–µ–π –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º. –í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ —ç—Ç–∞ –º–µ—Ä–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–∞–∫–∏–º –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º, –∫–∞–∫ –∏ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ï–≤–∫–ª–∏–¥–∞. –û–¥–Ω–∞–∫–æ –¥–ª—è —ç—Ç–æ–π –º–µ—Ä—ã –≤–ª–∏—è–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–Ω–æ—Å—Ç–µ–π (–≤—ã–±—Ä–æ—Å–æ–≤) —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è (—Ç.–∫. –æ–Ω–∏ –Ω–µ –≤–æ–∑–≤–æ–¥—è—Ç—Å—è –≤ –∫–≤–∞–¥—Ä–∞—Ç). –§–æ—Ä–º—É–ª–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ß–µ–±—ã—à–µ–≤–∞ –≠—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è –ø–æ–ª–µ–∑–Ω—ã–º, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–≤–∞ –æ–±—ä–µ–∫—Ç–∞ –∫–∞–∫ ¬´—Ä–∞–∑–ª–∏—á–Ω—ã–µ¬ª, –µ—Å–ª–∏ –æ–Ω–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ –∫–∞–∫–æ–π-–ª–∏–±–æ –æ–¥–Ω–æ–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ß–µ–±—ã—à–µ–≤–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ: –°—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–≤–µ–ª–∏—á–∏—Ç—å –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å –≤–µ—Å, –æ—Ç–Ω–æ—Å—è—â–∏–π—Å—è –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è. –°—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–æ—Ä–º—É–ª–µ: , –≥–¥–µ r –∏ p ‚Äì –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü–∞—Ä–∞–º–µ—Ç—Ä p –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–µ–Ω –∑–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ—Å—Ç–µ–π –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º, –ø–∞—Ä–∞–º–µ—Ç—Ä r –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–µ–Ω –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏. –ï—Å–ª–∏ –æ–±–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ‚Äì r –∏ p ‚Äî —Ä–∞–≤–Ω—ã –¥–≤—É–º, —Ç–æ —ç—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –ï–≤–∫–ª–∏–¥–∞. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∏ –ø–ª–æ—Å–∫–∏–µ. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (—Ç–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏) —Å—Ç—Ä–æ—è—Ç –Ω–µ –æ–¥–Ω–æ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ –Ω–µ–ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –∫–ª–∞—Å—Ç–µ—Ä—ã, –∞ —Å–∏—Å—Ç–µ–º—É –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ä–∞–∑–±–∏–µ–Ω–∏–π. –¢.–æ. –Ω–∞ –≤—ã—Ö–æ–¥–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –¥–µ—Ä–µ–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∫–æ—Ä–Ω–µ–º –∫–æ—Ç–æ—Ä–æ–≥–æ —è–≤–ª—è–µ—Ç—Å—è –≤—Å—è –≤—ã–±–æ—Ä–∫–∞, –∞ –ª–∏—Å—Ç—å—è–º–∏ ‚Äî –Ω–∞–∏–±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ü–ª–æ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å—Ç—Ä–æ—è—Ç –æ–¥–Ω–æ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã. –ß–µ—Ç–∫–∏–µ –∏ –Ω–µ—á–µ—Ç–∫–∏–µ. –ß–µ—Ç–∫–∏–µ (–∏–ª–∏ –Ω–µ–ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è) –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∫–∞–∂–¥–æ–º—É –æ–±—ä–µ–∫—Ç—É –≤—ã–±–æ—Ä–∫–∏ —Å—Ç–∞–≤—è—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞, —Ç.–µ. –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É. –ù–µ—á–µ—Ç–∫–∏–µ (–∏–ª–∏ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è) –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∫–∞–∂–¥–æ–º—É –æ–±—ä–µ–∫—Ç—É —Å—Ç–∞–≤—è—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–±–æ—Ä –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö —Å—Ç–µ–ø–µ–Ω—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º. –¢.–µ. –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –û–¥–∏–Ω–æ—á–Ω–∞—è —Å–≤—è–∑—å (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞) –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ (–±–ª–∏–∂–∞–π—à–∏–º–∏ —Å–æ—Å–µ–¥—è–º–∏) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–º–µ—é—Ç —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –æ–±—ä–µ–¥–∏–Ω—è—Ç—å—Å—è –≤ —Ü–µ–ø–æ—á–∫–∏. –ü–æ–ª–Ω–∞—è —Å–≤—è–∑—å (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å–æ—Å–µ–¥–µ–π) –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –Ω–∞–∏–±–æ–ª—å—à–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –º–µ–∂–¥—É –ª—é–±—ã–º–∏ –¥–≤—É–º—è –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö (—Ç.–µ. –Ω–∞–∏–±–æ–ª–µ–µ —É–¥–∞–ª–µ–Ω–Ω—ã–º–∏ —Å–æ—Å–µ–¥—è–º–∏). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, –∫–æ–≥–¥–∞ –æ–±—ä–µ–∫—Ç—ã –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø. –ï—Å–ª–∏ –∂–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–º–µ—é—Ç —É–¥–ª–∏–Ω–µ–Ω–Ω—É—é —Ñ–æ—Ä–º—É –∏–ª–∏ –∏—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–∏–ø —è–≤–ª—è–µ—Ç—Å—è ¬´—Ü–µ–ø–æ—á–µ—á–Ω—ã–º¬ª, —Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ–ø—Ä–∏–≥–æ–¥–µ–Ω. –ù–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –Ω–∏—Ö. –ú–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω, –∫–æ–≥–¥–∞ –æ–±—ä–µ–∫—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≥—Ä—É–ø–ø—ã, –æ–¥–Ω–∞–∫–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–æ –∏ –≤ —Å–ª—É—á–∞—è—Ö –ø—Ä–æ—Ç—è–∂–µ–Ω–Ω—ã—Ö (¬´—Ü–µ–ø–æ—á–µ—á–Ω–æ–≥–æ¬ª —Ç–∏–ø–∞) –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ú–µ—Ç–æ–¥ –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –º–µ—Ç–æ–¥—É –Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –ø–æ–ø–∞—Ä–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö —Ä–∞–∑–º–µ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—Ç.–µ. —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö—Å—è –≤ –Ω–∏—Ö) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Å–æ–≤–æ–≥–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞. –ü–æ—ç—Ç–æ–º—É –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω, –∫–æ–≥–¥–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–µ—Ä–∞–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ù–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∏—Ö —Ü–µ–Ω—Ç—Ä–∞–º–∏ —Ç—è–∂–µ—Å—Ç–∏. –í–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ (–º–µ–¥–∏–∞–Ω–∞) –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–µ—Å–∞ –¥–ª—è —É—á–µ—Ç–∞ —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ü–æ—ç—Ç–æ–º—É, –µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è –∏–ª–∏ –ø–æ–¥–æ–∑—Ä–µ–≤–∞—é—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ. –û–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –°–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞—Ç—å k —Ç–æ—á–µ–∫, —è–≤–ª—è—é—â–∏—Ö—Å—è –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ ¬´—Ü–µ–Ω—Ç—Ä–∞–º–∏ –º–∞—Å—Å¬ª –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –û—Ç–Ω–µ—Å—Ç–∏ –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –∫ –∫–ª–∞—Å—Ç–µ—Ä—É —Å –±–ª–∏–∂–∞–π—à–∏–º ¬´—Ü–µ–Ω—Ç—Ä–æ–º –º–∞—Å—Å¬ª. –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å ¬´—Ü–µ–Ω—Ç—Ä—ã –º–∞—Å—Å¬ª –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∏—Ö —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–∞–≤—É. –ï—Å–ª–∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –Ω–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω, –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ø. 2. –í—ã–±—Ä–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω–æ–µ –Ω–µ—á–µ—Ç–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ n –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ k –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ U —Ä–∞–∑–º–µ—Ä–∞ n x k . –ò—Å–ø–æ–ª—å–∑—É—è –º–∞—Ç—Ä–∏—Ü—É U, –Ω–∞–π—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è –Ω–µ—á–µ—Ç–∫–æ–π –æ—à–∏–±–∫–∏: , –≥–¥–µ c k ‚Äî ¬´—Ü–µ–Ω—Ç—Ä –º–∞—Å—Å¬ª –Ω–µ—á–µ—Ç–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ k : . –ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å —Ü–µ–ª—å—é —É–º–µ–Ω—å—à–µ–Ω–∏—è —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è –Ω–µ—á–µ—Ç–∫–æ–π –æ—à–∏–±–∫–∏. –í–æ–∑–≤—Ä–∞—â–∞—Ç—å—Å—è –≤ –ø. 2 –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã U –Ω–µ —Å—Ç–∞–Ω—É—Ç –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ù–µ–º–Ω–æ–≥–æ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
    "extraction_method": "requests+bs4",
    "content_length": 5233,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "duckduckgo",
    "url": "https://proprogrammer.ru/programmirovanie-i-razrabotka/obzor-algoritmov-klasterizatsii-dannix-metodi-primeri-i-primeneniya",
    "title": "–û–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: –º–µ—Ç–æ–¥—ã, –ø—Ä–∏–º–µ—Ä—ã –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
    "text": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –í –º–∏—Ä–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–∞—Ö–æ–¥—è—â–µ–π—Å—è –≤ –æ–±—à–∏—Ä–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö. –ö–∞–∂–¥—ã–π –∏–∑ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω—ã—Ö —É–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é. –ó–¥–µ—Å—å –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –Ω–∞—á–∏–Ω–∞—è –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö —Ä–∞—Å—á–µ—Ç–æ–≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ –¥–æ —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –û–¥–Ω–∏–º –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –º–µ—Ç–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏. –¢–∞–∫, —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü–∞—Ö –∏–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –º–æ–∂–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã, –æ–±—Ä–∞–∑—É—é—â–∏–µ —ç–ª–ª–∏–ø—Å–æ–∏–¥—ã –∏–ª–∏ –∏–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏–≥—É—Ä—ã –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏–Ω–¥–µ–∫—Å—ã –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –î—ç–≤–∏—Å–∞-–ë–æ–ª–¥–∏–Ω–∞ –∏–ª–∏ –∏–Ω–µ—Ä—Ü–∏—è. –≠—Ç–∏ –∏–Ω–¥–µ–∫—Å—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∏—Ö —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞, —á—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É–¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É –ø–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –û–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º k-—Å—Ä–µ–¥–Ω–∏—Ö –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ú–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ –û–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ú–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Å–µ–º–µ–π—Å—Ç–≤–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –∏—Ö —Å—Ö–æ–∂–µ—Å—Ç–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å, –ø–æ–∑–≤–æ–ª—è—è –≤—ã–¥–µ–ª—è—Ç—å –≥—Ä—É–ø–ø—ã –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–∏—Ä–æ–¥—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, —Ç–∞–∫ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—Å—Ç—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ k-—Å—Ä–µ–¥–Ω–∏—Ö, –≤—ã—á–∏—Å–ª—è—é—Ç —Ü–µ–Ω—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞—é—Ç –æ–±—ä–µ–∫—Ç—ã –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –±—ã—Å—Ç—Ä–µ–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –¥–∞–Ω–Ω—ã—Ö. –ù–∞–æ–±–æ—Ä–æ—Ç, —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —ç–ª–ª–∏–ø—Å–æ–∏–¥—ã –∏–ª–∏ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã, –≤—ã–≥–ª—è–¥—è—Ç –±–æ–ª–µ–µ –Ω–∞–≥–ª—è–¥–Ω–æ –∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –≤–æ –≤—Å–µ–º –æ–±—â–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ò—Ç–∞–∫, –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –≤—ã–¥–µ–ª—è—è –∏—Ö –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —á–µ—Ç–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–µ, –Ω–∞—Ö–æ–¥—è—â–∏–º—Å—è –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ. –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –ö–∞–∂–¥—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±–ª–∞–¥–∞–µ—Ç —Å–≤–æ–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ –∏ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ —Ü–µ–ª–µ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –ö–∞–∂–¥—ã–π –∏–∑ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á. –í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–∏—Ä–æ–¥—ã –¥–∞–Ω–Ω—ã—Ö, —Ç—Ä–µ–±—É–µ–º–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ç–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –∫–∞–∫ —Ç–∏–ø —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏, —Å–ø–æ—Å–æ–± –∑–∞–¥–∞–Ω–∏—è —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠—Ç–∏ –∞—Å–ø–µ–∫—Ç—ã –≤–ª–∏—è—é—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º k-—Å—Ä–µ–¥–Ω–∏—Ö –ú–µ—Ç–æ–¥ –∫-—Å—Ä–µ–¥–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–¥–∏–Ω –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä—É–ø–ø—ã —Å—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–¥–µ–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ –∏—Ö —Å—Ö–æ–∂–µ—Å—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–¥–µ–ª–∏—Ç—å —á–µ—Ç–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã —Å –±–ª–∏–∑–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏. –í —Ö–æ–¥–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —á–∞—Å—Ç—å—é –∫–ª–∞—Å—Ç–µ—Ä–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º–æ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–º, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–æ—á–µ–∫ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–æ–¥ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Ö —Å—Ö–æ–¥—Å—Ç–≤–µ, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –æ–±—ä–µ–∫—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–∞–∑—ã–≤–∞–µ–º—É—é –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–æ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Å–ª–æ–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–ª–∏ –∫–æ–≥–¥–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –∏—Ö –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–≤—É–º—è –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏: –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –∏ –¥–∏–≤–∏–∑–∏–æ–Ω–Ω—ã–º. –í –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞—á–∏–Ω–∞–µ—Ç —Å —Ç–æ–≥–æ, —á—Ç–æ –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã, –ø–æ–∫–∞ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±–µ—Ä—É—Ç—Å—è –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä. –î–∏–≤–∏–∑–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –Ω–∞–æ–±–æ—Ä–æ—Ç, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ–¥–Ω–æ–≥–æ –∫—Ä—É–ø–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã. –û–¥–Ω–∏–º –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –≤—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ –∫–æ—Ñ–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—é, –æ—Ç—Ä–∞–∂–∞—é—â–∞—è —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤–æ –º–Ω–æ–≥–æ–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç–æ–¥–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –û–¥–Ω–∏–º –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–∞–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —è–≤–ª—è–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º DBSCAN, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–¥–µ–ª—è—Ç—å –æ–±–ª–∞—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π —Ñ–æ—Ä–º—ã –∏ —Ä–∞–∑–º–µ—Ä–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º –≤ —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ —Ñ–æ—Ä–º–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã DBSCAN, —Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –º–µ—Ç—Ä–∏–∫, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º. –î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏–π, –≥–¥–µ –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –∏–º–µ–µ—Ç —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —à–∫–∞–ª—É –∑–Ω–∞—á–µ–Ω–∏–π. –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ñ–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ç—Ä–∏—Ü–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≥–æ–≤–æ—Ä—è—Ç –æ –ø—Ä—è–º–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫ –∫–∞–∫–æ–º—É-—Ç–æ –∫–ª–∞—Å—Å—É, –ø–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≥–¥–µ –æ–±—ä–µ–∫—Ç—ã –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É —Å —Ä–∞–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–æ—â–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≥–∏–±–∫–æ–≥–æ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –∑–¥–µ—Å—å, –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã—è–≤–ª—è—Ç—å –≥—Ä—É–ø–ø—ã —Å—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –ø–æ–ª–µ–∑–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –û–¥–∏–Ω –∏–∑ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî —ç—Ç–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è. –ü—É—Ç–µ–º –∞–Ω–∞–ª–∏–∑–∞ –¥–µ–π—Å—Ç–≤–∏–π –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ –∏–ª–∏ –º–æ–±–∏–ª—å–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –≥—Ä—É–ø–ø—ã, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Å—Ö–æ–∂–∏–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏ –∏–ª–∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —É–ª—É—á—à–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç. –î—Ä—É–≥–∏–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å —Å—Ö–æ–∂–∏–º–∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏. –ê–ª–≥–æ—Ä–∏—Ç–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ–¥–≥—Ä—É–ø–ø—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å–∏–º–ø—Ç–æ–º–∞–º–∏ –∏–ª–∏ —Ä–µ–∞–∫—Ü–∏—è–º–∏ –Ω–∞ –ª–µ—á–µ–Ω–∏–µ, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –≤ –º–µ–¥–∏—Ü–∏–Ω–µ. –ï—â–µ –æ–¥–Ω–∏–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–æ—á–µ–∫. –≠—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –∞–Ω–∞–ª–∏–∑–æ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–ª–æ—Ç–Ω—ã—Ö –∏–ª–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –Ω–∞ –∫–∞—Ä—Ç–∞—Ö, –∏–ª–∏ –¥–∞–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç. –í –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–º–æ–≥–∞–µ—Ç –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∏ –æ—Ü–µ–Ω–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π. –í –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –∏–ª–∏ —Å—Ç–∏–ª—è–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è. –í –±–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –≤ –∞–Ω–∞–ª–∏–∑–µ –≥–µ–Ω–æ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≥–µ–Ω–Ω—ã—Ö –∫–ª–∞–¥ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏—Ö —Ñ—É–Ω–∫—Ü–∏–π. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –æ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –¥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ –í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑–µ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –º–µ—Ç–æ–¥–∞–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ö–æ–∂–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–ª—è—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –í —Ä–∞–º–∫–∞—Ö —Ç–∞–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π. –û–¥–Ω–∏–º –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —è–≤–ª—è—é—Ç—Å—è –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –≥—Ä—É–ø–ø—ã —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –æ–±—ä–µ–∫—Ç—ã –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã –±—ã–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞ –ø–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –Ω–µ —Ç–æ–ª—å–∫–æ –≤–∏–¥–∏–º—ã—Ö, –Ω–æ –∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –≥—Ä—É–ø–ø, –Ω–∞—á–∏–Ω–∞—è —Å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—è –∏—Ö –≤ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Ö —Å—Ö–æ–¥—Å—Ç–≤–∞. –¢–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–¥–µ–ª–∏—Ç—å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Å–µ–≥–¥–∞ –æ—á–µ–≤–∏–¥–Ω—ã –ø—Ä–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö, –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏—Ö –≤–∑–∞–∏–º–Ω–∞—è —Å–≤—è–∑—å. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ —Å—Ç–µ–ø–µ–Ω—å —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω–∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥ –¥—Ä—É–≥–∞.",
    "extraction_method": "requests+bs4",
    "content_length": 10213,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "semantic_scholar",
    "paperId": "9ae1efcd202ab2431454304b23087045f1d5ab00",
    "url": "https://www.semanticscholar.org/paper/9ae1efcd202ab2431454304b23087045f1d5ab00",
    "title": "–ú–µ—Ç–æ–¥—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥—Ä–æ–∑–æ–≤—ã–º —Ä–∞–∑—Ä—è–¥–∞–º",
    "year": 2016,
    "authors": [
      "–ë–µ–ª–∏–∫–æ–≤–∞ –ú–∞—Ä–∏–Ω–∞ –Æ—Ä—å–µ–≤–Ω–∞",
      "–ö—Ä–µ—á–µ—Ç–æ–≤–∞ –°–≤–µ—Ç–ª–∞–Ω–∞ –Æ—Ä—å–µ–≤–Ω–∞",
      "–ü–µ—Ä–µ–ª—ã–≥–∏–Ω –ê–Ω—Ç–æ–Ω –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á"
    ],
    "abstract": null,
    "isOpenAccess": false,
    "externalIds": {
      "MAG": "2556231283",
      "CorpusId": 132664107
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "semantic_scholar",
    "paperId": "a6e0f0bf43ba33e0cad108c08d615275c8cd6096",
    "url": "https://www.semanticscholar.org/paper/a6e0f0bf43ba33e0cad108c08d615275c8cd6096",
    "title": "–ú–µ—Ç–æ–¥–´ –∫–ª–∞—Å—Ç–µ—Ä–ò–∑–∞—Ü–ò–ò –ò –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–Ø –í –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–ú –º–æ–¥—É–ª–ï —Å–∏—Å—Ç–µ–º–´ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–ô –∫–æ–º–º–µ—Ä—Ü–ò–ò Magento",
    "year": 2012,
    "authors": [
      "–û–ª–µ—Å—è –í–∏–∫—Ç–æ—Ä–æ–≤–Ω–∞ –û—Å–∏–ø—á—É–∫",
      "–¢–∞—Ç—å—è–Ω–∞ –ë–æ—Ä–∏—Å–æ–≤–Ω–∞ –®–∞—Ç–æ–≤—Å–∫–∞—è"
    ],
    "abstract": "–í —Ä–∞–±–æ—Ç–µ –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥—É–ª—è —Å–∏—Å—Ç–µ–º—ã —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ Magento —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–º–µ–Ω–Ω–æ: –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–æ—á–Ω–æ–π –Ω–∏—à–∏, –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ –ø—Ä–æ–¥–∞–∂ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –æ—Ç–¥–µ–ª–∞ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞ —Ç–æ—Ä–≥–æ–≤–æ–π —Ñ–∏—Ä–º—ã",
    "isOpenAccess": false,
    "externalIds": {
      "MAG": "1875413585",
      "DOI": "10.15587/1729-4061.2012.4001",
      "CorpusId": 227086982
    },
    "pdf_url": null,
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "not_open_access",
    "error_message": "–°—Ç–∞—Ç—å—è –Ω–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ (–ø–æ –¥–∞–Ω–Ω—ã–º API)."
  },
  {
    "query": "–º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
    "plan_item": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è",
    "plan_item_id": "plan_2",
    "query_id": "q_2_0",
    "source": "semantic_scholar",
    "paperId": "59df98c81515c53ff90cb56892c08b9cb8afa830",
    "url": "https://www.semanticscholar.org/paper/59df98c81515c53ff90cb56892c08b9cb8afa830",
    "title": "–ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê",
    "year": 2024,
    "authors": [
      "–ï. –ü. –ê–∫–∏—à–∏–Ω–∞",
      "–í. –í. –ò–≤–∞–Ω–æ–≤",
      "–ê. –í. –ö—Ä—è–Ω–µ–≤",
      "–ê. –°. –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞"
    ],
    "abstract": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –±–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í —Å–≤—è–∑–∏ —Å —á–µ–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Å–ª–µ–¥—É–µ–º—ã—Ö —è–≤–ª–µ–Ω–∏–π –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—é—â–∏—Ö —Å–≤–æ—é –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º —Ä—ã–Ω–∫–µ. –ù–∞—Å—Ç–æ—è—â–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—Ü–µ–ª–µ–Ω–æ –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–µ—Ç–æ–¥–∏–∫–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–∑—ã–≤–∞ —É –Ω–∏—Ö –ª–∏—Ü–µ–Ω–∑–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∏ –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∞ —Ç–∞–∫–∂–µ –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∑–æ–Ω—ã —Ä–∏—Å–∫–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤ –ª–∏—Ü–µ–Ω–∑–∏–π —É –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –í–∞—Ä–¥–∞, –∞ —Ç–∞–∫–∂–µ –º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è, –º–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤, –º–µ—Ç–æ–¥ –î—ç–≤–∏—Å–∞‚Äì–ë–æ—É–ª–¥–∏–Ω–∞. –°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–∏. –í –Ω–∞—Å—Ç–æ—è—â–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ ‚Ññ 101.",
    "isOpenAccess": true,
    "externalIds": {
      "DOI": "10.26583/vestnik.2024.302",
      "CorpusId": 268196151
    },
    "pdf_url": "https://vestnikmephi.elpub.ru/jour/article/download/302/286",
    "text": "–í–ï–°–¢–ù–ò–ö –ù–ê–¶–ò–û–ù–ê–õ–¨–ù–û–ì–û –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–û–ì–û –Ø–î–ï–†–ù–û–ì–û –£–ù–ò–í–ï–†–°–ò–¢–ï–¢–ê ¬´–ú–ò–§–ò¬ª, 2024, —Ç. 13, ‚Ññ 1, —Å. 22‚Äì29 ‚Äì 22 ‚Äì –ú–ê–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ú–û–î–ï–õ–ò –ò –ß–ò–°–õ–ï–ù–ù–´–ï –ú–ï–¢–û–î–´ –£–î–ö 004.62 –ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê –ï.–ü. –ê–∫–∏—à–∏–Ω–∞1, –í.–í. –ò–≤–∞–Ω–æ–≤1,2, –ê.–í. –ö—Ä—è–Ω–µ–≤1,2*, –ê.–°. –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞2 1–û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –∏–Ω—Å—Ç–∏—Ç—É—Ç —è–¥–µ—Ä–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –î—É–±–Ω–∞, 141980, –†–æ—Å—Å–∏—è 2–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —è–¥–µ—Ä–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç ¬´–ú–ò–§–ò¬ª, –ú–æ—Å–∫–≤–∞, 115409, –†–æ—Å—Å–∏—è *e-mail: AVKryanev@mephi.ru –ü–æ—Å—Ç—É–ø–∏–ª–∞ –≤ —Ä–µ–¥–∞–∫—Ü–∏—é: 10.12.2023 –ü–æ—Å–ª–µ –¥–æ—Ä–∞–±–æ—Ç–∫–∏: 10.12.2023 –ü—Ä–∏–Ω—è—Ç–∞ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: 26.12.2023 –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –±–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í —Å–≤—è–∑–∏ —Å —á–µ–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Å–ª–µ–¥—É–µ–º—ã—Ö —è–≤–ª–µ–Ω–∏–π –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏- –º–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞- —Ü–∏–∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—é- —â–∏—Ö —Å–≤–æ—é –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º —Ä—ã–Ω–∫–µ. –ù–∞—Å—Ç–æ—è—â–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—Ü–µ–ª–µ–Ω–æ –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–µ—Ç–æ–¥–∏–∫–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–∑—ã–≤–∞ —É –Ω–∏—Ö –ª–∏—Ü–µ–Ω–∑–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∏ –∏—Ç–µ—Ä–∞- —Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∞ —Ç–∞–∫–∂–µ –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑—Ä–∞–±–æ- —Ç–∞–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∑–æ–Ω—ã —Ä–∏—Å–∫–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤ –ª–∏—Ü–µ–Ω–∑–∏–π —É –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞- –Ω–∏–∑–∞—Ü–∏–π. –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –í–∞—Ä–¥–∞, –∞ —Ç–∞–∫–∂–µ –º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è, –º–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤, –º–µ—Ç–æ–¥ –î—ç–≤–∏—Å–∞‚Äì–ë–æ—É–ª–¥–∏–Ω–∞. –°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–∏. –í –Ω–∞—Å—Ç–æ—è- —â–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ ‚Ññ 101. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –º–µ—Ç–æ–¥ k-—Å—Ä–µ–¥–Ω–∏—Ö, –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, –∫—Ä–µ–¥–∏—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, –æ—Ç–∑—ã–≤ –ª–∏—Ü–µ–Ω–∑–∏–∏. DOI: 10.26583/vestnik.2024.302 EDN HUDHFW –í–í–ï–î–ï–ù–ò–ï –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –±–∞- –∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç- —Ä–æ–≤. –í —Å–≤—è–∑–∏ —Å —á–µ–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å- —Å–ª–µ–¥—É–µ–º—ã—Ö —è–≤–ª–µ–Ω–∏–π –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö [1]. –í –Ω–∞—Å—Ç–æ—è—â–µ–π —Ä–∞–±–æ—Ç–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–µ—è- —Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–∞–Ω–∫–æ–≤ –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º —Ä—ã–Ω–∫–µ –∏—Å–ø–æ–ª—å- –∑–æ–≤–∞–ª–∏—Å—å –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ú–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –¥–≤–∞ –≤–∏–¥–∞ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä- –Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∏ –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —Ä–∞–∑- –±–∏–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä—É–ø–ø—ã (–∫–ª–∞—Å—Ç–µ—Ä—ã) —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã —ç–ª–µ–º–µ–Ω—Ç—ã, –≤—Ö–æ–¥—è—â–∏–µ –≤ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É, –±—ã–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ ¬´—Å—Ö–æ–∂–∏¬ª, –∞ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø –±—ã–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ ¬´–æ—Ç–ª–∏—á–Ω—ã–º–∏¬ª –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞. –ö –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ—Ç–Ω–æ—Å–∏—Ç—Å—è, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º–µ—Ç–æ–¥ k-—Å—Ä–µ–¥- –Ω–∏—Ö, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Ö–∞—Ä–∞–∫- —Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ [2]. –ê–≤—Ç–æ—Ä–∞–º–∏ —Ä–∞–±–æ—Ç—ã [3] –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —Å–æ–≤–º–µ—Å—Ç- –Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç [4] –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—É—â–µ- —Å—Ç–≤–µ–Ω–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∑–∞–º–µ—Ç–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞- —Ü–∏–∏. –ù–∞—Å—Ç–æ—è—â–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—Ü–µ–ª–µ–Ω–æ –Ω–∞ —Ä–∞–∑—Ä–∞- –±–æ—Ç–∫—É –º–µ—Ç–æ–¥–∏–∫–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä- –≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–∑—ã–≤–∞ —É –Ω–∏—Ö –ª–∏- —Ü–µ–Ω–∑–∏–π. 1. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ö–õ–ê–°–¢–ï–†–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –†–∞–Ω–µ–µ, –≤ —Ä–∞–±–æ—Ç–∞—Ö [5, 6], –Ω–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∞—Å—å –±–∞–Ω–∫–æ–≤—Å–∫–∞—è —Å—Ñ–µ—Ä–∞ –≤ –†–æ—Å—Å–∏–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–µ–∑–∞–∫–æ–Ω- –Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –µ–µ –∑–≤–µ–Ω—å–µ–≤. –ü—Ä–∏–∑–Ω–∞- –∫–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–µ–µ –æ–ø–µ—Ä–∞—Ü–∏- –æ–Ω–Ω—É—é –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞–Ω–∫–æ–≤, –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–æ—Å—å —Å –ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê ‚Äì 23 ‚Äì –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π. –í –Ω–∞—Å—Ç–æ—è—â–µ–π —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ –æ 469 –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è—Ö (–∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö 111 –±–∞–Ω–∫–∏ —Å –æ—Ç–æ–∑–≤–∞–Ω–Ω–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π) —Å —è–Ω–≤–∞—Ä—è 2019 –ø–æ –¥–µ–∫–∞–±—Ä—å 2021 –≥., –≤—ã–±–æ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç 22 —Ñ–∏–Ω–∞–Ω—Å–æ- –≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –∏–∑ —Ñ–æ—Ä–º—ã –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ- —Å—Ç–∏ ‚Ññ 101. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ –≤—ã–±–æ—Ä–∫–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ ¬´–û—Ç–∑—ã–≤¬ª (¬´1¬ª ‚Äì —É –±–∞–Ω–∫–∞ –æ—Ç–æ–∑–≤–∞–Ω–∞ –ª–∏—Ü–µ–Ω–∑–∏—è, ¬´0¬ª ‚Äì –¥–µ–π—Å—Ç–≤—É—é—â–∏–π –±–∞–Ω–∫) –∏ –ø–æ–ª–µ ¬´–ü—Ä–∏—á–∏–Ω–∞¬ª, –≤ –∫–æ—Ç–æ—Ä–æ–º —É–∫–∞–∑–∞–Ω–∞ –ø—Ä–∏—á–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞ –ª–∏—Ü–µ–Ω–∑–∏–∏ —É –±–∞–Ω–∫–∞ –≤ —Å–ª—É—á–∞–µ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è –µ–≥–æ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. 1.1. –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ù–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ- –Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ–Ω–∏–º –º–µ—Ç–æ–¥ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –í–∞—Ä–¥–∞ [7]. –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏. –ü—Ä–∏–º–µ–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–∞–∫–æ–π –º–µ—Ä—ã ‚Äì –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –≤ –º–Ω–æ–≥–æ- –º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏, –∏ –≤—ã- —á–∏—Å–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º (1): ùëÜ(ùë•, ùë¶) = ‚àë‚àö(ùë•ùëñ‚àíùë¶ùëñ)2 ùëñ , (1) –≥–¥–µ ùë•ùëñ, ùë¶ùëñ ‚Äì –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ (–æ–±—ä–µ–∫—Ç–æ–≤), –º–µ–∂–¥—É –∫–æ—Ç–æ—Ä—ã–º–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, i = 1, 2, ..., k, k ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π [8]. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ Statistica [9] –∫–æ –≤—Å–µ–º –æ–±—ä–µ–∫—Ç–∞–º –∏—Å—Å–ª–µ- –¥–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ (—Ä–∏—Å. 1), –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –≤–∏–¥–Ω–æ, —á—Ç–æ –ø—Ä–∏ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–º —Ä–∞—Å—Å—Ç–æ- —è–Ω–∏–∏ 60 (–∫—Ä–∞—Å–Ω–∞—è –ª–∏–Ω–∏—è) –æ–±—ä–µ–∫—Ç—ã –æ–±—Ä–∞–∑—É—é—Ç —á–µ- —Ç—ã—Ä–µ –∫–ª–∞—Å—Ç–µ—Ä–∞. –†–∏—Å. 1. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞ –ö—Ä–æ–º–µ –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞, –Ω–∞–º–∏ –±—ã–ª–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ- –Ω—ã –º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è, –º–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤, –º–µ—Ç–æ–¥ –î–µ–≤–∏—Å–∞- –ë–æ—É–ª–¥–∏–Ω–∞ [10]. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –º–µ- —Ç–æ–¥–æ–≤ –∏ –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª. 1. –¢–∞–±–ª–∏—Ü–∞ 1. –û—Ü–µ–Ω–∫–∞ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ú–µ—Ç–æ–¥ –í–∞—Ä–¥–∞ 4 –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è 6 –ú–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤ 2 –ú–µ—Ç–æ–¥ –î–µ–≤–∏—Å–∞-–ë–æ—É–ª–¥–∏–Ω–∞ 5 –¢–∞–∫ –∫–∞–∫ –º–µ—Ç–æ–¥—ã –ª–æ–∫—Ç—è, —Å–∏–ª—É—ç—Ç–æ–≤ –∏ –î–µ–≤–∏—Å–∞‚Äì –ë–æ—É–ª–¥–∏–Ω–∞ –Ω–µ –¥–∞–ª–∏ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ—Å—Ç–∞–Ω–æ–≤–∏–º —Å–≤–æ–π –≤—ã–±–æ—Ä –Ω–∞ –º–µ—Ç–æ–¥–µ –í–∞—Ä–¥–∞ –∏ –ø—Ä–∏–º–µ–º —ç—Ç–æ —á–∏—Å–ª–æ –∑–∞ 4. –î–∞–ª–µ–µ –ø–µ—Ä–µ–π–¥–µ–º –∫ –º–µ—Ç–æ–¥—É k-—Å—Ä–µ–¥–Ω–∏—Ö. 1.2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ k-—Å—Ä–µ–¥–Ω–∏—Ö –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –í –º–µ—Ç–æ–¥–µ k-—Å—Ä–µ–¥–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞—Ç—å —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –¥–ª—è —á–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞ ‚Äì —á–µ- —Ç—ã—Ä–µ. –ò–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å–ª—É—á–∞–π- –Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤—ã–±–∏—Ä–∞—é—Ç—Å—è k-–Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –∫–æ—Ç–æ- —Ä—ã–µ –±—É–¥—É—Ç —Å–ª—É–∂–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —Ü–µ–Ω—Ç—Ä–∞–º–∏ –∫–ª–∞- —Å—Ç–µ—Ä–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –±–ª–∏–∂–∞–π—à–∏–π –∫ –Ω–µ–º—É —Ü–µ–Ω—Ç—Ä. –î–∞–ª–µ–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ü–µ–Ω—Ç—Ä –º–∞—Å—Å –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ- —Ä–∞. –ó–∞—Ç–µ–º –æ–±—ä–µ–∫—Ç—ã —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –≥—Ä—É–ø–ø—ã –≤–Ω–æ–≤—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç–µ–º, –∫ –∫–∞–∫–æ–º—É –∏–∑ –Ω–æ–≤—ã—Ö —Ü–µ–Ω- —Ç—Ä–æ–≤ –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –æ–∫–∞–∑–∞–ª—Å—è –±–ª–∏–∂–µ, –æ—Ü–µ–Ω–∏–≤ —ç—Ç–æ –ø–æ –º–µ—Ç—Ä–∏–∫–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è. –¢–∞–∫–∏–º –æ–±- —Ä–∞–∑–æ–º, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–≤–∞–¥—Ä–∞—Ç–∏—á- –Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–≤ —ç—Ç–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: ùëâ= ‚àë ‚àë (ùë•‚àíŒºùëñ)2 ùë•‚ààùëÜùëñ ùëò ùëñ=1 , (2) –≥–¥–µ k ‚Äì —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤; ùëÜùëñ ‚Äì –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–ª–∞- —Å—Ç–µ—Ä—ã, i = 1, 2, ..., k, –∞ Œºùëñ ‚Äì —Ü–µ–Ω—Ç—Ä—ã –º–∞—Å—Å –≤—Å–µ—Ö –≤–µ–∫- —Ç–æ—Ä–æ–≤ ùë• –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ ùëÜùëñ [11]. –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ k-—Å—Ä–µ–¥–Ω–∏—Ö –æ—Ü–µ–Ω–∏–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã, –¥–ª—è —á–µ–≥–æ –∏—Å- –ø–æ–ª—å–∑—É–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–π —Ä–∞–∑–ª–∏—á–∏—è (t-–∫—Ä–∏—Ç–µ—Ä–∏–π –°—Ç—å—é–¥–µ–Ω—Ç–∞). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∏—Å. 2. –ó–Ω–∞—á–µ–Ω–∏–µ p (—Ä-—É—Ä–æ–≤–µ–Ω—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏) –º–µ–Ω–µ–µ —á–µ–º 0.05 –≥–æ–≤–æ—Ä–∏—Ç –æ –∑–Ω–∞—á–∏–º–æ–º —Ä–∞–∑–ª–∏—á–∏–∏ –º–µ–∂–¥—É –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏. –ï.–ü. –ê–∫–∏—à–∏–Ω–∞, –í.–í. –ò–≤–∞–Ω–æ–≤, –ê.–í. –ö—Ä—è–Ω–µ–≤, –ê.–°. –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞ ‚Äì 24 ‚Äì –†–∏—Å. 2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ø–∞–∫–µ—Ç–µ Statistica –ø—Ä–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–∞ k-—Å—Ä–µ–¥–Ω–∏—Ö –í —Ç–∞–±–ª. 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Å–æ—Å—Ç–∞–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –≤–∫–ª—é- —á–∞—é—â–µ–≥–æ –º–µ—Ç–æ–¥ –í–∞—Ä–¥–∞ –∏ –º–µ—Ç–æ–¥ k-—Å—Ä–µ–¥–Ω–∏—Ö. –ò–∑ —Ç–∞–±–ª. 2 –≤–∏–¥–Ω–æ, —á—Ç–æ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã 1, 2 –∏ 3 –ø–æ–ø–∞–ª–∏ –∫—Ä—É–ø–Ω—ã–µ –∏ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏. –¢–∞–±–ª–∏—Ü–∞ 2. –°–æ—Å—Ç–∞–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ‚Ññ –∫–ª–∞- —Å—Ç–µ—Ä–∞ –°–æ—Å—Ç–∞–≤ 1 –ü–ê–û ¬´–°–±–µ—Ä–±–∞–Ω–∫¬ª 2 –ê–û ¬´–ì–∞–∑–ø—Ä–æ–º¬ª, –ê–û ¬´–í–¢–ë¬ª 3 –ê–û ¬´–ê–ª—å—Ñ–∞-–±–∞–Ω–∫¬ª, –ü–ê–û ¬´–†–æ—Å–±–∞–Ω–∫¬ª, –ê–û ¬´–†–æ—Å—Å–µ–ª—å—Ö–æ–∑–±–∞–Ω–∫¬ª, –ü–ê–û –ë–∞–Ω–∫ ¬´–§–ö –û—Ç–∫—Ä—ã—Ç–∏–µ¬ª, –ü–ê–û ¬´–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫¬ª, –ü–ê–û ¬´–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç–Ω—ã–π –±–∞–Ω–∫¬ª, –ù–ö–û –ê–û ¬´–ù–†–î¬ª, –ù–ö–û –ê–û ¬´–ù–ö–¶¬ª 4 –û—Å—Ç–∞–ª—å–Ω—ã–µ –±–∞–Ω–∫–∏ –ê–Ω–∞–ª–∏–∑ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞- –∑–∞—Ç–µ–ª–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—Ä–∏—Å. 3) –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ç–≤–µ—Ä—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –¥–ª—è –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –†–∏—Å. 3. –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: 1 ‚Äì –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä; 2 ‚Äì –≤—Ç–æ—Ä–æ–π –∫–ª–∞—Å—Ç–µ—Ä; 3 ‚Äì —Ç—Ä–µ—Ç–∏–π –∫–ª–∞—Å—Ç–µ—Ä; 4 ‚Äì —á–µ—Ç–≤–µ—Ä—Ç—ã–π –∫–ª–∞—Å—Ç–µ—Ä –ò–∑ —Ç–∞–±–ª. 3 –≤–∏–¥–Ω–æ, —á—Ç–æ –≤—Å–µ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–ª–∏—Å—å –≤ —á–µ—Ç–≤–µ—Ä—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ (111 –æ–±—ä–µ–∫—Ç–æ–≤). –¢–∞–±–ª–∏—Ü–∞ 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ ‚Ññ –∫–ª–∞- —Å—Ç–µ—Ä–∞ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ- –Ω–∞–¥–µ–∂–Ω—ã—Ö –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–±–ª–∞–≥–æ- –Ω–∞–¥–µ–∂–Ω—ã—Ö –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ 1 1 0 1 2 2 0 2 3 8 0 8 4 335 111 446 –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞- —Ü–∏–∏ –º—ã –º–æ–∂–µ–º –≤—ã–¥–µ–ª–∏—Ç—å —á–µ—Ç–≤–µ—Ä—Ç—ã–π –∫–ª–∞—Å—Ç–µ—Ä, –∫–∞–∫ —Ü–µ–ª–µ–≤–æ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ- —á–∏—Ç—å—Å—è –Ω–∞ –æ–±—ä–µ–∫—Ç–∞—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö—Å—è –≤ –Ω–µ–º. –†–µ–∞–ª–∏–∑—É–µ–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É –∫–ª–∞—Å—Ç–µ—Ä- –Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ø–µ—Ä—å –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ç–≤–µ—Ä—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞: –ø—Ä–∏–º–µ–Ω–∏–º –º–µ—Ç–æ–¥ –í–∞—Ä–¥–∞, –æ–ø—Ä–µ–¥–µ–ª–∏–º —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏–º –º–µ—Ç–æ–¥ k- —Å—Ä–µ–¥–Ω–∏—Ö. –£–∫–∞–∑–∞–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ —à–∞–≥–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –º—ã –±—É–¥–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ —Å–∞–º–æ–º—É –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º –∫–ª–∞—Å—Ç–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–æ–∏–∑–º–µ—Ä–∏- –º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞- –¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –í –Ω–∞—Å—Ç–æ—è—â–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –ø—è—Ç—å —ç—Ç–∞–ø–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ –∫–æ—Ç–æ- —Ä—ã—Ö –æ—Å—Ç–∞–ª–æ—Å—å 194 –æ–±—ä–µ–∫—Ç–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–µ–º—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ò–∑ —Ç–∞–±–ª. 4 –≤–∏–¥–Ω–æ, —á—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä ‚Ññ 1 —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ- –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –¢–∞–±–ª–∏—Ü–∞ 4. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—è—Ç–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ‚Ññ –∫–ª–∞- —Å—Ç–µ—Ä–∞ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –û–±—â–µ–µ –∫–æ–ª–∏—á–µ- —Å—Ç–≤–æ 1 62 42 104 2 0 3 3 3 14 4 18 4 26 6 32 5 4 2 6 6 7 7 14 7 13 4 17 –ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê ‚Äì 25 ‚Äì 2. –ê–ù–ê–õ–ò–ó –ó–û–ù–´ –†–ò–°–ö–ê –ù–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞–º–∏ –ø—Ä–æ- –≤–æ–¥–∏–ª—Å—è –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –Ω–∞ –ø—è—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏. –í –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ —Å–æ–¥–µ—Ä- –∂–∞—Ç—Å—è 62 –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞ –∏ 42 –Ω–µ–±–ª–∞–≥–æ- –Ω–∞–¥–µ–∂–Ω—ã—Ö. –í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –≤ –¥–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –ø–æ–ø–∞–ª–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫- —Ç–æ–≤, –º—ã –Ω–∞–∑–≤–∞–ª–∏ –æ–±—ä–µ–∫—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤—ã- —Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã–º–∏, –∏ –∏–º–µ–Ω–Ω–æ –æ–Ω–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –∑–æ–Ω—É –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞. –î–∞–ª–µ–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π 42 –Ω–µ–±–ª–∞–≥–æ–Ω–∞- –¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å—Ç—Ä–æ–∏—Ç—Å—è —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∑–æ–Ω—ã —Ä–∏—Å- –∫–∞. –¶–µ–Ω—Ç—Ä–æ–∏–¥–æ–º –Ω–∞–∑–æ–≤–µ–º —Ç–æ—á–∫—É –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∫–∞–∂–¥–∞—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–µ–¥- —Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –ø–æ –∫–∞–∂–¥–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –ù–∞ —Ä–∏—Å. 4 –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –¥–∏–∞- –≥—Ä–∞–º–º–∞ –ï–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –¥–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∏—Å. 4. –î–∏–∞–≥—Ä–∞–º–º–∞ –µ–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –¥–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ò–∑ —Ä–∏—Å. 4 –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ, —á—Ç–æ –≤—Å–µ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ –º–µ–Ω—å—à–µ–º, —á–µ–º 10-4, —è–≤–ª—è—é—Ç—Å—è –ø–æ–¥–æ- –∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏. –í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º –∏–º–µ–Ω–Ω–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–¥–≤–µ—Ä–≥–Ω—É—Ç—ã —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å. –ù–∞ —Ä–∏—Å. 5 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ–≤–∫–ª–∏–¥–æ- –≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –¥–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –ò–∑ —Ä–∏—Å—É–Ω–∫–∞ –≤–∏–¥–Ω–æ, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –Ω–µ–±–ª–∞- –≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –≤ –ø–µ—Ä–≤—ã—Ö –ø—è—Ç–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö (–≤—Å–µ–≥–æ 26 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π) —Ä–∞—Å–ø—Ä–µ- –¥–µ–ª–µ–Ω–∏—è. –≠—Ç–æ –¥–∞–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∞–≥–∞—Ç—å, —á—Ç–æ –±–ª–∞- –≥–æ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ —ç—Ç–∏—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ —è–≤–ª—è- —é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ–π –Ω–µ—Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –≤–æ–≤–ª–µ- —á–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø—Ä–∞–≤–Ω—É—é –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –†–∏—Å. 5. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –µ–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ (–∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ) –î–∞–ª–µ–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏—Å—å –¥–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ—Ä–≥–∞- –Ω–∏–∑–∞—Ü–∏–π: –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ—Ä–≥–∞- –Ω–∏–∑–∞—Ü–∏–π –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ —Ä–µ—à–µ–Ω–∏—é –ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏ –±—ã–ª–∞ –æ—Ç–æ- –∑–≤–∞–Ω–∞ –ª–∏—Ü–µ–Ω–∑–∏—è –≤ –ø–µ—Ä–∏–æ–¥ —Å —è–Ω–≤–∞—Ä—è 2022 –ø–æ –∞–≤–≥—É—Å—Ç 2023 –≥., –≤—Å–µ–≥–æ 15 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–µ- —Ä–µ—Å–µ—á–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ –±—ã–ª–∏ –∏–¥–µ–Ω—Ç–∏- —Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã —Å–µ–º—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: ùëò1 = ùëù1 ùëõ, (3) –≥–¥–µ p1 ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏; n ‚Äì –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å –æ—Ç–æ–∑–≤–∞–Ω–Ω–æ–π –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ- –∂–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–∏ —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–æ 46.6 % —Ñ–∞–∫- —Ç–æ–≤ –æ—Ç–∑—ã–≤–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ª–∏—Ü–µ–Ω–∑–∏–π. –ò–∑ –¥–∏–∞–≥—Ä–∞–º–º—ã –Ω–∞ —Ä–∏—Å. 4 –≤–∏–¥–Ω–æ, —á—Ç–æ –≤—Å–µ —Å–µ–º—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–æ—Ç–º–µ—á–µ–Ω—ã –∂–µ–ª—Ç—ã–º —Ü–≤–µ—Ç–æ–º) –æ–∫–∞–∑–∞–ª–∏—Å—å –≤ –∑–æ–Ω–µ –Ω–µ–±–ª–∞–≥–æ–Ω–∞- –¥–µ–∂–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. 3. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ö–õ–ê–°–¢–ï–†–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö –î–ê–ù–ù–´–ú, –ü–û–õ–£–ß–ï–ù–ù–´–ú –ù–ê –û–°–ù–û–í–ï –ú–ï–¢–û–î–ê –ì–õ–ê–í–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢ –í –≠–õ–ï–ö–¢–†–û–ù–ù–û–ú –í–ò–î–ï –ò –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ó–û–ù–´ –†–ò–°–ö–ê –í —Ä–∞–±–æ—Ç–µ [3] –ø–æ–∫–∞–∑–∞–Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫- —Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º- –ø–æ–Ω–µ–Ω—Ç (–ì–ö), –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä—ã—Ö –∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª- —Å—è –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑. –í –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ –ì–ö –ª–µ–∂–∞—Ç –≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã 22 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. –ù–∞ —Ä–∏—Å. 6 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≥—Ä–∞—Ñ–∏–∫ ¬´–∫–∞–º–µ–Ω–∏—Å—Ç–æ–π –æ—Å—ã–ø–∏¬ª, –ï.–ü. –ê–∫–∏—à–∏–Ω–∞, –í.–í. –ò–≤–∞–Ω–æ–≤, –ê.–í. –ö—Ä—è–Ω–µ–≤, –ê.–°. –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞ ‚Äì 26 ‚Äì –∫–æ—Ç–æ—Ä—ã–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –≥–ª–∞–≤–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ —Å–æ–≤–æ–∫—É–ø–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é. –ò–∑ —Ä–∏- —Å—É–Ω–∫–∞ –≤–∏–¥–Ω–æ, —á—Ç–æ –Ω–∞–∏–±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –¥–∞–µ—Ç –ø–µ—Ä–≤–∞—è –≥–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, –∞ –Ω–∞—á–∏–Ω–∞—è —Å —à–µ—Å—Ç–æ–π –∫–æ–º–ø–æ- –Ω–µ–Ω—Ç—ã –≤–∫–ª–∞–¥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª–µ–Ω. –ü–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—è—Ç—å –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 22), –æ–±—â–∞—è —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 96.4 %. –° —Ü–µ–ª—å—é –æ—Ü–µ–Ω–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–µ —Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–±–∏—Ç—å —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∏- —Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏–º–µ–Ω–∏–º –∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º –ì–ö –º–µ—Ç–æ–¥ –í–∞—Ä–¥–∞ (—Ä–∏—Å. 7), –º–µ—Ç–æ–¥ –î–µ–≤–∏—Å–∞‚Äì–ë–æ—É–ª–¥–∏–Ω–∞ (—Ä–∏—Å. 8), –º–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤ (—Ä–∏—Å. 9) –∏ –º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è (—Ä–∏—Å. 10). –†–∏—Å. 6. –ì—Ä–∞—Ñ–∏–∫ ¬´–∫–∞–º–µ–Ω–∏—Å—Ç–æ–π¬ª –æ—Å—ã–ø–∏ –†–∏—Å. 7. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –í–∞—Ä–¥–∞ –†–∏—Å. 8. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –î–µ–≤–∏—Å–∞‚Äì–ë–æ—É–ª–¥–∏–Ω–∞ –†–∏—Å. 9. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ —Å–∏–ª—É—ç—Ç–æ–≤ –†–∏—Å. 10. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –ª–æ–∫—Ç—è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Ä–∞–±–æ—Ç–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –∫–ª–∞- —Å—Ç–µ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ì–ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª. 5. –ò—Å—Ö–æ–¥—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏, –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º k-—Å—Ä–µ–¥–Ω–∏—Ö —Å —á–∏—Å–ª–æ–º –∫–ª–∞- —Å—Ç–µ—Ä–æ–≤ 6. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä- –≤–æ–≥–æ —ç—Ç–∞–ø–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª. 6. –¢–∞–±–ª–∏—Ü–∞ 5. –û—Ü–µ–Ω–∫–∞ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ (–ì–ö) –ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ú–µ—Ç–æ–¥ –í–∞—Ä–¥–∞ 6 –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è 6 –ú–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–æ–≤ 6 –ú–µ—Ç–æ–¥ –î–µ–≤–∏—Å–∞‚Äì–ë–æ—É–ª–¥–∏–Ω–∞ 6 –ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê ‚Äì 27 ‚Äì –¢–∞–±–ª–∏—Ü–∞ 6. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ (–ì–ö) ‚Ññ –∫–ª–∞- —Å—Ç–µ—Ä–∞ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –û–±—â–µ–µ –∫–æ–ª–∏—á–µ- —Å—Ç–≤–æ 1 2 0 2 2 4 0 4 3 337 123 460 4 1 0 1 5 1 0 1 6 1 0 1 –ù–∞ —Ä–∏—Å. 11 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞- —á–µ–Ω–∏–π –ø—è—Ç–∏ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —à–µ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤–∏–¥–Ω–æ, —á—Ç–æ —Å—Ä–µ–¥- –Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —Ç—Ä–µ—Ç—å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≤ –∑–æ–Ω–µ –Ω—É–ª—è. –†–∏—Å. 11. –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ì–ö –æ–±—ä–µ–∫—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ò–∑ —Ç–∞–±–ª. 6 –≤–∏–¥–Ω–æ, —á—Ç–æ –≤ —Ç—Ä–µ—Ç–∏–π –∫–ª–∞—Å—Ç–µ—Ä –ø–æ–ø–∞- –ª–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∫—Ä–µ- –¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. –ü–æ–≤—Ç–æ—Ä–∏–º –≤—ã—à–µ–æ–ø–∏—Å–∞–Ω- –Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É (–æ—Ü–µ–Ω–∫–∞ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∫–ª–∞- —Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º k-—Å—Ä–µ–¥–Ω–∏—Ö) –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —Ç—Ä–µ—Ç—å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞. –£–∫–∞–∑–∞–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã –∫–ª–∞—Å—Ç–µ—Ä–∏- –∑–∞—Ü–∏–∏ –±—É–¥–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ —Å–∞- –º–æ–º—É –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ –¥–µ–ª–∞–ª–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏) –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º –∫–ª–∞—Å—Ç–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä- –∂–∏—Ç —Å–æ–∏–∑–º–µ—Ä–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –í—Å–µ–≥–æ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —à–µ—Å—Ç—å —ç—Ç–∞–ø–æ–≤ –∫–ª–∞—Å—Ç–µ- —Ä–∏–∑–∞—Ü–∏–∏, –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ª—É—á–µ–Ω–æ —á–µ—Ç—ã- —Ä–µ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ö–ª–∞—Å—Ç–µ—Ä ‚Ññ 1 (—Ç–∞–±–ª. 7) —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –¢–∞–±–ª–∏—Ü–∞ 7. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ —à–µ—Å—Ç–æ–≥–æ —ç—Ç–∞–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ‚Ññ –∫–ª–∞- —Å—Ç–µ—Ä–∞ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –û–±—â–µ–µ –∫–æ–ª–∏—á–µ- —Å—Ç–≤–æ 1 53 28 81 2 18 4 22 3 46 21 67 4 2 1 3 4. –ê–ù–ê–õ–ò–ó –ó–û–ù–´ –†–ò–°–ö–ê –£–°–ï–ß–ï–ù–ù–´–• –î–ê–ù–ù–´–• –ù–∞ –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–æ- –≤–æ–¥–∏–ª—Å—è –∞–Ω–∞–ª–∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –≤ —Ç—Ä–µ—Ç—å–µ–º —Ä–∞–∑–¥–µ–ª–µ –Ω–∞—Å—Ç–æ—è—â–µ–π —Ä–∞–±–æ- —Ç—ã. –í –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è 53 –±–ª–∞–≥–æ–Ω–∞- –¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞ –∏ 28 –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö. –í —Å–≤—è–∑–∏ —Å —Ç–µ–º, —á—Ç–æ 53 –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–ø–∞–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å —ç—Ç–∏ 53 –æ–±—ä–µ–∫—Ç–∞ –ø–æ- –¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏. –û–Ω–∏ –∏ –±—É–¥—É—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—É—é –∑–æ–Ω—É —Ä–∏—Å–∫–∞. –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂- –Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö —Å—Ç—Ä–æ–∏–ª—Å—è —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞. –ù–∞ —Ä–∏—Å. 12 –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –¥–∏–∞–≥—Ä–∞–º–º–∞ –µ–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å- —Å—Ç–æ—è–Ω–∏–π –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –¥–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –í –¥–∞–Ω- –Ω–æ–º —Å–ª—É—á–∞–µ –≤ –∑–æ–Ω—É –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –ø–æ–ø–∞–¥–∞—é—Ç –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã (–∏–∑ –Ω–∏—Ö 37 –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏- –∑–∞—Ü–∏–π), –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –º–µ–Ω–µ–µ —á–µ–º 3 ÔÉó 10‚Äì3. –û—Ç–º–µ—Ç–∏–º, —á—Ç–æ –Ω–µ–±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã–π –æ–±—ä–µ–∫—Ç —Å –µ–≤–∫–ª–∏–¥–æ–≤—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º 0.0068 –±—ã–ª –ø—Ä–∏–∑–Ω–∞–Ω –∞–Ω–æ–º–∞–ª—å–Ω—ã–º –∏ –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª—Å—è –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞- –Ω–∏–∏ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞. –†–∏—Å. 12. –î–∏–∞–≥—Ä–∞–º–º–∞ –µ–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –æ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –¥–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ì–ö) –ï.–ü. –ê–∫–∏—à–∏–Ω–∞, –í.–í. –ò–≤–∞–Ω–æ–≤, –ê.–í. –ö—Ä—è–Ω–µ–≤, –ê.–°. –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞ ‚Äì 28 ‚Äì –í —Ö–æ–¥–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Å—Ç–∞–≤–∞ –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ —Ä–∞—Å—Å–º–∞—Ç- —Ä–∏–≤–∞–ª–∏—Å—å –¥–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞: –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö –±–ª–∞- –≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∑–æ–Ω—ã —Ä–∏—Å–∫–∞ –∏ –º–Ω–æ–∂–µ- —Å—Ç–≤–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ —Ä–µ—à–µ–Ω–∏—é –ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏ –±—ã–ª–∞ –æ—Ç–æ–∑–≤–∞–Ω–∞ –ª–∏—Ü–µ–Ω–∑–∏—è –≤ –ø–µ—Ä–∏–æ–¥ —Å —è–Ω- –≤–∞—Ä—è 2022 –ø–æ –∞–≤–≥—É—Å—Ç 2023 –≥., –≤—Å–µ–≥–æ 15. –í —Ä–µ–∑—É–ª—å- —Ç–∞—Ç–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ –±—ã–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã —à–µ—Å—Ç—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Å—Ç—Ä–æ–µ–Ω- –Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å 40 % —Ñ–∞–∫—Ç–æ–≤ –æ—Ç–∑—ã–≤–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ª–∏—Ü–µ–Ω–∑–∏–π: ùëò2 = ùëù2 ùëõ, (4) –≥–¥–µ p2 ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏; n ‚Äì –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å –æ—Ç–æ–∑–≤–∞–Ω–Ω–æ–π –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π. –ê–ù–ê–õ–ò–ó –ü–û–õ–£–ß–ï–ù–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –í–´–í–û–î–´ –í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ–¥–≤–µ–¥–µ–º –∏—Ç–æ–≥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: ÔÇ∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–µ—Ç–æ–¥–∏- –∫–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ø–∞–¥–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ –∑–æ–Ω—É —Ä–∏—Å–∫–∞; ÔÇ∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞- —Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–æ–¥–∞ –ì–ö –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø—Ä–æ- –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–∏; ÔÇ∑ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∞ –∑–æ–Ω–∞ —Ä–∏—Å–∫–∞, –≤ –∫–æ—Ç–æ—Ä—É—é –ø–æ- –ø–∞–ª–æ –∑–∞–º–µ—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –æ—Ä–≥–∞- –Ω–∏–∑–∞—Ü–∏–π, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ, –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å, –Ω–µ–æ–±—Ö–æ- –¥–∏–º–æ –æ–±—Ä–∞—Ç–∏—Ç—å –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ; ÔÇ∑ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ –ø–æ–∑–≤–æ–ª–∏–ª–∞ —Å–ø—Ä–æ- –≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å 40‚Äì47 % —Ñ–∞–∫—Ç–æ–≤ –æ—Ç–∑—ã–≤–∞ –ª–∏—Ü–µ–Ω–∑–∏–π —É –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π. –§–ò–ù–ê–ù–°–ò–†–û–í–ê–ù–ò–ï –†–∞–±–æ—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø—Ä–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–µ –ú–∏–Ω–∏—Å—Ç–µ—Ä- —Å—Ç–≤–∞ –Ω–∞—É–∫–∏ –∏ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–æ–µ–∫—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ‚Ññ FSWU-2023-0031). –°–ü–ò–°–û–ö –õ–ò–¢–ï–†–ê–¢–£–†–´ 1. –ê–Ω–¥–µ—Ä—Å–æ–Ω –¢. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π —Å—Ç–∞—Ç–∏- —Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑. –ú.: –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ñ–∏–∑–∏–∫–æ-–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã, 1963. 500 —Å. 2. –î—é—Ä–∞–Ω –ë., –û–¥–µ–ª–ª –ü. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑. –ú.: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, 1977. 128 —Å. 3. Dr. Tirthajyoti Sarkar. Clustering and dimen- sionality reduction techniques combined. [–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π —Ä–µ—Å—É—Ä—Å]. URL: https://github.com/ tirthajyoti/Machine- Learning-with-Python/blob/master/Clustering-Dimensio- nality-Reduction/Clustering_with_dim_reduction.ipynb (–¥–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 07.07.2023). 4. –ê–π–≤–∞–∑—è–Ω –°.–ê., –ë—É—Ö—à—Ç–∞–±–µ—Ä –í.–ú., –ï–Ω—é–∫–æ–≤ –ò.–°. –ü—Ä–∏–∫–ª–∞–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –ú.: –§–∏–Ω–∞–Ω—Å—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, 1989. 607 —Å. ISBN 5-279-00054-X. 5. –ê–∫–∏—à–∏–Ω–∞ –ï.–ü., –ò–≤–∞–Ω–æ–≤ –í.–í., –ö—Ä—è–Ω–µ–≤ –ê.–í., –ü—Ä–∏- –∫–∞–∑—á–∏–∫–æ–≤–∞ –ê.–°. C—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –¥–µ—Ä–µ- –≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏- –∫–∞—Ü–∏–∏ –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π // –í–µ—Å—Ç–Ω–∏–∫ –ù–ò–Ø–£ –ú–ò–§–ò, 2022. –¢. 11. ‚Ññ 6. –°. 442‚Äì449. https://doi.org/ 10.26583/vestnik.2022.12. 6. –ê–∫–∏—à–∏–Ω–∞ –ï.–ü., –ò–≤–∞–Ω–æ–≤ –í.–í., –ü—Ä–∏–∫–∞–∑—á–∏–∫–æ–≤–∞ –ê.–°. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –º–µ—Ç–æ–¥–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞- —Ü–∏–π, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–æ–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ –ª–µ–≥–∞- –ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω—ã—Ö –¥–æ—Ö–æ–¥–æ–≤ // –ò–∑–≤–µ—Å—Ç–∏—è –ò—Å—Å—ã–∫- –ö—É–ª—å—Å–∫–æ–≥–æ —Ñ–æ—Ä—É–º–∞ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–æ–≤ –∏ –∞—É–¥–∏—Ç–æ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π –ê–∑–∏–∏, 2022. ‚Ññ 2(37). –°. 294‚Äì296. 7. Murtagh F. Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion. // Journal of Classification, 2014. ‚Ññ 31. P. 274‚Äì295. 8. –ê–π–≤–∞–∑—è–Ω –°.–ê. –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö // –≠–∫–æ–Ω–æ–º–∏–∫–∞ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã, 1977. ‚Ññ 13 (5). –°. 968‚Äì985. 9. StatSoft Inc., 2011. STATISTICA (data analysis software system), version 10. [–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π —Ä–µ—Å—É—Ä—Å]. URL: www.statsoft.com (–¥–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 07.07.2023). 10. –ë–∞–π–º—É—Ä–∞—Ç–æ–≤ –ò.–†. –ú–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –°–ü–±.: –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò–¢–ú–û, 2020. 40 —Å. 11. –ê–π–≤–∞–∑—è–Ω –°.–õ., –ï–Ω—é–∫–æ–≤ –ò.–°., –ú–µ—à–∞–ª–∫–∏–Ω –õ.–î. –ü—Ä–∏–∫–ª–∞–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ú.: –ò–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ ¬´–§–∏–Ω–∞–Ω—Å—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞¬ª, 1985. 488 —Å. –ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –í –ó–ê–î–ê–ß–ï –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –ü–û–ü–ê–î–ê–ù–ò–Ø –ö–†–ï–î–ò–¢–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô –í –ó–û–ù–£ –†–ò–°–ö–ê ‚Äì 29 ‚Äì Vestnik Natsional'nogo Issledovatel'skogo Yadernogo Universiteta ¬´MIFI¬ª, 2024, vol. 13, no. 1, pp. 21‚Äì29 MULTIDIMENSIONAL DATA ANALYSIS IN THE TASK OF PREDICTING THE ENTRY OF CREDIT INSTITUTIONS INTO THE RISK ZONE E.P. Akishina1, V.V. Ivanov1,2, A.V. Kryanev2,*, A.S. Prikazchikova2 1Joint Institute for Nuclear Research, Dubna, 141980 Russia 2National Research Nuclear University ¬´MEPhI¬ª, Moscow, 115409 Russia *e-mail: AVKryanev@mephi.ru Received December 10, 2024; revised December 10, 2024; accepted December 26, 2024 The study of economic processes is based on the study of a large number of parameters. In this connection, in order to analyze the phenomena under study and solve prognostic problems, there is a need to use methods of multi- dimensional data analysis. The article examines the problem of identifying suspicious, from the point of view of fi- nancial solvency, credit institutions operating in the Russian market. This study is aimed at developing a methodol- ogy for multidimensional data analysis to identify suspicious credit institutions and predict the revocation of their li- censes. To solve this problem, it is proposed to use hierarchical and iterative methods of cluster analysis, as well as the principal component method. Based on these methods, a methodology for forming a risk zone has been devel- oped that makes it possible to predict the revocation of licenses from credit institutions. To determine the number of clusters, the Ward clustering method was used, as well as the elbow method, the silhouette method, and the Davis- Bouldin method. The combined use of cluster analysis methods and the principal component method made it possi- ble to demonstrate the robustness of the proposed methodology. Data from Bank Reporting Form No. 101 were used in this study. Keywords: cluster analysis methods, k-means method, principal component method, Euclidean distance, credit institution, license revocation. REFERENCES 1. Anderson T. Vvedenie v mnogomernyj statistich- eskij analiz. [Introduction to multivariate statistical anal- ysis]. Moscow, State Publishing House of Physical and Mathematical Literature Publ., 1963. 500 p. 2. Durand B., Odell P. Klasternyj analiz. [Cluster analysis]. Moscow, Statistika Publ., 1977. 128 p. 3. Dr. Tirthajyoti Sarkar. Clustering and dimen- sionality reduction techniques combined. Available at: https: //github.com/tirthajyoti/Machine- Learning-with-Python/blob/master/Clustering-Dimensio- nality-Reduction/Clustering_with_dim_reduction.ipynb (accessed 07.07.2023). 4. Ajvazyan S.A., Buhshtaber V.M., Enyukov I.S. Prikladnaya statistika. Klassifikaciya i snizhenie razmer- nosti [Applied statistics. Classification and dimension reduction]. Moscow: Finansy i statistika Publ., 1989. 607 p. ISBN 5-279-00054-X. 5. Akishina E.P., Ivanov V.V., Kryanev A.V., Pri- kazchikova A.S. Cravnitel'nyj analiz metodov derev'ev reshenij i nejronnyh setej v zadache klassifikacii kredit- nyh organizacij [Comparative analysis of decision tree and neural network methods in the problem of classifica- tion of credit institutions]. Vestnik NIYaU MIFI, 2022. Vol. 11. No. 6. Pp. 442‚Äì449. https://doi.org/10.26583/ vestnik.2022.12 (in Russian) 6. Akishina E.P., Ivanov V.V., Prikazchikova A.S. Primenenie nejronnyh setej i metoda glavnyh komponent dlya identifikacii kreditnyh organizacij, potencial'no vovlechennyh v process po legalizacii prestupnyh dohodov [Application of neural networks and the princi- pal component method for identifying credit institutions potentially involved in the process of money launder- ing]. Izvestiya Issyk-Kul'skogo foruma buhgalterov i auditorov stran Central'noj Azii, 2022. No. 2(37). Pp. 294‚Äì296 (in Russian). 7. Murtagh F. Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion. Journal of Classification, 2014. No. 31. Pp. 274‚Äì295. 8. Ayvazyan S.A. Mnogomernyj statisticheskij analiz v social'no-ekonomicheskih issledovaniyah [Multivariate statistical analysis in socio-economic research]. Ekonomika i matematicheskie metody, 1977. No. 13 (5). Pp. 968‚Äì985 (in Russian). 9. StatSoft Inc., 2011. STATISTICA (data analysis software system), version 10. Available at: www.statsoft.com (accessed 07.07.2023). 10. Baimuratov I.R. Metody avtomatizacii mash- innogo obucheniya [Methods for automating machine learning[. St. Petersburg, Universitet ITMO Publ., 2020. 40 p. 11. Ajvazyan S.L., Enyukov I.S., Meshalkin L.D. Pri- kladnaya statistika. Issledovanie zavisimostej [Applied statistics. Dependency research]. Moscow, Izdatel'stvo ¬´Finansy i statistika¬ª Publ., 1985. 488 p.",
    "extraction_method": "semantic_scholar_api+pymupdf",
    "content_length": 24533,
    "status": "success",
    "error_message": null
  },
  {
    "query": "quantum computing algorithms review",
    "plan_item": "Quantum Review",
    "plan_item_id": "plan_quantum_review",
    "query_id": "q_quantum_1",
    "source": "duckduckgo",
    "url": "https://www.nature.com/articles/npjqi201523",
    "title": "Quantum algorithms: an overview | npj Quantum Information - Nature",
    "text": "Computer science Quantum information Quantum computers are designed to outperform standard computers by running quantum algorithms. Areas in which quantum algorithms can be applied include cryptography, search and optimisation, simulation of quantum systems and solving large systems of linear equations. Here we briefly survey some known quantum algorithms, with an emphasis on a broad overview of their applications rather than their technical details. We include a discussion of recent developments and near-term applications of quantum algorithms. Similar content being viewed by others Challenges and opportunities in quantum optimization Quantum algorithms for quantum dynamics QDataSet, quantum datasets for machine learning A quantum computer is a machine designed to use quantum mechanics to do things which cannot be done by any machine based only on the laws of classical physics. Eventual applications of quantum computing range from breaking cryptographic systems to the design of new medicines. These applications are based on quantum algorithms‚Äîalgorithms that run on a quantum computer and achieve a speedup, or other efficiency improvement, over any possible classical algorithm. Although large-scale general-purpose quantum computers do not yet exist, the theory of quantum algorithms has been an active area of study for over 20 years. Here we aim to give a broad overview of quantum algorithmics, focusing on algorithms with clear applications and rigorous performance bounds, and including recent progress in the field. Contrary to a rather widespread popular belief that quantum computers have few applications, the field of quantum algorithms has developed into an area of study large enough that a brief survey such as this cannot hope to be remotely comprehensive. Indeed, at the time of writing the ‚ÄòQuantum Algorithm Zoo‚Äô website cites 262 papers on quantum algorithms. 1 There are now a number of excellent surveys about quantum algorithms, 2 ‚Äì 5 and we defer to these for details of the algorithms we cover here, and many more. In particular, we omit all discussion of how the quantum algorithms mentioned work. We will also not cover the important topics of how to actually build a quantum computer 6 (in theory or in practice) and quantum error-correction, 7 nor quantum communication complexity 8 or quantum Shannon theory. 9 Measuring quantum speedup What does it mean to say that a quantum computer solves a problem more quickly than a classical computer? As is typical in computational complexity theory, we will generally consider asymptotic scaling of complexity measures such as runtime or space usage with problem size, rather than individual problems of a fixed size. In both the classical and quantum settings, we measure runtime by the number of elementary operations used by an algorithm. In the case of quantum computation, this can be measured using the quantum circuit model, where a quantum circuit is a sequence of elementary quantum operations called quantum gates, each applied to a small number of qubits (quantum bits). To compare the performance of algorithms, we use computer science style notation O ( f ( n )), which should be interpreted as ‚Äòasymptotically upper-bounded by f ( n )‚Äô. We sometimes use basic ideas from computational complexity theory, 10 and in particular the notion of complexity classes, which are groupings of problems by difficulty. See Table 1 for informal descriptions of some important complexity classes. If a problem is said to be complete for a complexity class, then this means that it is one of the ‚Äòhardest‚Äô problems within that class: it is contained within that class, and every other problem within that class reduces to it. The hidden subgroup problem and applications to cryptography One of the first applications of quantum computers discovered was Shor‚Äôs algorithm for integer factorisation. 11 In the factorisation problem, given an integer N = p √ó q for some prime numbers p and q , our task is to determine p and q . The best classical algorithm known (the general number field sieve) runs in time exp( O (log N ) 1/3 (log log N ) 2/3 )) 12 (in fact, this is a heuristic bound; the best rigorous bound is somewhat higher), while Shor‚Äôs quantum algorithm solves this problem substantially faster, in time O (log N ) 3 ). This result might appear only of mathematical interest, were it not for the fact that the widely used RSA public-key cryptosystem 13 relies on the hardness of integer factorisation. Shor‚Äôs efficient factorisation algorithm implies that this cryptosystem is insecure against attack by a large quantum computer. As a more specific comparison than the above asymptotic runtimes, in 2010 Kleinjung et al. 14 reported classical factorisation of a 768-bit number, using hundreds of modern computers over a period of 2 years, with a total computational effort of ~10 20 operations. A detailed analysis of one fault-tolerant quantum computing architecture, 7 making reasonable assumptions about the underlying hardware, suggests that a 2,000-bit number could be factorised by a quantum computer using ~3√ó10 11 quantum gates, and approximately a billion qubits, running for just over a day at a clock rate of 10 MHz. This is clearly beyond current technology, but does not seem unrealistic as a long-term goal. Shor‚Äôs approach to integer factorisation is based on reducing the task to a special case of a mathematical problem known as the hidden subgroup problem (HSP), 15 , 16 then giving an efficient quantum algorithm for this problem. The HSP is parametrised by a group G , and Shor‚Äôs algorithm solves the case G= ‚Ñ§ . Efficient solutions to the HSP for other groups G turn out to imply efficient algorithms to break other cryptosystems; we summarise some important cases of the HSP and some of their corresponding cryptosystems in Table 2 . Two particularly interesting cases of the HSP for which polynomial-time quantum algorithms are not currently known are the dihedral and symmetric groups. A polynomial-time quantum algorithm for the former case would give an efficient algorithm for finding shortest vectors in lattices; 17 an efficient quantum algorithm for the latter case would give an efficient test for isomorphism of graphs (equivalence under relabelling of vertices). Search and optimisation One of the most basic problems in computer science is unstructured search. This problem can be formalised as follows: boxed-text It is easy to see that, with no prior information about f , any classical algorithm, which solves the unstructured search problem with certainty must evaluate f N =2 n times in the worst case. Even if we seek a randomised algorithm which succeeds, say, with probability 1/2 in the worst case, then the number of evaluations required is of order N . However, remarkably, there is a quantum algorithm due to Grover, 18 which solves this problem using O ( N ) evaluations of f in the worst case (Grover‚Äôs original algorithm solved the special case where the solution is unique; the extension to multiple solutions came slightly later. 19 ). The algorithm is bounded error; that is, it fails with probability œµ , for arbitrarily small (but fixed) œµ >0. Although f may have some kind of internal structure, Grover‚Äôs algorithm does not use this at all; we say that f is used as an oracle or black box in the algorithm. Grover‚Äôs algorithm can immediately be applied to any problem in the complexity class NP. This class encapsulates decision problems whose solutions can be checked efficiently, in the following sense: there exists an efficient classical checking algorithm A such that, for any instance of the problem where the answer should be ‚Äòyes‚Äô, there is a certificate that can be input to A such that A accepts the certificate. In other words, a certificate is a proof that the answer is ‚Äòyes‚Äô, which can be checked by A . On the other hand, for any instance where the answer should be ‚Äòno‚Äô, there should be no certificate that can make A accept it. The class NP encompasses many important problems involving optimisation and constraint satisfaction. Given a problem in NP that has a certificate of length m , by applying Grover‚Äôs algorithm to A and searching over all possible certificates, we obtain an algorithm which uses time O (2 m /2 poly( m )), rather than the O (2 m poly( m )) used by classical exhaustive search over all certificates. This (nearly) quadratic speedup is less marked than the super-polynomial speedup achieved by Shor‚Äôs algorithm, but can still be rather substantial. Indeed, if the quantum computer runs at approximately the same clock speed as the classical computer, then this implies that problem instances of approximately twice the size can be solved in a comparable amount of time. As a prototypical example of this, consider the fundamental NP-complete circuit satisfiability problem (Circuit SAT), which is illustrated in Figure 1 . An instance of this problem is a description of an electronic circuit comprising AND, OR and NOT gates which takes n bits as input and produces 1 bit of output. The task is to determine whether there exists an input to the circuit such that the output is 1. Algorithms for Circuit SAT can be used to solve a plethora of problems related to electronic circuits; examples include design automation, circuit equivalence and model checking. 20 The best classical algorithms known for Circuit SAT run in worst-case time of order 2 n for n input variables, i.e., not significantly faster than exhaustive search. 21 By applying Grover‚Äôs algorithm to the function f ( x ) which evaluates the circuit on input x ‚àà {0, 1} n , we immediately obtain a runtime of O (2 n /2 poly( n )), where the poly( n ) comes from the time taken to evaluate the circuit on a given input. Amplitude amplification Grover‚Äôs algorithm speeds up the naive classical algorithm for unstructured search. Quantum algorithms can also accelerate more complicated classical algorithms. boxed-text One way to solve the heuristic search problem classically is simply to repeatedly run A and check the output each time using f , which would result in an average of O (1/ œµ ) evaluations of f . However, a quantum algorithm due to Brassard, H√∏yer, Mosca and Tapp 22 can find w such that f ( w )=1 with only O (1/ Œµ ) uses of f , and failure probability arbitrarily close to 0, thus achieving a quadratic speedup. This algorithm is known as amplitude amplification, by analogy with classical probability amplification. The unstructured search problem discussed above fits into this framework, by simply taking A to be the algorithm, which outputs a uniformly random n -bit string. Further, if there are k inputs w ‚àà {0, 1} n such that f ( w )=1, then so we can find a w such that f ( w )=1 with O ( N / k ) queries to f . However, we could imagine A being a more complicated algorithm or heuristic targeted at a particular problem we would like to solve. For example, one of the most efficient classical algorithms known for the fundamental NP-complete constraint satisfaction problem 3-SAT is randomised and runs in time O ((4/3) n poly( n )). 23 Amplitude amplification can be applied to this algorithm to obtain a quantum algorithm with runtime O ((4/3) n /2 poly( n )), illustrating that quantum computers can speedup non-trivial classical algorithms for NP-complete problems. An interesting future direction for quantum algorithms is finding accurate approximate solutions to optimisation problems. Recent work of Farhi, Goldstone and Gutmann 24 gave the first quantum algorithm for a combinatorial task (simultaneously satisfying many linear equations of a certain form) which outperformed the best efficient classical algorithm known in terms of accuracy; in this case, measured by the fraction of equations satisfied. This inspired a more efficient classical algorithm for the same problem, 25 leaving the question open of whether quantum algorithms for optimisation problems can substantially outperform the accuracy of their classical counterparts. Box 2: Heuristic search problem Given the ability to execute a probabilistic ‚Äòguessing‚Äô algorithm Œë , and a ‚Äòchecking‚Äô function f , such that Pr[ Œë outputs w such that f ( w ) = 1] = Œµ, output w such that f ( w ) = 1. Applications of Grover‚Äôs algorithm and amplitude amplification Grover‚Äôs algorithm and amplitude amplification are powerful subroutines, which can be used as part of more complicated quantum algorithms, allowing quantum speedups to be obtained for many other problems. We list just a few of these speedups here. 1 Finding the minimum of an unsorted list of N integers (equivalently, finding the minimum of an arbitrary and initially unknown function f :{0,1} n ‚Üí ‚Ñ§ ). A quantum algorithm due to D√ºrr and H√∏yer 26 solves this problem with O ( N ) evaluations of f , giving a quadratic speedup over classical algorithms. Their algorithm is based on applying Grover‚Äôs algorithm to a function g :{0, 1} n ‚Üí{0, 1} defined by g ( x )=1, if and only if f ( x )< T for some threshold T . This threshold is initially random, and then updated as inputs x are found such that f ( x ) is below the threshold. Finding the minimum of an unsorted list of N integers (equivalently, finding the minimum of an arbitrary and initially unknown function f :{0,1} n ‚Üí ‚Ñ§ ). A quantum algorithm due to D√ºrr and H√∏yer 26 solves this problem with O ( N ) evaluations of f , giving a quadratic speedup over classical algorithms. Their algorithm is based on applying Grover‚Äôs algorithm to a function g :{0, 1} n ‚Üí{0, 1} defined by g ( x )=1, if and only if f ( x )< T for some threshold T . This threshold is initially random, and then updated as inputs x are found such that f ( x ) is below the threshold. 2 Determining graph connectivity. To determine whether a graph on N vertices is connected requires time of order N 2 classically in the worst case. D√ºrr, Heiligman, H√∏yer and Mhalla 27 give a quantum algorithm which solves this problem in time O ( N 3/2 ), up to logarithmic factors, as well as efficient algorithms for some other graph-theoretic problems (strong connectivity, minimum spanning tree, shortest paths). Determining graph connectivity. To determine whether a graph on N vertices is connected requires time of order N 2 classically in the worst case. D√ºrr, Heiligman, H√∏yer and Mhalla 27 give a quantum algorithm which solves this problem in time O ( N 3/2 ), up to logarithmic factors, as well as efficient algorithms for some other graph-theoretic problems (strong connectivity, minimum spanning tree, shortest paths). 3 Pattern matching, a fundamental problem in text processing and bioinformatics. Here the task is to find a given pattern P of length M within a text T of length N , where the pattern and the text are strings over some alphabet. Ramesh and Vinay have given a quantum algorithm 28 which solves this problem in time O ( N + M ) , up to logarithmic factors, as compared with the best possible classical complexity O ( N + M ). These are both worst-case time bounds, but one could also consider an average-case setting where the text and pattern are both picked at random. Here the quantum speedup is more pronounced: there is a quantum algorithm which combines amplitude amplification with ideas from the dihedral hidden subgroup problem and runs in time O ( N / M 2 O ( log M ) ) up to logarithmic factors, as compared with the best possible classical runtime O ( N / M + N ) . 29 This is a super-polynomial speedup when M is large. Pattern matching, a fundamental problem in text processing and bioinformatics. Here the task is to find a given pattern P of length M within a text T of length N , where the pattern and the text are strings over some alphabet. Ramesh and Vinay have given a quantum algorithm 28 which solves this problem in time O ( N + M ) , up to logarithmic factors, as compared with the best possible classical complexity O ( N + M ). These are both worst-case time bounds, but one could also consider an average-case setting where the text and pattern are both picked at random. Here the quantum speedup is more pronounced: there is a quantum algorithm which combines amplitude amplification with ideas from the dihedral hidden subgroup problem and runs in time O ( N / M 2 O ( log M ) ) up to logarithmic factors, as compared with the best possible classical runtime O ( N / M + N ) . 29 This is a super-polynomial speedup when M is large. Adiabatic optimisation An alternative approach to quantum combinatorial optimisation is provided by the quantum adiabatic algorithm. 30 The adiabatic algorithm can be applied to any constraint satisfaction problem (CSP) where we are given a sequence of constraints applied to some input bits, and are asked to output an assignment to the input bits, which maximises the number of satisfied constraints. Many such problems are NP-complete and of significant practical interest. The basic idea behind the algorithm is physically motivated, and based around a correspondence between CSPs and physical systems. We start with a quantum state that is the uniform superposition over all possible solutions to the CSP. This is the ground (lowest energy) state of a Hamiltonian that can be prepared easily. This Hamiltonian is then gradually modified to give a new Hamiltonian whose ground state encodes the solution maximising the number of satisfied constraints. The quantum adiabatic theorem guarantees that if this process is carried out slowly enough, the system will remain in its ground-state throughout; in particular, the final state gives an optimal solution to the CSP. The key phrase here is ‚Äòslowly enough‚Äô; for some instances of CSPs on n bits, the time required for this evolution might be exponential in n . Unlike the algorithms described in the rest of this survey, the adiabatic algorithm lacks general, rigorous worst-case upper bounds on its runtime. Although numerical experiments can be carried out to evaluate its performance on small instances, 31 this rapidly becomes infeasible for larger problems. One can construct problem instances on which the standard adiabatic algorithm provably takes exponential time; 32 , 33 however, changing the algorithm can evade some of these arguments. 34 , 35 The adiabatic algorithm can be implemented on a universal quantum computer. However, it also lends itself to direct implementation on a physical system whose Hamiltonian can be varied smoothly between the desired initial and final Hamiltonians. The most prominent exponent of this approach is the company D-Wave Systems, which has built large machines designed to implement this algorithm, 36 with the most recent such machine (‚ÄòD-Wave 2X‚Äô) announced as having up to 1,152 qubits. For certain instances of CSPs, these machines have been demonstrated to outperform classical solvers running on a standard computer, 37 , 38 although the speedup (or otherwise) seems to have a rather subtle dependence on the problem instance, classical solver compared, and measure of comparison. 38 , 39 As well as the theoretical challenges to the adiabatic algorithm mentioned above, there are also some significant practical challenges faced by the D-Wave system. In particular, these machines do not remain in their ground state throughout, but are in a thermal state above absolute zero. Because of this, the algorithm actually performed has some similarities to classical simulated annealing, and is hence known as ‚Äòquantum annealing‚Äô. It is unclear at present whether a quantum speedup predicted for the adiabatic algorithm would persist in this setting. Box 1: Unstructured search problem Given the ability to evaluate a function f :{0, 1} n ‚Üí {0, 1}, find x such that f ( x ) = 1, if such an x exists; otherwise, output ‚Äònot found‚Äô. Quantum simulation In the early days of classical computing, one of the main applications of computer technology was the simulation of physical systems (such applications arguably go back at least as far as the Antikythera mechanism from the 2nd century BC.). Similarly, the most important early application of quantum computers is likely to be the simulation of quantum systems. 40 ‚Äì 42 Applications of quantum simulation include quantum chemistry, superconductivity, metamaterials and high-energy physics. Indeed, one might expect that quantum simulation would help us understand any system where quantum mechanics has a role. The word ‚Äòsimulation‚Äô can be used to describe a number of problems, but in quantum computation is often used to mean the problem of calculating the dynamical properties of a system. This can be stated more specifically as: given a Hamiltonian H describing a physical system, and a description of an initial state | œà „Äâ of that system, output some property of the state | œà t „Äâ = e ‚àí i H t | œà „Äâ corresponding to evolving the system according to that Hamiltonian for time t . As all quantum systems obey the Schr√∂dinger equation, this is a fundamentally important task; however, the exponential complexity of completely describing general quantum states suggests that it should be impossible to achieve efficiently classically, and indeed no efficient general classical algorithm for quantum simulation is known. This problem originally motivated Feynman to ask whether a quantum computer could efficiently simulate quantum mechanics. 43 A general-purpose quantum computer can indeed efficiently simulate quantum mechanics in this sense for many physically realistic cases, such as systems with locality restrictions on their interactions. 44 Given a description of a quantum state | œà „Äâ , a description of H , and a time t , the quantum simulation algorithm produces an approximation to the state | œà t „Äâ . Measurements can then be performed on this state to determine quantities of interest about it. The algorithm runs in time polynomial in the size of the system being simulated (the number of qubits) and the desired evolution time, giving an exponential speedup over the best general classical algorithms known. However, there is still room for improvement and quantum simulation remains a topic of active research. Examples include work on increasing the accuracy of quantum simulation while retaining a fast runtime; 45 optimising the algorithm for particular applications such as quantum chemistry; 46 and exploring applications to new areas such as quantum field theory. 47 The above, very general, approach is sometimes termed digital quantum simulation: we assume we have a large-scale, general-purpose quantum computer and run the quantum simulation algorithm on it. By contrast, in analogue quantum simulation we mimic one physical system directly using another. That is, if we would like to simulate a system with some Hamiltonian H , then we build another system that can be described by a Hamiltonian approximating H . We have gained something by doing this if the second system is easier to build, to run or to extract information from than the first. For certain systems analogue quantum simulation may be significantly easier to implement than digital quantum simulation, at the expense of being less flexible. It is therefore expected that analogue simulators outperforming their classical counterparts will be implemented first. 40 In classical computer science the concept of the random walk or Markov chain is a powerful algorithmic tool, and is often applied to search and sampling problems. Quantum walks provide a similarly powerful and general framework for designing fast quantum algorithms. Just as a random walk algorithm is based on the simulated motion of a particle moving randomly within some underlying graph structure, a quantum walk is based on the simulated coherent quantum evolution of a particle moving on a graph. Quantum walk algorithms generally take advantage of one of two ways in which quantum walks outperform random walks: faster hitting (the time taken to find a target vertex from a source vertex), and faster mixing (the time taken to spread out over all vertices after starting from one source vertex). For some graphs, hitting time of quantum walks can be exponentially less than their classical counterparts. 48 , 49 The separation between quantum and classical mixing time can be quadratic, but no more than this 50 (approximately). Nevertheless, fast mixing has proven to be a very useful tool for obtaining general speedups over classical algorithms. Figure 2 illustrates special cases of three families of graphs for which quantum walks display faster hitting than random walks: the hypercube, the ‚Äòglued trees‚Äô graph, and the ‚Äòglued trees‚Äô graph with a random cycle added in the middle. This third example is of particular interest because quantum walks can be shown to outperform any classical algorithm for navigating the graph, even one not based on a random walk. A continuous-time quantum walk that starts at the entrance (on the left-hand side) and runs for time O (log N ) finds the exit (on the right-hand side) with probability at least 1/poly(log N ). However, any classical algorithm requires time of order N 1/6 to find the exit. 51 Intuitively, the classical algorithm can progress quickly at first, but then gets ‚Äòstuck‚Äô in the random part in the middle of the graph. The coherence and symmetry of the quantum walk make it essentially blind to this randomness, and it efficiently progresses from the left to the right. A possibly surprising application of quantum walks is fast evaluation of boolean formulae. A boolean formula on N binary inputs x 1 ,‚Ä¶, x N is a tree whose internal vertices represent AND ( ‚àß ), OR ( ‚à® ) or NOT (¬¨) gates applied to their child vertices, and whose N leaves are labelled with the bits x 1 ,‚Ä¶, x N . Two such formulae are illustrated in Figure 3 . There is a quantum algorithm which allows any such formula to be evaluated in slightly more than O ( N 1/2 ) operations, 52 while it is known that for a wide class of boolean formulae, any randomised classical algorithm requires time of order N 0.753‚Ä¶ in the worst case. 53 The quantum algorithm is based around the use and analysis of a quantum walk on the tree graph corresponding to the formula‚Äôs structure. A particularly interesting special case of the formula evaluation problem which displays a quantum speedup is evaluating AND‚ÄìOR trees, which corresponds to deciding the winner of certain two-player games. Quantum walks can also be used to obtain a very general speedup over classical algorithms based on Markov chains. A discrete-time Markov chain is a stochastic linear map defined in terms of its transition matrix P, where P xy is the probability of transitioning from state x to state y . Many classical search algorithms can be expressed as simulating a Markov chain for a certain number of steps, and checking whether a transition is made to a ‚Äòmarked‚Äô element for which we are searching. A key parameter that determines the efficiency of this classical algorithm is the spectral gap Œ¥ of the Markov chain (i.e., the difference between the largest and second-largest eigenvalues of P ). There are analogous algorithms based on quantum walks, which improve the dependence on Œ¥ quadratically, from 1/ Œ¥ to 1/ Œ¥ . 54 ‚Äì 56 This framework has been used to obtain quantum speedups for a variety of problems, 4 ranging from determining whether a list of integers are all distinct 54 to finding triangles in graphs. 57 Solving linear equations and related tasks A fundamental task in mathematics, engineering and many areas of science is solving systems of linear equations. We are given an N √ó N matrix A , and a vector b ‚àà ‚Ñù N , and are asked to output x such that A x = b . This problem can be solved in time polynomial in N by straightfoward linear-algebra methods such as Gaussian elimination. Can we do better than this? This appears difficult, because even to write down the answer x would require time of order N . The quantum algorithm of Harrow, Hassidim and Lloyd 58 (HHL) for solving systems of linear equations sidesteps this issue by ‚Äòsolving‚Äô the equations in a peculiarly quantum sense: given the ability to create the quantum state | b „Äâ = ‚àë i =1 N b i | i „Äâ , and access to A , the algorithm outputs a state approximately proportional to | x „Äâ = ‚àë i =1 N x i | i „Äâ . This is an N -dimensional quantum state, which can be stored in O (log N ) qubits. The algorithm runs efficiently, assuming that the matrix A satisfies some constraints. First, it should be sparse‚Äîeach row should contain at most d elements, for some d ‚â™ N . We should be given access to A via an function to which we can pass a row number r and an index i , with 1 ‚©Ω i ‚©Ω d , and which returns the i ‚Äôth nonzero element in the r ‚Äôth row. Also, the condition number Œ∫ = ‚à• A ‚àí 1 ‚à• ‚à• A ‚à• , a parameter measuring the numerical instability of A , should be small. Assuming these constraints, | x „Äâ can be approximately produced in time polynomial in log N , d and Œ∫ . 58 , 59 If d and Œ∫ are small, then this is an exponential improvement on standard classical algorithms. Indeed, one can even show that achieving a similar runtime classically would imply that classical computers could efficiently simulate any polynomial-time quantum computation. 58 Of course, rather than giving as output the entirety of x , the algorithm produces an N -dimensional quantum state | x „Äâ ; to output the solution x itself would then involve making many measurements to completely characterise the state, requiring time of order N in general. However, we may not be interested in the entirety of the solution, but rather in some global property of it. Such properties can be determined by performing measurements on | x „Äâ . For example, the HHL algorithm allows one to efficiently determine whether two sets of linear equations have the same solution, 59 as well as many other simple global properties. 60 The HHL algorithm is likely to find applications in settings where the matrix A and the vector b are generated algorithmically, rather than being written down explicitly. One such setting is the finite element method (FEM) in engineering. Recent work by Clader, Jacobs and Sprouse has shown that the HHL algorithm, when combined with a preconditioner, can be used to solve an electromagnetic scattering problem via the FEM. 60 The same algorithm, or closely related ideas, can also be applied to problems beyond linear equations themselves. These include solving large systems of differential equations, 61 , 62 data fitting 63 and various tasks in machine learning. 64 It should be stressed that in all these cases the quantum algorithm ‚Äòsolves‚Äô these problems in the same sense as the HHL algorithm solves them: it starts with a quantum state and produces a quantum state as output. Whether this is a reasonable definition of ‚Äòsolution‚Äô depends on the application, and again may depend on whether the input is produced algorithmically or is provided explicitly as arbitrary data. 65 Few-qubit applications and experimental implementations Although progress in experimental quantum computation has been rapid, there is still some way to go before we have a large-scale, general-purpose quantum computer, with current implementations consisting of only a few qubits. Any quantum computation operating on at most 20‚Äì30 qubits in the standard quantum circuit model can be readily simulated on a modern classical computer. Therefore, existing implementations of quantum algorithms should usually be seen as proofs of principle rather than demonstrating genuine speedups over the classical state of the art. In Table 3 we highlight some experimental implementations of algorithms discussed here, focusing on the largest problem sizes considered thus far (although note that one has to be careful when using ‚Äòproblem size‚Äô as a proxy for ‚Äòdifficulty in solving on a quantum computer‚Äô. 66 ). An important algorithm omitted from this table is quantum simulation. This topic has been studied since the early days of quantum computation (with perhaps the first implementation dating from 1999 67 , and quantum simulations have now been implemented, in some form, on essentially every technological platform for quantum computing. One salient example is the use of a 6-qubit ion trap system 68 to implement general digital quantum simulation; we defer to survey papers 40 , 42 , 69 , 70 for many further references. It is arguable that quantum simulations, in the sense of measuring the properties of a controllable quantum system, have already been performed that are beyond the reach of current classical simulation techniques. 71 One application of digital quantum simulation which is currently the object of intensive study is quantum chemistry. 46 , 72 , 73 Classical techniques for molecular simulation are currently limited to molecules with 50‚Äì70 spin orbitals. 72 As each spin orbital corresponds to a qubit in the quantum simulation algorithm, a quantum computer with as few as 100 logical qubits could perform calculations beyond the reach of classical computation. The challenge in this context is optimising the simulation time; although polynomial in the number of orbitals, this initially seemed prohibitively long, 73 but was rapidly improved via detailed analysis. 72 The demonstration of quantum algorithms which outperform classical computation in the more immediate future is naturally of considerable interest. The Boson Sampling problem was designed specifically to address this. 74 Boson Sampling is the problem of sampling from the probability distribution obtained by feeding n photons through a linear-optical network on m modes, where m ‚â´ n . This task is conjectured to be hard for a classical computer to solve. 74 However, Boson Sampling can be performed easily using linear optics, and indeed several small-scale experimental demonstrations with a few photons have already been carried out. 75 Although Boson Sampling was not originally designed with practical applications in mind, subsequent work has explored connections to molecular vibrations and vibronic spectra. 76 , 77 One way in which quantum algorithms can be profitably applied for even very small-scale systems is ‚Äòquantum algorithmic thinking‚Äô: applying ideas from the design of quantum algorithms to physical problems. An example of this from the field of quantum metrology is the development of high-precision quantum measurement schemes based on quantum phase estimation algorithms. 78 Zero-qubit applications We finally mention some ways in which quantum computing is useful now, without the need for an actual large-scale quantum computer. These can be summarised as the application of ideas from the theory of quantum computation to other scientific and mathematical fields. First, the field of Hamiltonian complexity aims to characterise the complexity of computing quantities of interest about quantum-mechanical systems. A prototypical example, and a fundamental task in quantum chemistry and condensed-matter physics, is the problem of approximately calculating the ground-state energy of a physical system described by a local Hamiltonian. It is now known that this problem‚Äîalong with many others‚Äîis Quantum Merlin‚ÄìArthur (QMA)-complete. 79 , 80 Problems in the class QMA are those which can be efficiently solved by a quantum computer given access to a quantum ‚Äòcertificate‚Äô. We imagine that the certificate is produced by an all-powerful (yet untrustworthy) wizard Merlin, and given to a polynomial-time human Arthur to check; hence Quantum Merlin‚ÄìArthur. Classically, if a problem is proven NP-complete, then this is considered as good evidence that there is no efficient algorithm to solve it. Similarly, QMA-complete problems are considered unlikely to have efficient quantum (or classical) algorithms. One can even go further than this, and attempt to characterise for which families of physical systems calculating ground-state energies is hard, and for which the problem is easy. 29 , 81 Although this programme is not yet complete, it has already provided some formal justification for empirical observations in condensed-matter physics about relative hardness of these problems. Second, using the model of quantum information as a mathematical tool can provide insight into other problems of a purely classical nature. For example, a strong lower bound on the classical communication complexity of the inner product function can be obtained based on quantum information-theoretic principles. 82 Ideas from quantum computing have also been used to prove new limitations on classical data structures, codes and formulae. 83 We have described a rather large number of quantum algorithms, solving a rather large number of problems. However, one might still ask why more algorithms are not known‚Äîand in particular, more exponential speedups? One reason is that strong lower bounds have been proven on the power of quantum computation in the query complexity model, where one considers only the number of queries to the input as the measure of complexity. For example, the complexity achieved by Grover‚Äôs algorithm cannot be improved by even one query while maintaining the same success probability. 84 More generally, in order to achieve an exponential speedup over classical computation in the query complexity model there has to be a promise on the input, i.e., some possible inputs must be disallowed. 85 This is one reason behind the success of quantum algorithms in cryptography: the existence of hidden problem structure that quantum computers can exploit in ways that classical computers cannot. Finding such hidden structure in other problems of practical interest remains an important open problem. In addition, a cynical reader might point out that known quantum algorithms are mostly based on a rather small number of quantum primitives (such as the quantum Fourier transform and quantum walks). An observation attributed to van Dam (see http://dabacon.org/pontiff/?p=1291 ) provides some justification for this. It is known that any quantum circuit can be approximated using only Toffoli and Hadamard quantum gates. 86 The first of these is a purely classical gate, and the second is equivalent to the Fourier transform over the group ‚Ñ§ 2 . Thus any quantum algorithm whatsoever can be expressed as the use of quantum Fourier transforms interspersed with classical processing! However, the intuition behind the quantum algorithms described above is much more varied than this observation would suggest. The inspiration for other quantum algorithms, not discussed here, includes topological quantum field theory; 87 connections between quantum circuits and spin models; 88 the Elitzur‚ÄìVaidman quantum bomb tester; 89 and directly solving the semidefinite programming problem characterising quantum query complexity. 90 , 91 As well as the development of new quantum algorithms, an important direction for future research seems to be the application of known quantum algorithms (and algorithmic primitives) to new problem areas. This is likely to require significant input from, and communication with, practitioners in other fields. Jordan, S. The quantum algorithm zoo. Available at http://math.nist.gov/quantum/zoo/ . Jordan, S. The quantum algorithm zoo. Available at http://math.nist.gov/quantum/zoo/ . Childs, A. & van Dam, W. Quantum algorithms for algebraic problems. Rev. Mod. Phys. 82 , 1‚Äì52 (2010). Article ADS MathSciNet Google Scholar Childs, A. & van Dam, W. Quantum algorithms for algebraic problems. Rev. Mod. Phys. 82 , 1‚Äì52 (2010). Article ADS MathSciNet Google Scholar Mosca, M. in Computational Complexity 2303‚Äì2333 (Springer, 2012). Book Google Scholar Mosca, M. in Computational Complexity 2303‚Äì2333 (Springer, 2012). Book Google Scholar Santha, M. Quantum walk based search algorithms. in Theory Appl. Model. Comput. 4978 , 31‚Äì46 (2008). MathSciNet MATH Google Scholar Santha, M. Quantum walk based search algorithms. in Theory Appl. Model. Comput. 4978 , 31‚Äì46 (2008). MathSciNet MATH Google Scholar Bacon, D. & van Dam, W. Recent progress in quantum algorithms. Commun. ACM 53 , 84‚Äì93 (2010). Article Google Scholar Bacon, D. & van Dam, W. Recent progress in quantum algorithms. Commun. ACM 53 , 84‚Äì93 (2010). Article Google Scholar Ladd, T. et al. Quantum computing. Nature 464 , 45‚Äì53 (2010). Article ADS Google Scholar Ladd, T. et al. Quantum computing. Nature 464 , 45‚Äì53 (2010). Article ADS Google Scholar Fowler, A., Mariantoni, M., Martinis, J. & Cleland, A. Surface codes: Towards practical large-scale quantum computation. Phys. Rev. A 86 , 032324 (2012). Article ADS Google Scholar Fowler, A., Mariantoni, M., Martinis, J. & Cleland, A. Surface codes: Towards practical large-scale quantum computation. Phys. Rev. A 86 , 032324 (2012). Article ADS Google Scholar Buhrman, H., Cleve, R., Massar, S. & de Wolf, R. Non-locality and communication complexity. Rev. Mod. Phys. 82 , 665‚Äì698 (2010). Article ADS Google Scholar Buhrman, H., Cleve, R., Massar, S. & de Wolf, R. Non-locality and communication complexity. Rev. Mod. Phys. 82 , 665‚Äì698 (2010). Article ADS Google Scholar Wilde, M . Quantum Information Theory (Cambridge Univ. Press, 2013). Book Google Scholar Wilde, M . Quantum Information Theory (Cambridge Univ. Press, 2013). Book Google Scholar Papadimitriou, C . Computational Complexity (Addison-Wesley, 1994). MATH Google Scholar Papadimitriou, C . Computational Complexity (Addison-Wesley, 1994). MATH Google Scholar Shor, P. W. Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM J. Comput. 26 , 1484‚Äì1509 (1997). Article MathSciNet Google Scholar Shor, P. W. Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM J. Comput. 26 , 1484‚Äì1509 (1997). Article MathSciNet Google Scholar Buhler, J. Jr., H. W. L. & Pomerance, C. Factoring integers with the number field sieve in The Development Of The Number Field Sieve Vol. 1554 , 50‚Äì94 (Springer, 1993). Chapter Google Scholar Buhler, J. Jr., H. W. L. & Pomerance, C. Factoring integers with the number field sieve in The Development Of The Number Field Sieve Vol. 1554 , 50‚Äì94 (Springer, 1993). Chapter Google Scholar Rivest, R., Shamir, A. & Adleman, L. A method for obtaining digital signatures and public-key cryptosystems. Commun. ACM 21 , 120‚Äì126 (1978). Article MathSciNet Google Scholar Rivest, R., Shamir, A. & Adleman, L. A method for obtaining digital signatures and public-key cryptosystems. Commun. ACM 21 , 120‚Äì126 (1978). Article MathSciNet Google Scholar Kleinjung, T. et al. in Advances in Cryptology‚ÄîCRYPTO 2010 Vol. 1554 , 333‚Äì350 (2010). Book Google Scholar Kleinjung, T. et al. in Advances in Cryptology‚ÄîCRYPTO 2010 Vol. 1554 , 333‚Äì350 (2010). Book Google Scholar Boneh, D. & Lipton, R. in Advances in Cryptology‚ÄîCRYPT0‚Äô 95 , 424‚Äì437 (1995). Book Google Scholar Boneh, D. & Lipton, R. in Advances in Cryptology‚ÄîCRYPT0‚Äô 95 , 424‚Äì437 (1995). Book Google Scholar Brassard, G. & H√∏yer, P. in Proceedings of the Fifth Israeli Symposium on Theory of Computing and Systems 12‚Äì23 (Ramat Gan, Israel, 1997). Brassard, G. & H√∏yer, P. in Proceedings of the Fifth Israeli Symposium on Theory of Computing and Systems 12‚Äì23 (Ramat Gan, Israel, 1997). Regev, O. Quantum computation and lattice problems. SIAM J. Comput. 33 , 738‚Äì760 (2004). Article MathSciNet Google Scholar Regev, O. Quantum computation and lattice problems. SIAM J. Comput. 33 , 738‚Äì760 (2004). Article MathSciNet Google Scholar Grover, L. Quantum mechanics helps in searching for a needle in a haystack. Phys. Rev. Lett. 79 , 325‚Äì328 (1997). Article ADS Google Scholar Grover, L. Quantum mechanics helps in searching for a needle in a haystack. Phys. Rev. Lett. 79 , 325‚Äì328 (1997). Article ADS Google Scholar Boyer, M., Brassard, G., H√∏yer, P. & Tapp, A. Tight bounds on quantum searching. Fortschr. Phys. 46 , 493‚Äì505 (1998). Article Google Scholar Boyer, M., Brassard, G., H√∏yer, P. & Tapp, A. Tight bounds on quantum searching. Fortschr. Phys. 46 , 493‚Äì505 (1998). Article Google Scholar Prasad, M., Biere, A. & Gupta, A. A survey of recent advances in SAT-based formal verification. Int. J. Softw. Tool. Technol. Transf. 7 , 156‚Äì173 (2005). Article Google Scholar Prasad, M., Biere, A. & Gupta, A. A survey of recent advances in SAT-based formal verification. Int. J. Softw. Tool. Technol. Transf. 7 , 156‚Äì173 (2005). Article Google Scholar Williams, R. Improving exhaustive search implies superpolynomial lower bounds. SIAM J. Comput. 42 , 1218‚Äì1244 (2013). Article MathSciNet Google Scholar Williams, R. Improving exhaustive search implies superpolynomial lower bounds. SIAM J. Comput. 42 , 1218‚Äì1244 (2013). Article MathSciNet Google Scholar Brassard, G., H√∏yer, P., Mosca, M. & Tapp, A. Quantum amplitude amplification and estimation. in Quantum Computation and Information Vol. 305 of AMS Contemporary Mathematics Series (eds Lomonaco, S. J. & Brandt, H. E.), 53‚Äì74 (2002). Brassard, G., H√∏yer, P., Mosca, M. & Tapp, A. Quantum amplitude amplification and estimation. in Quantum Computation and Information Vol. 305 of AMS Contemporary Mathematics Series (eds Lomonaco, S. J. & Brandt, H. E.), 53‚Äì74 (2002). Sch√∂ning, U. in Proceedings of 40th Annual Symposium on Foundations of Computer Science , 410‚Äì414 (Washington DC, USA, 1999). Sch√∂ning, U. in Proceedings of 40th Annual Symposium on Foundations of Computer Science , 410‚Äì414 (Washington DC, USA, 1999). Farhi, E., Goldstone, J. & Gutmann, S. A quantum approximate optimization algorithm applied to a bounded occurrence constraint problem. Preprint at arXiv:1412.6062 (2014). Farhi, E., Goldstone, J. & Gutmann, S. A quantum approximate optimization algorithm applied to a bounded occurrence constraint problem. Preprint at arXiv:1412.6062 (2014). Barak, B. et al. Beating the random assignment on constraint satisfaction problems of bounded degree. Preprint at arXiv:1505.03424 (2015). Barak, B. et al. Beating the random assignment on constraint satisfaction problems of bounded degree. Preprint at arXiv:1505.03424 (2015). D√ºrr, C. & H√∏yer, P. A quantum algorithm for finding the minimum. Preprint at quant-ph/9607014 (1996). D√ºrr, C. & H√∏yer, P. A quantum algorithm for finding the minimum. Preprint at quant-ph/9607014 (1996). D√ºrr, C., Heiligman, M., H√∏yer, P. & Mhalla, M. in Proceedings of 31st International Conference on Automata, Languages and Programming (ICALP‚Äô04) , 481‚Äì493 (Turku, Finland, 2004). D√ºrr, C., Heiligman, M., H√∏yer, P. & Mhalla, M. in Proceedings of 31st International Conference on Automata, Languages and Programming (ICALP‚Äô04) , 481‚Äì493 (Turku, Finland, 2004). Ramesh, H. & Vinay, V. String matching in √ï(‚àön+‚àöm) quantum time. J. Discrete Algorithms 1 , 103‚Äì110 (2003). Article MathSciNet Google Scholar Ramesh, H. & Vinay, V. String matching in √ï(‚àön+‚àöm) quantum time. J. Discrete Algorithms 1 , 103‚Äì110 (2003). Article MathSciNet Google Scholar Montanaro, A. Quantum pattern matching fast on average. Algorithmica 1‚Äì24 (2015). Montanaro, A. Quantum pattern matching fast on average. Algorithmica 1‚Äì24 (2015). Farhi, E., Goldstone, J., Gutmann, S. & Sipser, M. Quantum computation by adiabatic evolution. Tech. Rep. , MIT-CTP-2936, MIT (2000). Farhi, E., Goldstone, J., Gutmann, S. & Sipser, M. Quantum computation by adiabatic evolution. Tech. Rep. , MIT-CTP-2936, MIT (2000). Farhi, E. et al. A quantum adiabatic evolution algorithm applied to random instances of an NP-complete problem. Science 292 , 472‚Äì475 (2001). Article ADS MathSciNet Google Scholar Farhi, E. et al. A quantum adiabatic evolution algorithm applied to random instances of an NP-complete problem. Science 292 , 472‚Äì475 (2001). Article ADS MathSciNet Google Scholar van Dam, W., Mosca, M. & Vazirani, U. in Proceedings of 42nd Annual Symposium on Foundations of Computer Science , 279‚Äì287 (IEEE, 2001). van Dam, W., Mosca, M. & Vazirani, U. in Proceedings of 42nd Annual Symposium on Foundations of Computer Science , 279‚Äì287 (IEEE, 2001). Farhi, E., Goldstone, J., Gutmann, S. & Nagaj, D. How to make the quantum adiabatic algorithm fail. Int. J. Quantum Inform. 6 , 503 (2008). Article Google Scholar Farhi, E., Goldstone, J., Gutmann, S. & Nagaj, D. How to make the quantum adiabatic algorithm fail. Int. J. Quantum Inform. 6 , 503 (2008). Article Google Scholar Farhi, E. et al. Quantum adiabatic algorithms, small gaps, and different paths. Quantum Inf. Comput. 11 , 0181‚Äì0214 (2011). MathSciNet Google Scholar Farhi, E. et al. Quantum adiabatic algorithms, small gaps, and different paths. Quantum Inf. Comput. 11 , 0181‚Äì0214 (2011). MathSciNet Google Scholar Choi, V. Different adiabatic quantum optimization algorithms for the NP-complete exact cover and 3SAT problems. Quantum Inf. Comput. 11 , 0638‚Äì0648 (2011). MathSciNet Google Scholar Choi, V. Different adiabatic quantum optimization algorithms for the NP-complete exact cover and 3SAT problems. Quantum Inf. Comput. 11 , 0638‚Äì0648 (2011). MathSciNet Google Scholar Johnson, M. et al. Quantum annealing with manufactured spins. Nature 473 , 194‚Äì198 (2011). Article ADS Google Scholar Johnson, M. et al. Quantum annealing with manufactured spins. Nature 473 , 194‚Äì198 (2011). Article ADS Google Scholar McGeoch, C. & Wang, C. in Proceedings of ACM International Conference on Computing Frontiers (CF‚Äô13) , 1‚Äì23 (Ischia, Italy, 2013). McGeoch, C. & Wang, C. in Proceedings of ACM International Conference on Computing Frontiers (CF‚Äô13) , 1‚Äì23 (Ischia, Italy, 2013). King, J., Yarkoni, S., Nevisi, M., Hilton, J. & McGeoch, C. Benchmarking a quantum annealing processor with the time-to-target metric. Preprint at arXiv:1508.05087 (2015). King, J., Yarkoni, S., Nevisi, M., Hilton, J. & McGeoch, C. Benchmarking a quantum annealing processor with the time-to-target metric. Preprint at arXiv:1508.05087 (2015). R√∏nnow, T. et al. Defining and detecting quantum speedup. Science 345 , 420‚Äì424 (2014). Article ADS Google Scholar R√∏nnow, T. et al. Defining and detecting quantum speedup. Science 345 , 420‚Äì424 (2014). Article ADS Google Scholar Bulata, I. & Nori, F. Quantum simulators. Science 326 , 108‚Äì111 (2009). Article ADS Google Scholar Bulata, I. & Nori, F. Quantum simulators. Science 326 , 108‚Äì111 (2009). Article ADS Google Scholar Brown, K., Munro, W. & Kendon, V. Using quantum computers for quantum simulation. Entropy 12 , 2268‚Äì2307 (2010). Article ADS MathSciNet Google Scholar Brown, K., Munro, W. & Kendon, V. Using quantum computers for quantum simulation. Entropy 12 , 2268‚Äì2307 (2010). Article ADS MathSciNet Google Scholar Georgescu, I., Ashhab, S. & Nori, F. Quantum simulation. Rev. Mod. Phys. 86 , 153 (2014). Article ADS Google Scholar Georgescu, I., Ashhab, S. & Nori, F. Quantum simulation. Rev. Mod. Phys. 86 , 153 (2014). Article ADS Google Scholar Feynman, R. Simulating physics with computers. Int. J. Theor. Phys. 21 , 467‚Äì488 (1982). Article MathSciNet Google Scholar Feynman, R. Simulating physics with computers. Int. J. Theor. Phys. 21 , 467‚Äì488 (1982). Article MathSciNet Google Scholar Lloyd, S. Universal quantum simulators. Science 273 , 1073‚Äì1078 (1996). Article ADS MathSciNet Google Scholar Lloyd, S. Universal quantum simulators. Science 273 , 1073‚Äì1078 (1996). Article ADS MathSciNet Google Scholar Berry, D., Childs, A. & Kothari, R. Hamiltonian simulation with nearly optimal dependence on all parameters. Preprint at arXiv:1501.01715 (2015). Berry, D., Childs, A. & Kothari, R. Hamiltonian simulation with nearly optimal dependence on all parameters. Preprint at arXiv:1501.01715 (2015). Hastings, M., Wecker, D., Bauer, B. & Troyer, M. Improving quantum algorithms for quantum chemistry. Quantum Inf. Comput. 15 , 1‚Äì21 (2015). MathSciNet Google Scholar Hastings, M., Wecker, D., Bauer, B. & Troyer, M. Improving quantum algorithms for quantum chemistry. Quantum Inf. Comput. 15 , 1‚Äì21 (2015). MathSciNet Google Scholar Jordan, S., Lee, K. & Preskill, J. Quantum algorithms for quantum field theories. Science 336 , 1130‚Äì1133 (2012). Article ADS Google Scholar Jordan, S., Lee, K. & Preskill, J. Quantum algorithms for quantum field theories. Science 336 , 1130‚Äì1133 (2012). Article ADS Google Scholar Childs, A., Farhi, E. & Gutmann, S. An example of the difference between quantum and classical random walks. Quantum Inform. Process. 1 , 35‚Äì43 (2002). Article MathSciNet Google Scholar Childs, A., Farhi, E. & Gutmann, S. An example of the difference between quantum and classical random walks. Quantum Inform. Process. 1 , 35‚Äì43 (2002). Article MathSciNet Google Scholar Kempe, J. Quantum random walks hit exponentially faster. Probab. Theory Rel. Fields 133 , 215‚Äì235 (2005). Article MathSciNet Google Scholar Kempe, J. Quantum random walks hit exponentially faster. Probab. Theory Rel. Fields 133 , 215‚Äì235 (2005). Article MathSciNet Google Scholar Aharonov, D., Ambainis, A., Kempe, J. & Vazirani, U. Quantum walks on graphs. in Proceedings of 33rd Annual ACM Symposium on Theory of Computing , 50‚Äì59 (Heraklion, Crete, Greece, 2001). Aharonov, D., Ambainis, A., Kempe, J. & Vazirani, U. Quantum walks on graphs. in Proceedings of 33rd Annual ACM Symposium on Theory of Computing , 50‚Äì59 (Heraklion, Crete, Greece, 2001). Childs, A. et al. Exponential algorithmic speedup by a quantum walk. in Proceedings of 35th Annual ACM Symposium on Theory of Computing , 59‚Äì68 (San Diego, CA, USA, 2003). Childs, A. et al. Exponential algorithmic speedup by a quantum walk. in Proceedings of 35th Annual ACM Symposium on Theory of Computing , 59‚Äì68 (San Diego, CA, USA, 2003). Ambainis, A., Childs, A., Reichardt, B., ≈†palek, R. & Zhang, S. Any AND-OR formula of size n can be evaluated in time n 1/2+ o (1) on a quantum computer. SIAM J. Comput. 39 , 2513‚Äì2530 (2010). Article MathSciNet Google Scholar Ambainis, A., Childs, A., Reichardt, B., ≈†palek, R. & Zhang, S. Any AND-OR formula of size n can be evaluated in time n 1/2+ o (1) on a quantum computer. SIAM J. Comput. 39 , 2513‚Äì2530 (2010). Article MathSciNet Google Scholar Santha, M. On the Monte Carlo boolean decision tree complexity of read-once formulae. Random. Struct. Algorithms 6 , 75‚Äì87 (1995). Article MathSciNet Google Scholar Santha, M. On the Monte Carlo boolean decision tree complexity of read-once formulae. Random. Struct. Algorithms 6 , 75‚Äì87 (1995). Article MathSciNet Google Scholar Ambainis, A., Schulman, L. J., Ta-Shma, A., Vazirani, U. & Wigderson, A. The quantum communication complexity of sampling. SIAM J. Comput. 32 , 1570‚Äì1585 (2003). Article MathSciNet Google Scholar Ambainis, A., Schulman, L. J., Ta-Shma, A., Vazirani, U. & Wigderson, A. The quantum communication complexity of sampling. SIAM J. Comput. 32 , 1570‚Äì1585 (2003). Article MathSciNet Google Scholar Szegedy, M. in Proceedings of 45th Annual Symposium on Foundations of Computer Science , 32‚Äì41 (Rome, Italy, 2004). Szegedy, M. in Proceedings of 45th Annual Symposium on Foundations of Computer Science , 32‚Äì41 (Rome, Italy, 2004). Magniez, F., Nayak, A., Roland, J. & Santha, M. Search via quantum walk. SIAM J. Comput. 40 , 142‚Äì164 (2011). Article MathSciNet Google Scholar Magniez, F., Nayak, A., Roland, J. & Santha, M. Search via quantum walk. SIAM J. Comput. 40 , 142‚Äì164 (2011). Article MathSciNet Google Scholar Le Gall, F. in Proceedings of 55th Annual Symposium on Foundations of Computer Science , 216‚Äì225 (Philadelphia, USA, 2014). Le Gall, F. in Proceedings of 55th Annual Symposium on Foundations of Computer Science , 216‚Äì225 (Philadelphia, USA, 2014). Harrow, A., Hassidim, A. & Lloyd, S. Quantum algorithm for solving linear systems of equations. Phys. Rev. Lett. 15 , 150502 (2009). Article Google Scholar Harrow, A., Hassidim, A. & Lloyd, S. Quantum algorithm for solving linear systems of equations. Phys. Rev. Lett. 15 , 150502 (2009). Article Google Scholar Ambainis, A. in Proceedings of 29th Annual Symposium on Theoretical Aspects of Computer Science , 636‚Äì647 (Paris, France, 2012). Ambainis, A. in Proceedings of 29th Annual Symposium on Theoretical Aspects of Computer Science , 636‚Äì647 (Paris, France, 2012). Clader, B., Jacobs, B. & Sprouse, C. Preconditioned quantum linear system algorithm. Phys. Rev. Lett. 110 , 250504 (2013). Article ADS Google Scholar Clader, B., Jacobs, B. & Sprouse, C. Preconditioned quantum linear system algorithm. Phys. Rev. Lett. 110 , 250504 (2013). Article ADS Google Scholar Leyton, S. & Osborne, T. A quantum algorithm to solve nonlinear differential equations. Preprint at arXiv:0812.4423 (2008). Leyton, S. & Osborne, T. A quantum algorithm to solve nonlinear differential equations. Preprint at arXiv:0812.4423 (2008). Berry, D. High-order quantum algorithm for solving linear differential equations. J. Phys. A Math. Gen. 47 , 105301 (2014). Article ADS MathSciNet Google Scholar Berry, D. High-order quantum algorithm for solving linear differential equations. J. Phys. A Math. Gen. 47 , 105301 (2014). Article ADS MathSciNet Google Scholar Wiebe, N., Braun, D. & Lloyd, S. Quantum algorithm for data fitting. Phys. Rev. Lett. 109 , 050505 (2012). Article ADS Google Scholar Wiebe, N., Braun, D. & Lloyd, S. Quantum algorithm for data fitting. Phys. Rev. Lett. 109 , 050505 (2012). Article ADS Google Scholar Lloyd, S., Mohseni, M. & Rebentrost, P. Quantum algorithms for supervised and unsupervised machine learning. Preprint at arXiv:1307.0411 (2013) . Lloyd, S., Mohseni, M. & Rebentrost, P. Quantum algorithms for supervised and unsupervised machine learning. Preprint at arXiv:1307.0411 (2013) . Aaronson, S. Quantum machine learning algorithms: Read the fine print. Nat. Phys. 11 , 291‚Äì293 (2015). Article Google Scholar Aaronson, S. Quantum machine learning algorithms: Read the fine print. Nat. Phys. 11 , 291‚Äì293 (2015). Article Google Scholar Smolin, J., Smith, G. & Vargo, A. Oversimplifying quantum factoring. Nature 499 , 163‚Äì165 (2013). Article ADS Google Scholar Smolin, J., Smith, G. & Vargo, A. Oversimplifying quantum factoring. Nature 499 , 163‚Äì165 (2013). Article ADS Google Scholar Somaroo, S., Tseng, C., Havel, T., Laflamme, R. & Cory, D. Quantum simulations on a quantum computer. Phys. Rev. Lett. 82 , 5381 (1999). Article ADS Google Scholar Somaroo, S., Tseng, C., Havel, T., Laflamme, R. & Cory, D. Quantum simulations on a quantum computer. Phys. Rev. Lett. 82 , 5381 (1999). Article ADS Google Scholar Lanyon, B. et al. Universal digital quantum simulations with trapped ions. Science 334 , 57‚Äì61 (2011). Article ADS Google Scholar Lanyon, B. et al. Universal digital quantum simulations with trapped ions. Science 334 , 57‚Äì61 (2011). Article ADS Google Scholar Blatt, R. & Roos, C. Quantum simulations with trapped ions. Nat. Phys. 8 , 277‚Äì284 (2012). Article Google Scholar Blatt, R. & Roos, C. Quantum simulations with trapped ions. Nat. Phys. 8 , 277‚Äì284 (2012). Article Google Scholar Aspuru-Guzik, A. & Walther, P. Photonic quantum simulators. Nat. Phys. 8 , 285‚Äì291 (2012). Article Google Scholar Aspuru-Guzik, A. & Walther, P. Photonic quantum simulators. Nat. Phys. 8 , 285‚Äì291 (2012). Article Google Scholar Trotzky, S. et al. Probing the relaxation towards equilibrium in an isolated strongly correlated one-dimensional Bose gas. Nat. Phys. 8 , 325‚Äì330 (2012). Article Google Scholar Trotzky, S. et al. Probing the relaxation towards equilibrium in an isolated strongly correlated one-dimensional Bose gas. Nat. Phys. 8 , 325‚Äì330 (2012). Article Google Scholar Poulin, D. et al. The Trotter step size required for accurate quantum simulation of quantum chemistry. Quantum Inf. Comput. 15 , 361‚Äì384 (2014). Google Scholar Poulin, D. et al. The Trotter step size required for accurate quantum simulation of quantum chemistry. Quantum Inf. Comput. 15 , 361‚Äì384 (2014). Wecker, D., Bauer, B., Clark, B., Hastings, M. & Troyer, M. Gate count estimates for performing quantum chemistry on small quantum computers. Phys. Rev. A 90 , 022305 (2014). Article ADS Google Scholar Wecker, D., Bauer, B., Clark, B., Hastings, M. & Troyer, M. Gate count estimates for performing quantum chemistry on small quantum computers. Phys. Rev. A 90 , 022305 (2014). Article ADS Google Scholar Aaronson, S. & Arkhipov, A. The computational complexity of linear optics. Theory Comput. 9 , 143‚Äì252 (2013). Article MathSciNet Google Scholar Aaronson, S. & Arkhipov, A. The computational complexity of linear optics. Theory Comput. 9 , 143‚Äì252 (2013). Article MathSciNet Google Scholar Ralph, T. Quantum computation: Boson sampling on a chip. Nat. Photon. 7 , 514‚Äì515 (2013). Article ADS Google Scholar Ralph, T. Quantum computation: Boson sampling on a chip. Nat. Photon. 7 , 514‚Äì515 (2013). Article ADS Google Scholar Huh, J., Guerreschi, G., Peropadre, B., McClean, J. & Aspuru-Guzik, A. Boson sampling for molecular vibronic spectra. Nat. Photon. 9 , 615‚Äì620 (2015). Article ADS Google Scholar Huh, J., Guerreschi, G., Peropadre, B., McClean, J. & Aspuru-Guzik, A. Boson sampling for molecular vibronic spectra. Nat. Photon. 9 , 615‚Äì620 (2015). Article ADS Google Scholar Mart√≠n-Lop√©z, E. et al. Simulating molecular vibrations with photons (2015). in preparation. Mart√≠n-Lop√©z, E. et al. Simulating molecular vibrations with photons (2015). in preparation. Higgins, B., Berry, D., Bartlett, S., Wiseman, H. & Pryde, G. Entanglement-free Heisenberg-limited phase estimation. Nature 450 , 396‚Äì396 (2007). ADS Google Scholar Higgins, B., Berry, D., Bartlett, S., Wiseman, H. & Pryde, G. Entanglement-free Heisenberg-limited phase estimation. Nature 450 , 396‚Äì396 (2007). ADS Google Scholar Kitaev, A. Y., Shen, A. H. & Vyalyi, M. N . Classical and Quantum Computation Vol. 47 . (AMS, 2002). Book Google Scholar Kitaev, A. Y., Shen, A. H. & Vyalyi, M. N . Classical and Quantum Computation Vol. 47 . (AMS, 2002). Book Google Scholar Bookatz, A. QMA-complete problems. Quantum Inform. Comput. 14 , 361‚Äì383 (2014). MathSciNet Google Scholar Bookatz, A. QMA-complete problems. Quantum Inform. Comput. 14 , 361‚Äì383 (2014). MathSciNet Google Scholar Schuch, N. & Verstraete, F. Computational complexity of interacting electrons and fundamental limitations of Density Functional Theory. Nat. Phys. 5 , 732‚Äì735 (2009). Article Google Scholar Schuch, N. & Verstraete, F. Computational complexity of interacting electrons and fundamental limitations of Density Functional Theory. Nat. Phys. 5 , 732‚Äì735 (2009). Article Google Scholar Cleve, R., van Dam, W., Nielsen, M. & Tapp, A. in Procedings of the 1st NASA International Conference on Quantum Computing and Quantum Communications , 61‚Äì74 (Palm Springs, CA, USA, 1998). Cleve, R., van Dam, W., Nielsen, M. & Tapp, A. in Procedings of the 1st NASA International Conference on Quantum Computing and Quantum Communications , 61‚Äì74 (Palm Springs, CA, USA, 1998). Drucker, A. & de Wolf, R. Quantum proofs for classical theorems. Theory Comput. Grad. Surv. 2 , 1‚Äì54 (2011). Google Scholar Drucker, A. & de Wolf, R. Quantum proofs for classical theorems. Theory Comput. Grad. Surv. 2 , 1‚Äì54 (2011). Zalka, C. Grover‚Äôs quantum searching algorithm is optimal. Phys. Rev. A 60 , 2746‚Äì2751 (1999). Article ADS Google Scholar Zalka, C. Grover‚Äôs quantum searching algorithm is optimal. Phys. Rev. A 60 , 2746‚Äì2751 (1999). Article ADS Google Scholar Beals, R., Buhrman, H., Cleve, R., Mosca, M. & de Wolf, R. Quantum lower bounds by polynomials. J. ACM 48 , 778‚Äì797 (2001). Article MathSciNet Google Scholar Beals, R., Buhrman, H., Cleve, R., Mosca, M. & de Wolf, R. Quantum lower bounds by polynomials. J. ACM 48 , 778‚Äì797 (2001). Article MathSciNet Google Scholar Shi, Y. Both Toffoli and controlled-NOT need little help to do universal quantum computing. Quantum Inf. Comput. 3 , 84‚Äì92 (2003). MathSciNet MATH Google Scholar Shi, Y. Both Toffoli and controlled-NOT need little help to do universal quantum computing. Quantum Inf. Comput. 3 , 84‚Äì92 (2003). MathSciNet MATH Google Scholar Freedman, M., Larsen, M. & Wang, Z. A modular functor which is universal for quantum computation. Commun. Math. Phys. 227 , 605‚Äì622 (2002). Article ADS MathSciNet Google Scholar Freedman, M., Larsen, M. & Wang, Z. A modular functor which is universal for quantum computation. Commun. Math. Phys. 227 , 605‚Äì622 (2002). Article ADS MathSciNet Google Scholar De las Cuevas, G., D√ºr, W., van den Nest, M. & Martin-Delgado, M. Quantum algorithms for classical lattice models. New J. Phys. 13 , 093021 (2011). Article ADS Google Scholar De las Cuevas, G., D√ºr, W., van den Nest, M. & Martin-Delgado, M. Quantum algorithms for classical lattice models. New J. Phys. 13 , 093021 (2011). Article ADS Google Scholar Lin, C. & Lin, H. in Proceedings of 30th Annual IEEE Conference on Computational Complexity , 537-566 (Portland, OR, USA, 2015). Lin, C. & Lin, H. in Proceedings of 30th Annual IEEE Conference on Computational Complexity , 537-566 (Portland, OR, USA, 2015). Reichardt, B. in Proceedings of 50th Annual Symposium on Foundations of Computer Science , 544‚Äì551 (Atlanta, GA, USA, 2009). Reichardt, B. in Proceedings of 50th Annual Symposium on Foundations of Computer Science , 544‚Äì551 (Atlanta, GA, USA, 2009). Belovs, A. Quantum algorithms for learning symmetric juntas via adversary bound. in Proceedings of 29th Annual IEEE Conference on Computational Complexity , 22‚Äì31 (Vancouver, Canada, 2014). Belovs, A. Quantum algorithms for learning symmetric juntas via adversary bound. in Proceedings of 29th Annual IEEE Conference on Computational Complexity , 22‚Äì31 (Vancouver, Canada, 2014). Proos, J. & Zalka, C. Shor‚Äôs discrete logarithm quantum algorithm for elliptic curves. Quantum Inform. Comput. 3 , 317‚Äì344 (2003). MathSciNet MATH Google Scholar Proos, J. & Zalka, C. Shor‚Äôs discrete logarithm quantum algorithm for elliptic curves. Quantum Inform. Comput. 3 , 317‚Äì344 (2003). MathSciNet MATH Google Scholar Hallgren, S. Polynomial-time quantum algorithms for Pell‚Äôs equation and the principal ideal problem. J. ACM 54 , 4:1‚Äì4:19 (2007). Article MathSciNet Google Scholar Hallgren, S. Polynomial-time quantum algorithms for Pell‚Äôs equation and the principal ideal problem. J. ACM 54 , 4:1‚Äì4:19 (2007). Article MathSciNet Google Scholar Kuperberg, G. A subexponential-time quantum algorithm for the dihedral hidden subgroup problem. SIAM J. Comput. 35 , 170‚Äì188 (2005). Article MathSciNet Google Scholar Kuperberg, G. A subexponential-time quantum algorithm for the dihedral hidden subgroup problem. SIAM J. Comput. 35 , 170‚Äì188 (2005). Article MathSciNet Google Scholar Regev, O. A subexponential time algorithm for the dihedral hidden subgroup problem with polynomial space. Preprint at quant-ph/0406151 (2004). Regev, O. A subexponential time algorithm for the dihedral hidden subgroup problem with polynomial space. Preprint at quant-ph/0406151 (2004). Mart√≠n-L√≥pez, E. et al. Experimental realisation of Shor‚Äôs quantum factoring algorithm using qubit recycling. Nat. Photon. 6 , 773‚Äì776 (2012). Article ADS Google Scholar Mart√≠n-L√≥pez, E. et al. Experimental realisation of Shor‚Äôs quantum factoring algorithm using qubit recycling. Nat. Photon. 6 , 773‚Äì776 (2012). Article ADS Google Scholar Vandersypen, L. et al. Implementation of a three-quantum-bit search algorithm. Appl. Phys. Lett. 76 , 646‚Äì648 (2000). Article ADS Google Scholar Vandersypen, L. et al. Implementation of a three-quantum-bit search algorithm. Appl. Phys. Lett. 76 , 646‚Äì648 (2000). Article ADS Google Scholar Cai, X.-D. et al. Experimental quantum computing to solve systems of linear equations. Phys. Rev. Lett. 110 , 230501 (2013). Article ADS Google Scholar Cai, X.-D. et al. Experimental quantum computing to solve systems of linear equations. Phys. Rev. Lett. 110 , 230501 (2013). Article ADS Google Scholar Barz, S. et al. Solving systems of linear equations on a quantum computer. Sci. Rep. 4 , 115 (2014). Google Scholar Barz, S. et al. Solving systems of linear equations on a quantum computer. Sci. Rep. 4 , 115 (2014). Pan, J. et al. Experimental realization of quantum algorithm for solving linear systems of equations. Phys. Rev. A 89 , 022313 (2014). Article ADS Google Scholar Pan, J. et al. Experimental realization of quantum algorithm for solving linear systems of equations. Phys. Rev. A 89 , 022313 (2014). Article ADS Google Scholar Download references Acknowledgements This work was supported by the UK EPSRC under Early Career Fellowship EP/L021005/1. Author information Authors and Affiliations School of Mathematics, University of Bristol, Bristol, UK Ashley Montanaro School of Mathematics, University of Bristol, Bristol, UK Ashley Montanaro Ashley Montanaro View author publications You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Corresponding author Correspondence to Ashley Montanaro . Ethics declarations Competing interests The author declares no conflict of interest. Rights and permissions This work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article‚Äôs Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder to reproduce the material. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ Reprints and permissions About this article Cite this article Montanaro, A. Quantum algorithms: an overview. npj Quantum Inf 2 , 15023 (2016). https://doi.org/10.1038/npjqi.2015.23 Download citation Received : 18 August 2015 Received : 18 August 2015 Revised : 02 October 2015 Revised : 02 October 2015 Accepted : 30 October 2015 Accepted : 30 October 2015 Published : 12 January 2016 Published : 12 January 2016 DOI : https://doi.org/10.1038/npjqi.2015.23 DOI : https://doi.org/10.1038/npjqi.2015.23 Share this article Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative",
    "extraction_method": "requests+bs4",
    "content_length": 69925,
    "status": "success",
    "error_message": null
  },
  {
    "query": "quantum computing algorithms review",
    "plan_item": "Quantum Review",
    "plan_item_id": "plan_quantum_review",
    "query_id": "q_quantum_1",
    "source": "duckduckgo",
    "url": "https://arxiv.org/abs/2310.03011",
    "title": "Quantum algorithms: A survey of applications and end-to-end complexities",
    "text": "Title: Quantum algorithms: A survey of applications and end-to-end complexities Submission history References & Citations Semantic Scholar BibTeX formatted citation Bibliographic and Citation Tools Code, Data and Media Associated with this Article Recommenders and Search Tools arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .",
    "extraction_method": "requests+bs4",
    "content_length": 796,
    "status": "success",
    "error_message": null
  },
  {
    "query": "quantum computing algorithms review",
    "plan_item": "Quantum Review",
    "plan_item_id": "plan_quantum_review",
    "query_id": "q_quantum_1",
    "source": "duckduckgo",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8988117/",
    "title": "A review on quantum computing and deep learning algorithms and their ...",
    "text": "A review on quantum computing and deep learning algorithms and their applications Communicated by Oscar Castillo. Corresponding author. Accepted 2022 Mar 9. This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic. In this paper, we describe a review concerning the Quantum Computing (QC) and Deep Learning (DL) areas and their applications in Computational Intelligence (CI). Quantum algorithms (QAs), engage the rules of quantum mechanics to solve problems using quantum information, where the quantum information is concerning the state of a quantum system, which can be manipulated using quantum information algorithms and other processing techniques. Nowadays, many QAs have been proposed, whose general conclusion is that using the effects of quantum mechanics results in a significant speedup (exponential, polynomial, super polynomial) over the traditional algorithms. This implies that some complex problems currently intractable with traditional algorithms can be solved with QA. On the other hand, DL algorithms offer what is known as machine learning techniques. DL is concerned with teaching a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in the form of plain text, images, or sound. The inspiration for deep learning is the way that the human brain filters information. Therefore, in this research, we analyzed these two areas to observe the most relevant works and applications developed by the researchers in the world. Keywords: Quantum computing, Deep learning, Neural networks, Fuzzy logic, Robotic, Medicine, Intelligent, Control Nowadays, many intelligent algorithms have been proposed to solve complex problems, some are based on nature Baskaran et al. ( 2015 ), evolution Valdez ( 2020 ), brain behavior Krizhevsky et al. ( 2012 ), physics Rere et al. ( 2015 ), etc; which can be found in the literature, it is calculated there are more than 100 different algorithms, and improved algorithms for finding the best results on the complex problems. However, it is not our aim to analyze all existent methods. Instead, our approach will be on the quantum and deep learning algorithms and their applications. Therefore, we have selected the most relevant applications in this work. Although, we have worked with different algorithms in different ways, for example, with neural networks, fuzzy logic, evolutionary computing, QA and DL have demonstrated be two areas in Computational Intelligence to solve several problems as pattern recognition, optimization problems and can be combined with other methods to improve the performance. Therefore, we focused with the applications about quantum and deep learning algorithms. Quantum computing works with machines that use the properties of quantum physics to store data and perform computations. This can be extremely advantageous for certain tasks where they could vastly outperform even our best supercomputers. In Montiel-Ross ( 2020 ) was made a review of quantum-inspired population-based metaheuristics, in this work, the authors reviewed which quantum-inspired metaheuristics could be translated to be used in the existing quantum computers based on the circuit model programming paradigm. Also, in Montiel-Ross et al. ( 2019 ), was proposed the quantum-inspired Acromyrmex evolutionary algorithm as a highly efficient global optimization method for complex systems. A review of quantum neural networks is shown in Zhao and Wang ( 2021 ), where the authors observed that the quantum neural networks are higher storage capacity and computational efficiency compared to its classical counterparts. Also, in Beer et al. ( 2020 ), was proposed a truly quantum analogue of classical neurons, which form quantum feedforward neural networks capable of universal quantum computation. In Figure 1 , is appreciated the domain which belong deep learning. In the figure, we can observe the relationship with machine learning and the artificial intelligence and DL is shown as a sub-field of artificial intelligence, and DL is a specialization of Machine Learning. Therefore, DL represents the advance stage of machine learning which mainly uses neural networks for learning and prediction of data. It is a group of different algorithms. These are used to design complex systems that can take any type of problems and give predictions. It uses the deep graph with numerous processing layers, made up of many linear and nonlinear conversions, (Schmidhuber ( 2015 ), Shinde and Shah ( 2018 )). The main contribution in this paper, is to visualize the evolution that QC and DL algorithms have had in the last years. For this reason, we made several queries in Scopus and Web of Science to validate in the best way the obtained results. After, we collected the data in plain text to generate the clusters, networks and relations of works around the world with specific queries of the analyzed applications. To achieve obtain the graphics and presented results corresponding to queries, the VosViewer software was used in this research. VOSviewer is a software tool for constructing and visualizing bibliometric networks. These networks may for instance include journals, researchers, or individual publications, and they can be constructed based on citation, bibliographic coupling, co-citation, or co-authorship relations (Perianes-Rodriguez et al. ( 2016 ), Van Eck and Waltman ( 2014 )). The parameters used in VosViewer to build the networks, clusters and relations presented in the paper are shown in Table 1 . Parameters of VosViewer Software This paper is organized as follows: in Section 1 a brief introduction about of main contribution is presented, Section 2 describes with detail the literature review analyzed in the paper, Section 3 shows the Applications with Quantum Computing Algorithms, in Section 4 the Applications with Deep Learning are presented, and the following Section presents the conclusions and future works and finally the conclusions in the last section are included. Literature review In this section, we made an exhaustive review about the topics described above. In this case, the consulted papers were collected from the Scopus database. Also, we made a search in Web of Science (WoS) to analyze the published journals papers with these two areas. In WoS, we made queries with the topic ‚Äôquantum computing‚Äô. With this query, were obtained as result from WoS 33,230 journal papers. However, is a big number of works with difficulty for analyzing. But, with this query is possible to understand the areas, quantity of paper by areas, authors and countries developing works with quantum computing as is shown in Figure 2 , where, it can be appreciated the number of papers by areas. The Figure only shows the first 10 areas. Also, we made a query from WoS search is with the topic ‚ÄôDeep Learning‚Äô, and were found 86,829 papers, that represent a major number of works with respect to QC. But, this is because the search was very general. Therefore, in the other section, we made specific queries to filter and refine in the best way this data. Figure 3 , shows the areas with the higher number of published journal papers, where we can highlight the number of works in the area Engineering Electrical Electronic with more than 20,000 papers. Also, this figure only shows the first ten areas. In this part, we presented related works about these two topics (QC and DL) algorithms. The source of collected data were reviewed in Scopus database. The search was made with the topic ‚ÄôQuantum Computing Algorithms‚Äô, the obtained results were 5785 papers in total. However, we are presenting a brief description only of the most relevant and recent works in this area. But, with the topic search above described is possible to find at any time the updated documents. Also, we presented a review about applications using Quantum Computing Algorithms. In this case, we decide to include applications based in Computational Intelligence, such has neural networks, fuzzy logic, intelligent control, robotic, medicine, etc. Before doing the separate queries, with the topics of QC and DL. A query was made in the Scopus database with the two topics together; this was, to know the authors who in the last 10 years have been working in this field of computational intelligence. Figure 4 shows the result of this query. From Scopus, with the topic QC and DL we made a query to know the classification of works according to type of document. In Figure 5 it can be appreciated, the types of work carried out in the last 10 years. In Figure 6 it can be noted in that the numbers of papers developed to date is increasing every day. In a recent work, Pathak et al. ( 2022 ), presented an Algorithm of Quantum Computing During Pandemic Situation of COVID-19. Further, this work presents fundamental about quantum properties such as superposition, entanglement, and quantum programming tools such as Qiskit (IBM), pyQuil (Google), etc. In Potempa and Porebski ( 2022 ) the authors Comparing Concepts of Quantum and Classical Neural Network Models for Image Classification Task. The comparative results of two models: classical and quantum neural networks of a similar number of training parameters, indicate that the quantum network, although its simulation is time-consuming, overcomes the classical network it has better convergence and achieves higher training and testing accuracy. On the other hand, an interesting work was proposed for Enhancing Security Using Quantum Computing. Were, the authors, developed quantum algorithms using qubit that run faster than classical algorithms and these algorithms reduce the time complexity and also it is impossible for the attackers to attack Peelam and Johari ( 2022 ), with this work is highlighting the importance for using quantum algorithms in security problems in the last years. Other use of QC can be seen in Ong and Tan ( 2022 ), where the authors proposed a work using a quantum circuit program generation with a genetic algorithm for the Open Quantum Assembly Language. In other research, was proposed a Supervised Machine Learning Strategies for Investigation of Weird Pattern Formulation from Large Volume Data Using Quantum Computing, in this work, Quantum machine learning accelerated the supervised, unsupervised, and reinforcement learning methods obtained better results than the classical machi Nivelkar and Bhirud ( 2022 ). In Xiao et al. ( 2021 ) was proposed a stochastic quantum program synthesis framework based on Bayesian optimization, where Quantum computers and algorithms offered an exponential performance improvement over some NP-complete programs which cannot be run efficiently through a Von Neumann computing approach. On the other hand, a classical simulation of the Quantum Approximate Optimization Algorithm was presented, in this work Medvidoviƒá and Carleo ( 2021 ), was developed A neural-network of the many qubit wave function, focusing on states relevant for the Quantum Approximate Optimization Algorithm. A practical application can be seen in Dalyac et al. ( 2021 ) where CA was used for hard industrial optimization problems. The case study in the field of smart-charging of electric vehicles. Also, an important application in the cryptography area was presented in Gaj ( 2018 ). Other important and recent works can be seen in Singh et al. ( 2021a ), Wang et al. ( 2021 ), Yunakovsky et al. ( 2021 ), Gao et al. ( 2021 ), Huber et al. ( 2021 ), Kulkarni et al. ( 2021 ), Ajagekar and You ( 2021 ), Alberts et al. ( 2021 ), Medvidoviƒá and Carleo ( 2021 ), Singh et al. ( 2021b ), Im et al. ( 2021 ) Liu et al. ( 2021 ). Applications with quantum computing algorithms In this section, we presented the most relevant applications in Medicine, Intelligent Control and Robotic. In each area, we made a brief description about the analyzed works. Also, in this part, we are presenting the most cited paper in each field. Table 2 shows the 10 most cited works in the medicine area using QC. 10 most cited works in medicine with QC In Table 3 , the 10 most cited works in the area of intelligent control with QC is presented. In this table is shown the difference with respect to the medicine area, where the number of citations is bigger than intelligent control. 10 most cited works in intelligent control with QC Finally, in Table 4 is shown the most cited works in robotic using QC. Here, it is shown that the number of citations is greater than the intelligent control topic, but less than that of medicine. 10 most cited works in robotic with QC Medicine quantum computing In this section, a review with QA algorithms applied to medicine are shown. We used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to appreciate the formed networks and relations in medicine. From Scopus database, was made a query to calculate the network, relations, clusters with the topic‚ÄôMedicine Quantum Computing‚Äô. We found 114 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 1675 keywords, which 292 meet the threshold. For each of the 292 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total 292 keywords were selected to obtain the final results. In Fig. 8 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. Figure 7 shows the citations in the last years. In this figure, we can appreciate how the number of citations has increased significantly each year (Fig. 8 ). Also, in this section, are presented a brief description about relevant works with the analyzed topic. In Boev et al. ( 2021 ), the authors developed a method for solving genome assembly tasks with the use of quantum and quantum-inspired optimization techniques. Within this method, we present experimental results on genome assembly using quantum annealers both for simulated data and the œÜ X 174 bacteriophages. The results pave a way for a significant increase in the efficiency of solving bioinformatics problems with the use of quantum computing technologies. Also, in Thomasian and Adashi ( 2021 ), a qualitative review of the medical cybersecurity literature was presented with collation of federal and international legal documents, policy reports, industry frameworks, cyberbreach analyses, and scientific journal papers. On the other hand, fuzzy logic with quantum computing was used in Kumar et al. ( 2020 ), in this the authors proposed a health analytics system by forming a knowledge repository of patient‚Äôs symptoms and medicines dosage to prescribe the precise quantum of medicine to cure an ailment and also prevent drug abuse. Also, in this field, Bianconi and Mohseni ( 2020 ), an Infrared detection and imaging are key enabling technologies for a vast number of applications, ranging from communication, to medicine and astronomy, were proposed using quantum computing. In big data applied to medicine have been developed some works with QC, for example, in the healthcare industry, various sources for big data include hospital records, medical records of patients, results of medical examinations, and devices that are a part of internet of things. The authors used QC to manage this quantity of data Dash et al. ( 2019 ). Intelligent control quantum computing applications In the same way that the previous topic, in this section, a review with QC algorithms applied to intelligent control are presented. Also, was used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to distinguish the formed networks and relations in intelligent control. From Scopus database, was made a query to calculate the network, relations, clusters with the topic ‚ÄòIntelligent Control Quantum Computing‚Äô. We found 141 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 1329 keywords, which 301 meet the threshold. For each of the 301 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total 301 keywords were selected to obtain the presented results. In Fig. 9 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. Figure 10 shows the density of this network, highlighting with yellow color the area of quantum computing and intelligent control with more strength that other analyzed keywords. Figure 11 shows the citations in the last years from Scopus. In this figure, we can appreciate how the number of citations has increased significantly each year. Also, in this section, are presented a brief description about relevant works with the analyzed topic. In Guan et al. ( 2020 ) proposed new Lyapunov control scheme for quantum systems using a Particle Swarm Optimization algorithm. Also, in Barchatova et al. ( 2015 ) was proposed an Intelligent robust control system based on quantum KB-selforganization. The authors, considered Quantum soft computing and Kansei/affective engineering technologies in this research. On the other hand, Mohanty and Rout ( 2015 ) presented a motion control method for mobile robots in indoor environments based on color object detection using quantum computing. Also, Han and Yuan ( 2014 ), proposed a Multivariable system identification based on double quantum particle swarm optimization and big data, in this case, the authors used an optimization method combined with quantum computing. Finally, in this section, we presented the application of an adaptive quantum particle swarm optimization algorithm for optimal dispatching of cascaded hydro power stations proposed by Zhang et al. ( 2012 ) Robotic control quantum computing applications Finally, in this section, a review with QC algorithms applied to Robotic are presented. Also, was used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to distinguish the formed networks and relations in Robotic. From Scopus database, was made a query to calculate the network, relations, clusters with the topic‚ÄôRobotic Control Quantum Computing‚Äô. We found 93 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 1013 keywords, which 197 meet the threshold. For each of the 197 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total, 197 keywords were selected to obtain the final results. In Fig. 12 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. Figure 13 shows the overlay visualization to appreciate clearly how are distributed each one the keywords. In Fig. 14 the density of this network is presented, highlighting with yellow color the area of quantum computing and robotic with more strength that other analyzed keywords. Figure 15 shows the citations in the last years from Scopus. In this figure, we can appreciate how the number of citations has increased significantly each year. Also, in this section are presented some important applications with robotic using QC. Atchade-Adelomou et al. ( 2021 ) proposed a quantum computing approach in mobile robot order picking and batching problem solver optimization, the authors developed a quantum algorithm to minimize the distance traveled in warehouses and distribution centers where order picking is applied. Also, in this field A New Quantum-computing based Algorithm for Robotic Arms and Rigid Bodies‚Äô Orientation was proposed by Zioui et al. ( 2021 ). Also, in Post Quantum Secure Command and Control of Mobile Agents Inserting quantum-resistant encryption schemes in the Secure Robot Operating System Varma et al. ( 2020 ). On the other hand, in Korenkov et al. ( 2020 ) quantum software engineering supremacy in Intelligent robotics was developed. Also, in Ulyanov ( 2020 ) Quantum fuzzy inference based on quantum genetic algorithm: Quantum simulator in intelligent robotics. Applications with deep learning algorithms In this section, we presented a review about applications using Deep Learning Algorithms. In this case, we decide to include applications based in Computational Intelligence, such has neural networks, fuzzy logic, etc. Also, in this section, we are presenting the most cited paper in each field. Table 5 shows the 10 most cited works in the medicine area using DL. Here, is observed how in the last years, the number of citations is increasing in this field. 10 most cited works in medicine with DL In Table 6 , are presented the most cited works in intelligent control with DL. Also, is shown as in the last years this topic is highly used by the researchers. 10 most cited works in intelligent control with DL Finally, in Table 7 , are presented the most cited works in robotic with DL. Also, in this topic the number of citations is increasing in the last years. 10 most cited works in robotic control with DL Medicine deep learning applications Also, in this section, a review with DL algorithms applied to Medicine are presented. Also, was used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to distinguish the formed networks and relations in medicine. From Scopus database, was made a query to calculate the network, relations, clusters with the topic‚ÄôRobotic Control Quantum Computing‚Äô. We found 1011 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 7782 keywords, which 2355 meet the threshold. For each of the 2355 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total 1000 keywords were selected to obtain the final results. In Fig. 16 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. In Fig. 17 the density of this network is presented, highlighting with yellow color the area of deep learning and human with more strength that other analyzed keywords. Figure 18 shows the citations in the last years from Scopus. In this figure, we can appreciate how the number of citations has increased significantly each year. Also, in this section are presented some applications with deep learning techniques as, neural networks, machine learning, convolutional neural networks, etc., applied to medicine. Nowadays, several web medical applications evolved in the field of medicine, there is need for an intelligent and efficient extraction technique. Therefore, neural networks, machine learning can be used to improve the obtained results with other classical techniques. For example, in Deepika and Radha ( 2022 ) was presented a study of Abstract-Based Classification of Medical Journals Using Machine Learning Techniques. Other relevant application using deep learning is presented in Gaxiola et al. ( 2018 ), in this work, the authors used modular neural networks for iris recognition. The authors used the human iris database improved with image preprocessing methods. Also, a recent work in the medical area with deep learning techniques is presented by Varela-Santos and Melin ( 2021 ). In this case, the authors used neural networks for classifying coronavirus based on its manifestation on chest X-rays using texture features. In Gonz√°lez et al. ( 2015 ), Fuzzy logic with the optimization method gravitational search was used to optimize the architecture of a modular neural networks in echocardiogram recognition. In the same way, the authors presented a work but using type-2 fuzzy logic with pattern recognition with modular neural networks Gonz√°lez et al. ( 2016 ). On the other hand, an overview of deep learning in medical imaging was presented by Lundervold and Lundervold ( 2019 ). In this work, the authors review the deep artificial neural networks, machine learning models to image analysis to natural language processing. Intelligent control deep learning application Also, in this section, a review with DL algorithms applied to Medicine are presented. Also, was used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to distinguish the formed networks and relations in medicine. From Scopus database, was made a query to calculate the network, relations, clusters with the topic‚ÄôRobotic Control Quantum Computing‚Äô. We found 1160 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 8721 keywords, which 2131 meet the threshold. For each of the 2131 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total 1000 keywords were selected to obtain the final results. In Fig. 19 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. In Fig. 20 the density of this network is presented, highlighting with yellow color the area of intelligent control and deep learning with more strength that other analyzed keywords. Figure 21 shows the citations in the last years from Scopus. In this figure, we can appreciate how the number of citations has increased significantly each year. Robotic deep learning applications Also, in this section, a review with DL algorithms applied to Medicine are presented. Also, was used the tool VosViewer Perianes-Rodriguez et al. ( 2016 ) to distinguish the formed networks and relations in medicine. From Scopus database, was made a query to calculate the network, relations, clusters with the topic‚ÄôRobotic Control Quantum Computing‚Äô. We found 516 linked papers considering title, abstract and keyword. The collected data from Scopus, were used in VosViewer to obtain in graph form the obtained results. First, we create a map based on bibliographic data from Scopus. The data were introduced in a csv format. The type of analysis was by Co-occurrence, the counting method was full counting. Finally, the measure unit used was by keywords with a minimum number of occurrences equal to 2. Where, we obtained 4010 keywords, which 990 meet the threshold. For each of the 990 keywords, the total strength of the co-occurrence links with other keywords was calculated. The keywords with the greatest total link strength were selected. In total, 990 keywords were selected to obtain the final results. In Fig. 22 , is shown the network, relations, clusters and links in a form graph of this query using data from Scopus. In Fig. 23 the density of this network is presented, highlighting with yellow color the area of deep learning and learning algorithms with more strength that other analyzed keywords. Figure 24 shows the citations in the last years from Scopus. In this figure, we can appreciate how the number of citations has increased significantly each year. In this field, we can see many works applying robotics with deep learning methods. Therefore, in this section are presented the most relevant and recent in this area. For example, in Wu et al. ( 2019 ) implemented a robot to make an automatic choreography system based on the deep learning technology. On the other hand, King and Hwang ( 1989 ). The authors proposed a ring VLSI systolic architecture for implementing neural networks with applications to robotic processing. Also, Liao et al. ( 2021 ) proposed an unsupervised fault detection and recovery for intelligent robotic rollators using deep neural networks. The experiments under several conditions confirmed that the method, which leverages machine learning-enhanced algorithms, exhibits reliable performance. Also, Chen et al. ( 2020 ) proposed a robotic device capable of introducing needles and catheters into deformable tissues such as blood vessels to draw blood or deliver fluids autonomously. Robotic cannulation is driven by predictions from a series of deep convolutional neural networks that encode information from multimodal image sequences to guide real-time serving. Also, in Alzubaidi et al. ( 2021 ) was made a review about applications in robotic with deep learning, convolutional neural networks architectures, challenges, etc. In this study, the authors attempt to provide a more comprehensive survey of the most important aspects of Deep Learning and including those enhancements recently added to the field. After performing an analysis from Scopus, an undertaking an exhaustive study about the applications and importance of Quantum Computing and Deep Learning; we can conclude that these areas have achieved an important advancement in the last years. We can observe, the increment of number of citations, for example, in Scopus, the tendency is developed every day more works based on these two fields of the computational intelligence. Also, we have found, the networks, clusters, and built relationships around the world by many researches that are working with the analyzed topics. As future works, other topics of WoS, Google Scholar, or other databases can be analyzed with this software to compare the number of citations or relevance considering other sources. Also, specific queries with other bibliometric software such as CiteSpace, Zhang et al. ( 2010 ), that is a free software for visualizing and analyzing trends and patterns in the scientific literature can be used. The relevance of the review is that with this paper, the researchers can observe the trends for using QC and DL in several areas. However, the paper can serve as reference to make other queries, with other topics from different databases. Also, the same searches can be explored to view the updated information about the analyzed topics in this review. Although, only were reviewed medicine, robotic and intelligent control to make the research, the paper can be extended with other important areas. We decided to explore these areas because we have some works in computational intelligence with these topics. Acknowledgements The authors would like to thank CONACYT and Tecnol√≥gico Nacional de Mexico/Tijuana Institute of Technology for the support during this research work. Author contribution Conceptualization, F.V. and P.M.; methodology, F.V.; software, F.V.; validation, F.V. and P.M.; formal analysis, P.M.; investigation, P.M.; resources, P.M.; writing‚Äîoriginal draft preparation, F.V. and P.M.; writing‚Äîreview and editing, P.M.; visualization, F.V.; supervision, F.V. and P.M.; project administration. All authors have read and agreed to the published version of the manuscript. This paper did not receive funding. Data availability Conflict of interest All the authors in the paper have no conflict of interest. Ethical approval This article does not contain any studies with human participants or animals performed by any of the authors. Informed Consent Statement Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Contributor Information Fevrier Valdez, Email: fevrier@tectijuana.mx. Patricia Melin, Email: pmelin@tectijuana.mx. Ajagekar A, You F. Quantum computing based hybrid deep learning for fault diagnosis in electrical power systems. Scopus. 2021 doi: 10.1016/j.apenergy.2021.117628. [ DOI ] [ Google Scholar ] Alberts GJN, Rol MA, Last T, Broer BW, Bultink CC, Rijlaarsdam MSC, Hauwermeiren AEV. Accelerating quantum computer developments. Scopus. 2021 doi: 10.1140/epjqt/s40507-021-00107-w. [ DOI ] [ Google Scholar ] Alzubaidi L, Zhang J, Humaidi AJ, Al-Dujaili A, Duan Y, Al-Shamma O, Santamar√≠a J, Fadhel MA, Al-Amidie M, Farhan L. Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. Scopus. 2021 doi: 10.1186/s40537-021-00444-8. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Ankita S, Shikha A, Jitendra A, Sanjeev S. A review on application of particle swarm optimization in association rule mining. Scopus. 2013;199:405‚Äì414. doi: 10.1007/978-3-642-35314-7_46. [ DOI ] [ Google Scholar ] Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. Deep reinforcement learning: a brief survey. Scopus. 2017;34(6):26‚Äì38. doi: 10.1109/MSP.2017.2743240. [ DOI ] [ Google Scholar ] Atchade-Adelomou P, Alonso-Linaje G, Albo-Canals J, Casado-Fauli D. Qrobot: a quantum computing approach in mobile robot order picking and batching problem solver optimization. Scopus. 2021 doi: 10.3390/a14070194. [ DOI ] [ Google Scholar ] Barchatova IA, Ulyanov SV, Albu VA. Intelligent robust control system based on quantum KB-self-organization: quantum soft computing and Kansei/affective engineering technologies. Scopus. 2015;323:37‚Äì48. doi: 10.1007/978-3-319-11310-4_4. [ DOI ] [ Google Scholar ] Baskaran A, Balaji N, Basha S, Vengattaraman T. A survey of nature inspired algorithms. Int J Appl Eng Res. 2015;10:19313‚Äì19324. [ Google Scholar ] Beer K, Bondarenko D, Farrelly T, Osborne T, Salzmann R, Scheiermann D, Wolf R. Training deep quantum neural networks. Nat Commun. 2020;11:808. doi: 10.1038/s41467-020-14454-2. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Bianconi S, Mohseni H. Recent advances in infrared imagers: toward thermodynamic and quantum limits of photon sensitivity. Scopus. 2020 doi: 10.1088/1361-6633/ab72e5. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Boev AS, Rakitko AS, Usmanov SR. Genome assembly using quantum and quantum-inspired annealing. Sci Rep. 2021;11:13183‚Äì13183. doi: 10.1038/s41598-021-88321-5. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Chen AI, Balter ML, Maguire TJ. Deep learning robotic guidance for autonomous vascular access. Nat Mach Intell. 2020;2:104‚Äì115. doi: 10.1038/s42256-020-0148-7. [ DOI ] [ Google Scholar ] Cheng L, Wang C, Feng L, Yang K, Liu Z. Functional nanomaterials for phototherapies of cancer. Chem Rev. 2014 doi: 10.1021/cr400532z. [ DOI ] [ PubMed ] [ Google Scholar ] Ching T, Himmelstein DS, Beaulieu-Jones BK, Kalinin AA, Do BT, Way GP, Ferrero E, Agapow PM, Zietz M, Hoffman MM, Xie W, Rosen GL, Lengerich BJ, Israeli J, Lanchantin J, Woloszynek S, Carpenter AE, Shrikumar A, Xu J, Cofer EM, Lavender CA, Turaga SC, Alexandari AM, Lu Z, Harris DJ, Decaprio D, Qi Y, Kundaje A, Peng Y, Wiley LK, Segler MHS, Boca SM, Swamidass SJ, Huang A, Gitter A, Greene CS. Opportunities and obstacles for deep learning in biology and medicine. Scopus. 2018 doi: 10.1098/rsif.2017.0387. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Cully A, Clune J, Tarapore D, Mouret JB. Robots that can adapt like animals. Scopus. 2015;521(7553):503‚Äì507. doi: 10.1038/nature14422. [ DOI ] [ PubMed ] [ Google Scholar ] Dalyac C, Henriet L, Jeandel E, Lechner W, Perdrix S, Porcheron M, Veshchezerova M. Qualifying quantum approaches for hard industrial optimization problems. A case study in the field of smart-charging of electric vehicles. Scopus. 2021 doi: 10.1140/epjqt/s40507-021-00100-3. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Das R, Baker D. Macromolecular modeling with Rosetta. Ann Rev Biochem. 2008;77:363‚Äì382. doi: 10.1146/annurev.biochem.77.062906.171838. [ DOI ] [ PubMed ] [ Google Scholar ] Dash S, Shakyawar SK, Sharma M, Kaushik S. Big data in healthcare: management, analysis and future prospects. Scopus. 2019 doi: 10.1186/s40537-019-0217-0. [ DOI ] [ Google Scholar ] Deepika A, Radha N. Performance analysis of abstract-based classification of medical journals using machine learning techniques. Scopus. 2022;75:613‚Äì626. doi: 10.1007/978-981-16-3728-5_47. [ DOI ] [ Google Scholar ] Dong D, Chen C, Zhang C, Chen Z. Quantum robot: structure, algorithms and applications. Scopus. 2006;24(4):513‚Äì521. doi: 10.1017/S0263574705002596. [ DOI ] [ Google Scholar ] Fadlullah ZM, Tang F, Mao B, Kato N, Akashi O, Inoue T, Mizutani K. State-of-the-art deep learning: evolving machine intelligence to-ward tomorrow‚Äôs intelligent network traffic control systems. Scopus. 2017;19(4):2432‚Äì2455. doi: 10.1109/COMST.2017.2707140. [ DOI ] [ Google Scholar ] Gaj K (2018) Challenges and rewards of implementing and benchmarking post-Quantum cryptography in hardware. doi: 10.1145/3194554.3194615 Gao H, Zhang S, Du Y, Wang Y, Diao M. Relay selection scheme based on quantum differential evolution algorithm in relay networks. Scopus. 2017;11(7):3501‚Äì3523. doi: 10.3837/tiis.2017.07.011. [ DOI ] [ Google Scholar ] Gao Q, Jones GO, Motta M, Sugawara M, Watanabe HC, Kobayashi T, Watanabe E, Ohnishi YY, Nakamura H, Yamamoto N. Applications of quantum computing for investigations of electronic transitions in phenylsulfonyl-carbazole TADF emitters. Scopus. 2021 doi: 10.1038/s41524-021-00540-6. [ DOI ] [ Google Scholar ] Gaxiola F, Melin P, Valdez F, Castro JR (2018) Person recognition with modular deep neural network using the Iris biometric measure. doi: 10.1007/978-3-319-71008-2_6 Giusti A, Guzzi J, Ciresan DC, He FL, Rodriguez JP, Fontana F, Faessler M, Forster C, Schmidhuber J, Caro GD, Scaramuzza D, Gambardella LM. A machine learning approach to visual perception of forest trails for mobile robots. Scopus. 2016;1(2):661‚Äì667. doi: 10.1109/LRA.2015.2509024. [ DOI ] [ Google Scholar ] Gonz√°lez B, Valdez F, Melin P, Prado-Arechiga G. Fuzzy logic in the gravitational search algorithm enhanced using fuzzy logic with dynamic alpha parameter value adaptation for the optimization of modular neural networks in echocardiogram recognition. Appl Soft Comput. 2015;37:245‚Äì254. doi: 10.1016/j.asoc.2015.08.034. [ DOI ] [ Google Scholar ] Gonz√°lez B, Melin P, Valdez F, Prado-Arechiga G (2016) Interval type-2 fuzzy logic gravitational search algorithm for the optimization of modular neural networks in echocardiogram recognition. In: Proceedings of the 2016 {IEEE} symposium series on computational intelligence, {SSCI} 2016, Athens, Greece, December 6‚Äì9, 2016, IEEE, pp 1‚Äì7, doi: 10.1109/SSCI.2016.7850134 Gu S, Holly E, Lillicrap T, Levine S (2017) Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. doi: 10.1109/ICRA.2017.7989385 Guan X, Kuang S, Dong D (2020) PSO-assisted Lyapunov control design for quantum systems. doi: 10.1109/SSCI47803.2020.9308347 H√§ffner H, Roos CF, Blatt R. Quantum computing with trapped ions. Phys Rep. 2008;469(4):155‚Äì203. doi: 10.1016/j.physrep.2008.09. [ DOI ] [ Google Scholar ] Hamet P, Tremblay J. Artificial intelligence in medicine. Metabolism. 2017;69S:S36‚ÄìS40. doi: 10.1016/j.metabol.2017.01.011. [ DOI ] [ PubMed ] [ Google Scholar ] Hammernik K, Klatzer T, Kobler E, Recht MP, Sodickson DK, Pock T, Knoll F. Learning a variational network for reconstruction of accelerated MRI data. Scopus. 2018;79(6):3055‚Äì3071. doi: 10.1002/mrm.26977. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Han P, Yuan S. Multivariable system identification based on double quantum particle swarm optimization and big data. Scopus. 2014;34(32):5779‚Äì5787. doi: 10.13334/j.0258-8013.pcsee.2014.32.012. [ DOI ] [ Google Scholar ] Hu C, Wu Q, Li H, Jian S, Li N, Lou Z. Deep learning with a long short-term memory networks approach for rainfall-runoff simulation. Scopus. 2018 doi: 10.3390/w10111543. [ DOI ] [ Google Scholar ] Huang Y, Tang C, Wang S (2007) Quantum-inspired swarm evolution algorithm Huber SP, Bosoni E, Bercx M, Br√∂der J, Degomme A, Dikan V, Eimre K, Flage-Larsen E, Garcia A, Genovese L, Gresch D, Johnston C, Petretto G, Ponc√© S, Rignanese GM, Sewell CJ, Smit B, Tseplyaev V, Uhrin M, Wortmann D, Yakutovich AV, Zadoks A, Zarabadi-Poor P, Zhu B, Marzari N, Pizzi G. Common workflows for computing material properties using different quantum engines. Scopus. 2021 doi: 10.1038/s41524-021-00594-6. [ DOI ] [ Google Scholar ] Im DG, Lee CH, Kim Y, Nha H, Kim MS, Lee SW, Kim YH. Optimal teleportation via noisy quantum channels without additional qubit resources. Scopus. 2021 doi: 10.1038/s41534-021-00426-x. [ DOI ] [ Google Scholar ] Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, Wang Y, Dong Q, Shen H, Wang Y. Artificial intelligence in healthcare: past, present and future. Stroke Vasc Neurol. 2017;2(4):230‚Äì243. doi: 10.1136/svn-2017-000101. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Johnson KW, Soto JT, Glicksberg BS, Shameer K, Miotto R, Ali M, Ashley E, Dudley JT. Artificial intelligence in cardiology. Scopus. 2018;71(23):2668‚Äì2679. doi: 10.1016/j.jacc.2018.03.521. [ DOI ] [ PubMed ] [ Google Scholar ] Kasabov N (2007) Evolving connectionist systems: the knowledge engineering approach. doi: 10.1007/978-1-84628-347-5 Kendall A, Cipolla R (2016) Modelling uncertainty in deep learning for camera relocalization. doi: 10.1109/ICRA.2016.7487679 Kim DH, Kim TJY, Wang X, Kim M, Quan YJ, Oh JW, Min SH, Kim H, Bhandari B, Yang I, Ahn SH. Smart machining process using machine learning: a review and perspective on machining industry. Scopus. 2018;5(4):555‚Äì568. doi: 10.1007/s40684-018-0057-y. [ DOI ] [ Google Scholar ] King SY, Hwang JN. Neural network architectures for robotic applications. IEEE Trans Robot Autom. 1989;5(5):641‚Äì657. doi: 10.1109/70.88082. [ DOI ] [ Google Scholar ] Kok P, Munro WJ, Nemoto K, Ralph TC, Dowling JP, Milburn GJ. Linear optical quantum computing with photonic qubits. Rev Mod Phys. 2007;79(1):135‚Äì174. doi: 10.1103/RevModPhys.79.135. [ DOI ] [ Google Scholar ] Korenkov VV, Reshetnikov AG, Ulyanov SV (2020) Quantum software engineering supremacy in intelligent robotics. doi: 10.1109/MoNeTeC49726.2020.9258000 Koza JR. Human-competitive results produced by genetic programming. Scopus. 2010;11(3‚Äì4):251‚Äì284. doi: 10.1007/s10710-010-9112-3. [ DOI ] [ Google Scholar ] Krittanawong C, Zhang H, Wang Z, Aydar M, Kitai T. Artificial intelligence in precision cardiovascular medicine. Scopus. 2017;69(21):2657‚Äì2664. doi: 10.1016/j.jacc.2017.03.571. [ DOI ] [ PubMed ] [ Google Scholar ] Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ (eds) Advances in neural information processing systems, Curran Associates, Inc., vol 25 Kulkarni V, Kulkarni M, Pant A. Quantum computing methods for supervised learning. Scopus. 2021 doi: 10.1007/s42484-021-00050-0. [ DOI ] [ Google Scholar ] Kumar KP, Dhinakaran K, Vinod D. Fuzzy quantum computing model for health analytics. Scopus. 2020;10(5):2006‚Äì2024. [ Google Scholar ] Lang AH, Vora S, Caesar H, Zhou L, Yang J, Beijbom O (2019) Pointpillars: fast encoders for object detection from point clouds. doi: 10.1109/CVPR.2019.01298 Lee JG, Jun S, Cho YW, Lee H, Kim GB, Seo JB, Kim N. Deep learning in medical imaging: general overview. Scopus. 2017;18(4):570‚Äì584. doi: 10.3348/kjr.2017.18.4.570. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Leibfried D, Knill E, Seidelin S, Britton J, Blakestad R, Chiaverini J, Hume D, Itano W, Jost J, Langer C, Ozeri R, Reichle R, Wineland D. Creation of a six-atom ‚ÄôSchrodinger cat‚Äô state. Nature. 2006;438:639‚Äì642. doi: 10.1038/nature04251. [ DOI ] [ PubMed ] [ Google Scholar ] Li J, Lu Y, Xu Y, Liu C, Tu Y, Ye S, Liu H, Xie Y, Qian H, Zhu X. AIR-Chem: authentic intelligent robotics for chemistry. Scopus. 2018;122(46):9142‚Äì9148. doi: 10.1021/acs.jpca.8b10680. [ DOI ] [ PubMed ] [ Google Scholar ] Liao Y, Yeaser A, Yang B, Tung J, Hashemi E. Unsupervised fault detection and recovery for intelligent robotic rollators. Robot Autonom Syst. 2021 doi: 10.1016/j.robot.2021.103876. [ DOI ] [ Google Scholar ] Limonov M, Rybin M, Poddubny A, Kivshar Y. Fano resonances in photonics. Nat Photon. 2017;11:543‚Äì554. doi: 10.1038/nphoton.2017.142. [ DOI ] [ Google Scholar ] Liu CY, Spicer M, Apuzzo MLJ, Kobayashi S, Hongo K, Black PML, Rutka JT, Benabid AL, Kelly PJ, Schramm J. The genesis of neuro-surgery and the evolution of the neurosurgical operative environment: Part II-concepts for future development, 2003 and beyond. Scopus. 2003;52(1):20‚Äì35. doi: 10.1097/00006123-200301000-00002. [ DOI ] [ PubMed ] [ Google Scholar ] Liu W, Zhang Y, Deng Z, Zhao J, Tong L. A hybrid quantum-classical conditional generative adversarial network algorithm for human-centered paradigm in cloud. Scopus. 2021 doi: 10.1186/s13638-021-01898-3. [ DOI ] [ Google Scholar ] Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing on MRI. Z Med Phys. 2019;29(2):102‚Äì127. doi: 10.1016/j.zemedi.2018.11.002. [ DOI ] [ PubMed ] [ Google Scholar ] Madjarov IS, Cooper A, Shaw AL, Covey JP, Schkolnik V, Yoon TH, Williams JR, Endres M. An atomic-array optical clock with single-atom readout. Scopus. 2019 doi: 10.1103/PhysRevX.9.041052. [ DOI ] [ Google Scholar ] Mahler J, Liang J, Niyaz S, Laskey M, Doan R, Liu X, Ojea JA, Goldberg K (2017) Dex-Net 2.0: deep learning to plan Robust grasps with synthetic point clouds and analytic grasp metrics. Doi: 10.15607/rss.2017.xiii.058 Mao Q, Hu F, Hao Q. Deep learning for intelligent wireless networks: a comprehensive survey. Scopus. 2018;20(4):2595‚Äì2621. doi: 10.1109/COMST.2018.2846401. [ DOI ] [ Google Scholar ] Medvidoviƒá M, Carleo G. Classical variational simulation of the quantum approximate optimization algorithm. Scopus. 2021 doi: 10.1038/s41534-021-00440-z. [ DOI ] [ Google Scholar ] Mobadersany P, Yousefi S, Amgad M, Gutman DA, Barnholtz-Sloan JS, Vega JEV, Brat DJ, Cooper LAD. Predicting cancer outcomes from histology and genomics using convolutional networks. Scopus. 2018;115(13):2970‚Äì2979. doi: 10.1073/pnas.1717139115. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Mohanty MN, Rout S. An intelligent method for moving object detection. Scopus. 2015;309(2):343‚Äì351. doi: 10.1007/978-81-322-2009-1_39. [ DOI ] [ Google Scholar ] Montiel-Ross OH. A review of quantum-inspired metaheuristics: going from classical computers to real quantum computers. IEEE Access. 2020;8:814‚Äì838. doi: 10.1109/ACCESS.2019.2962155. [ DOI ] [ Google Scholar ] Montiel-Ross O, Rubio Y, Olvera C, Rivera A. Quantum-inspired acromyrmex evolutionary algorithm. Sci Rep. 2019 doi: 10.1038/s41598-019-48409-5. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Nivelkar M, Bhirud SG. Supervised machine learning strategies for investigation of weird pattern formulation from large volume data using quantum computing. Scopus. 2022;218:569‚Äì576. doi: 10.1007/978-981-16-2164-2_45. [ DOI ] [ Google Scholar ] Nweke HF, Teh YW, Al-garadi MA, Alo UR. Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: state of the art and research challenges. Scopus. 2018;105:233‚Äì261. doi: 10.1016/j.eswa.2018.03.056. [ DOI ] [ Google Scholar ] O‚ÄôBrien JL. Optical quantum computing. Science. 2007;318(5856):1567‚Äì1570. doi: 10.1126/science.1142892. [ DOI ] [ PubMed ] [ Google Scholar ] Ong TJ, Tan CC. A genetic algorithm for quantum circuit generation in OpenQASM. Scopus. 2022;295:97‚Äì114. doi: 10.1007/978-3-030-82196-8_8. [ DOI ] [ Google Scholar ] Pathak N, Misra NK, Bhoi BK, Kumar S. Concept and algorithm of quantum computing during pandemic situation of COVID-19. Scopus. 2022;235:523‚Äì535. doi: 10.1007/978-981-16-2877-1_48. [ DOI ] [ Google Scholar ] Peelam MS, Johari R. Enhancing security using quantum computing (ESUQC) Scopus. 2022;768:227‚Äì235. doi: 10.1007/978-981-16-2354-7_21. [ DOI ] [ Google Scholar ] Peng X, Zhang Y, Xiao S, Zheng W, Cui JQ, Chen L, Xiao D (2008) An alert correlation method based on improved cluster algorithm. doi: 10.1109/PACIIA. 2008.285 Perianes-Rodriguez A, Waltman L, van Eck NJ. Constructing bibliometric networks: a comparison between full and fractional counting. J Inform. 2016;10(4):1178‚Äì1195. doi: 10.1016/j.joi.2016.10.006. [ DOI ] [ Google Scholar ] Potempa R, Porebski S. Comparing concepts of quantum and classical neural network models for image classification task. Scopus. 2022;255:61‚Äì71. doi: 10.1007/978-3-030-81523-3_6. [ DOI ] [ Google Scholar ] Rere LR, Fanany MI, Arymurthy AM. Simulated annealing algorithm for deep learning. Proc Comput Sci. 2015;72:137‚Äì144. doi: 10.1016/j.procs.2015.12.114. [ DOI ] [ Google Scholar ] Resconi G, der Wal AJV. Morphogenic neural networks encode abstract rules by data. Scopus. 2002;142(1‚Äì4):249‚Äì273. doi: 10.1016/S0020-0255(02)00168-8. [ DOI ] [ Google Scholar ] Sa I, Ge Z, Dayoub F, Upcroft B, Perez T, McCool C. Deepfruits: a fruit detection system using deep neural networks. Scopus. 2016 doi: 10.3390/s16081222. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Saffman M, Walker TG, M‚àÖlmer K (2010) Quantum information with Rydberg atoms. Rev Mod Phys 82(3):2313‚Äì2363. doi: 10.1103/RevModPhys.82.2313 Schmidhuber J. Deep learning in neural networks: an overview. Neural Netw. 2015;61:85‚Äì117. doi: 10.1016/j.neunet.2014.09.003. [ DOI ] [ PubMed ] [ Google Scholar ] Shinde PP, Shah S (2018) A review of machine learning and deep learning applications. In: Proceedings of the 2018 fourth international conference on computing communication control and automation (ICCUBEA), pp 1‚Äì6. Doi: 10.1109/ICCUBEA.2018.8697857 Shrestha A, Mahmood A. Review of deep learning algorithms and architectures. Scopus. 2019;7:53040‚Äì53065. doi: 10.1109/ACCESS.2019.2912200. [ DOI ] [ Google Scholar ] Singh AK, Saxena D, Kumar J, Gupta V. A quantum approach towards the adaptive prediction of cloud workloads. Scopus. 2021;32(12):2893‚Äì2905. doi: 10.1109/TPDS.2021.3079341. [ DOI ] [ Google Scholar ] Singh S, Chawla P, Sarkar A, Chandrashekar CM. Universal quantum computing using single-particle discrete-time quantum walk. Scopus. 2021 doi: 10.1038/s41598-021-91033-5. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Song T, Wang S, Wang X (2008) The design of reversible gate and reversible sequential circuit based on DNA computing. Doi: 10.1109/ISKE.2008.4730909 Sze V, Chen YH, Yang TJ, Emer JS. Efficient processing of deep neural networks: a tutorial and survey. Scopus. 2017;105(12):2295‚Äì2329. doi: 10.1109/JPROC.2017.2761740. [ DOI ] [ Google Scholar ] Tang F, Fadlullah ZM, Mao B, Kato N. An intelligent traffic load prediction-based adaptive channel assignment algorithm in SDN-IoT: a deep learning approach. Scopus. 2018;5(6):5141‚Äì5154. doi: 10.1109/JIOT.2018.2838574. [ DOI ] [ Google Scholar ] Thomasian NM, Adashi EY. Cybersecurity in the internet of medical things. Scopus. 2021 doi: 10.1016/j.hlpt.2021.100549. [ DOI ] [ Google Scholar ] Thomford NE, Senthebane DA, Rowe A, Munro D, Seele P, Maroyi A, Dzobo K. Natural products for drug discovery in the 21st century: innovations for novel drug discovery. Scopus. 2018 doi: 10.3390/ijms19061578. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Topol EJ. High-performance medicine: the convergence of human and artificial intelligence. Scopus. 2019;25(1):44‚Äì56. doi: 10.1038/s41591-018-0300-7. [ DOI ] [ PubMed ] [ Google Scholar ] Tuyls K, Weiss G. Multiagent learning: Basics, challenges, and prospects. Scopus. 2012;33(3):41‚Äì52. doi: 10.1609/aimag.v33i3.2426. [ DOI ] [ Google Scholar ] Ulyanov SV. Quantum fuzzy inference based on quantum genetic algorithm: quantum simulator in intelligent robotics. Scopus. 2020;1095:78‚Äì85. doi: 10.1007/978-3-030-35249-3_9. [ DOI ] [ Google Scholar ] Ulyanov SV (2004) Quantum soft computing in control process design: quantum genetic algorithms and quantum neural network approaches Valdez F. A review of optimization swarm intelligence-inspired algorithms with type-2 fuzzy logic parameter adaptation. Soft Comput. 2020;24(1):215‚Äì226. doi: 10.1007/s00500-019-04290-y. [ DOI ] [ Google Scholar ] Varela-Santos S, Melin P. A new approach for classifying coronavirus COVID-19 based on its manifestation on chest X-rays using texture features and neural networks. Inf Sci. 2021;545:403‚Äì414. doi: 10.1016/j.ins.2020.09.041. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Van Eck NJ, Waltman L (2014) Visualizing bibliometric networks. In: Measuring scholarly impact, vol 1. Springer, New York. DOI 10.1007/978-3-319-10377-8 Varma R, Melville C, Pinello C, Sahai T (2020) Post quantum secure command and control of mobile agents inserting quantum-resistant encryption schemes in the secure robot operating system. Doi: 10.1109/IRC.2020.00012 Wang S, Clark R, Wen H, Trigoni N. DeepVO: towards end-to-end visual odometry with deep re-current. Convolut Neural Netw. 2017 doi: 10.1109/ICRA.2017.7989236. [ DOI ] [ Google Scholar ] Wang Q, Chumak AV, Pirro P. Inverse-design magnonic devices. Scopus. 2021 doi: 10.1038/s41467-021-22897-4. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Wei J, He J, Chen K, Zhou Y, Tang Z. Collaborative filtering and deep learning based recommendation system for cold start items. Scopus. 2017;69:1339‚Äì1351. doi: 10.1016/j.eswa.2016.09.040. [ DOI ] [ Google Scholar ] Wei J, Luna PD, Bengio Y, Aspuru-Guzik A, Sargent E. Use machine learning to find energy materials. Scopus. 2017;552(7683):23‚Äì25. doi: 10.1038/d41586-017-07820-6. [ DOI ] [ PubMed ] [ Google Scholar ] Wei T, Wang Y, Zhu Q (2017c) Deep reinforcement learning for building HVAC control. doi: 10.1145/3061639.3062224 Werbos P, Dolmatova L. Analog quantum computing (AQC) and the need for time-symmetric physics. Quant Inform Process. 2016 doi: 10.1007/s11128-015-1146-2. [ DOI ] [ Google Scholar ] Wu R, Peng W, Zhou C, Chao F, Yang L, Lin CM, Shang C (2019) Towards deep learning based robot automatic choreography system. In: Yu H, Liu J, Liu L, Ju Z, Liu Y, Zhou D (eds) Intelligent robotics and applications 12th international conference, {ICIRA} 2019, Shenyang, China, August 8‚Äì11, 2019, Proceedings, Part {IV}, Springer, vol 11743, pp 629‚Äì640. Doi: 10.1007/978-3-030-27538-9_54 Xiao Y, Nazarian S, Bogdan P. A stochastic quantum program synthesis framework based on Bayesian optimization. Scopus. 2021 doi: 10.1038/s41598-021-91035-3. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Yang HF, Dillon TS, Chen YPP. Optimized structure of the traffic flow forecasting model with a deep learning approach. Scopus. 2017;28(10):2371‚Äì2381. doi: 10.1109/TNNLS.2016.2574840. [ DOI ] [ PubMed ] [ Google Scholar ] Yunakovsky SE, Kot M, Pozhar N, Nabokov D, Kudinov M, Guglya A, Kiktenko EO, Kolycheva E, Borisov A, Fedorov AK. Towards security recommendations for public-key infrastructures for production environments in the post-quantum era. Scopus. 2021 doi: 10.1140/epjqt/s40507-021-00104-z. [ DOI ] [ Google Scholar ] Zhang G, Wu Y, Zhang F, Liu X. Application of adaptive quantum particle swarm optimization algorithm for optimal dispatching of cascaded hydropower stations. Scopus. 2012;7390:463‚Äì470. doi: 10.1007/978-3-642-31576-3_59. [ DOI ] [ Google Scholar ] Zhang J, Chen C, Vogeley MSE (2010) The use of scientific data: a content analysis. In: Proceedings of the 73rd navigating streams in an information ecosystem ASIS{&amp;}T annual meeting, {ASIST} 2010, Pittsburgh, PA, USA, October 22‚Äì27, 2010, Wiley, vol 47, pp 1‚Äì2, DOI: 10.1002/meet.14504701319 Zhao R, Wang S (2021) A review of quantum neural networks: methods, models, dilemma Zhu M, Wang X, Wang Y. Human-like autonomous car-following model with deep reinforcement learning. Scopus. 2018;97:348‚Äì368. doi: 10.1016/j.trc.2018.10.024. [ DOI ] [ Google Scholar ] Zhu K, Jiang M (2010) Quantum artificial fish swarm algorithm. Doi: 10.1109/WCICA.2010.5553761 Zioui N, Mahmoudi Y, Mahmoudi A, Tadjine M, Bentouba S. A new quantum-computing-based algorithm for robotic arms and rigid bodies‚Äô orientation. Scopus. 2021;7(3):1836‚Äì1846. doi: 10.22055/jacm.2021.37611.3048. [ DOI ] [ Google Scholar ] This section collects any data citations, data availability statements, or supplementary materials included in this article. Data Availability Statement",
    "extraction_method": "requests+bs4",
    "content_length": 57599,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —á–µ–ø—É—Ö–∞ –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ xyzzy —Ñ—ã–≤–∞–æ–ª–¥–∂",
    "plan_item": "–¢–µ—Å—Ç –æ—à–∏–±–∫–∏ –ø–æ–∏—Å–∫–∞",
    "plan_item_id": "plan_3",
    "query_id": "q_3_0",
    "source": "duckduckgo",
    "url": "https://poncy.ru/crossword/?mask=-----&desc=%D0%B0%D0%B1%D1%81%D0%BE%D0%BB%D1%8E%D1%82%D0%BD%D0%B0%D1%8F%20%D1%87%D0%B5%D0%BF%D1%83%D1%85%D0%B0%20%D0%B8%20%D0%B0%D0%B1%D1%80%D0%B0%D0%BA%D0%B0%D0%B4%D0%B0%D0%B1%D1%80%D0%B0",
    "title": "–ê–±—Å–æ–ª—é—Ç–Ω–∞—è —á–µ–ø—É—Ö–∞ –∏ –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ ‚Äî 5 –±—É–∫–≤ —Å–∫–∞–Ω–≤–æ—Ä–¥",
    "text": null,
    "extraction_method": null,
    "content_length": 0,
    "status": "extraction_failed",
    "error_message": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç (–∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç)."
  },
  {
    "query": "–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —á–µ–ø—É—Ö–∞ –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ xyzzy —Ñ—ã–≤–∞–æ–ª–¥–∂",
    "plan_item": "–¢–µ—Å—Ç –æ—à–∏–±–∫–∏ –ø–æ–∏—Å–∫–∞",
    "plan_item_id": "plan_3",
    "query_id": "q_3_0",
    "source": "duckduckgo",
    "url": "https://ru.wikipedia.org/wiki/%D0%90%D0%B1%D1%80%D0%B0%D0%BA%D0%B0%D0%B4%D0%B0%D0%B1%D1%80%D0%B0",
    "title": "–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ ‚Äî –í–∏–∫–∏–ø–µ–¥–∏—è",
    "text": "A B R A C A D A B R A A B R A C A D A B R A B R A C A D A B A B R A C A D A A B R A C A D A B R A C A A B R A C A B R A A B R A B A –ê–±—Ä–∞–∫–∞–¥–∞ÃÅ–±—Ä–∞ ( –ª–∞—Ç. abracadabra , –≥—Ä. abrakos ‚Äî –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –±–æ–∂–µ—Å—Ç–≤–∞, –¥—Ä.–µ–≤—Ä. d√§b√§r ‚Äî —Å–ª–æ–≤–æ [ 1 ] ) ‚Äî —è–∫–æ–±—ã –º–∞–≥–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ , –º–∞–≥–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞, –≤–ø–µ—Ä–≤—ã–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ II –≤–µ–∫–∞ –Ω–∞—à–µ–π —ç—Ä—ã –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º —Ç—Ä–∞–∫—Ç–∞—Ç–µ De Medicina Praecepta –°–∞–º–º–æ–Ω–∏–∫–∞ , –≤—Ä–∞—á–∞ –∏–º–ø–µ—Ä–∞—Ç–æ—Ä–∞ –°–µ–ø—Ç–∏–º–∏—è –°–µ–≤–µ—Ä–∞ –¥–ª—è –ª–µ—á–µ–Ω–∏—è —Å–µ–Ω–Ω–æ–π –ª–∏—Ö–æ—Ä–∞–¥–∫–∏ . –°–ª–æ–≤–æ (–∑–∞–∫–ª–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–æ–ª–µ–∑–Ω–µ–π) –ø—Ä–µ–¥–ø–∏—Å—ã–≤–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: –æ–Ω–æ –≤—ã–ø–∏—Å—ã–≤–∞–ª–æ—Å—å —Å—Ç–æ–ª–±–∏–∫–æ–º –Ω–∞ –¥–æ—â–µ—á–∫–µ 11 —Ä–∞–∑, –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Å–ª–µ–¥–Ω—è—è –±—É–∫–≤–∞ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –æ—Ç—Å–µ–∫–∞–ª–∞—Å—å. –ü–æ–ª—É—á–∞–ª—Å—è —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫. –¢–∞–∫–æ–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–∫–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ –±—ã–ª–æ —É–Ω–∏—á—Ç–æ–∂–∞—Ç—å —Å–∏–ª—É –∑–ª–æ–≥–æ –¥—É—Ö–∞ , –∏ –±–æ–ª—å–Ω–æ–π, –Ω–∞–¥–µ–≤–∞—è –∞–º—É–ª–µ—Ç , –¥–æ–ª–∂–µ–Ω –±—ã–ª –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –≤—ã–∑–¥–æ—Ä–∞–≤–ª–∏–≤–∞—Ç—å. –í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ —Å–ª–æ–≤–æ –ø—Ä–∏–æ–±—Ä–µ–ª–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –±–µ—Å—Å–º—ã—Å–ª–∏—Ü—ã, –Ω–∞–±–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ , –Ω–µ –∏–º–µ—é—â–∏—Ö —Å–º—ã—Å–ª–∞ [ 2 ] [ 3 ] [ 4 ] . –í —Å–ª–æ–≤–∞—Ä–µ –í. –î–∞–ª—è —Å–ª–æ–≤–æ [ 5 ] –µ—â—ë —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∞–∫—Ä–∞–ª—å–Ω—ã–π —Å–º—ã—Å–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º –µ–º—É –ø—Ä–∏–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –º–∞–≥–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞: –∑–∞–≥–æ–≤–æ—Ä , –∫–æ—Ç–æ—Ä—ã–π –Ω–æ—Å—è—Ç –≤ –ª–∞–¥–∞–Ω–∫–µ ; –≤ –°–ª–æ–≤–∞—Ä–µ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ 1989 –≥–æ–¥–∞ –∏–∑–¥–∞–Ω–∏—è [ 6 ] —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –æ —Ç–æ–º, —á—Ç–æ —Å–ª–æ–≤–æ –∫–æ–≥–¥–∞-—Ç–æ –∏–º–µ–ª–æ –º–∞–≥–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞, –Ω–æ –µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–æ –∫–∞–∫ ¬´–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤, –∑–Ω–∞–∫–æ–≤¬ª. –í –¢–æ–ª–∫–æ–≤–æ–º —Å–ª–æ–≤–∞—Ä–µ –≤–æ–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤, –ø–æ–¥ —Ä–µ–¥–∞–∫—Ü–∏–µ–π –ë. –ò. –ò–º—à–µ–Ω–µ—Ü–∫–æ–≥–æ —É–∫–∞–∑–∞–Ω–æ —á—Ç–æ –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ ‚Äì –≤–æ–ª—à–µ–±–Ω–æ–µ —Å–ª–æ–≤–æ, –µ–≥–æ –ø–∏—à—É—Ç –Ω–∞ –¥–æ—â–µ—á–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∏–Ω—ã –ó–∞–ø–∞–¥–Ω–æ–π –ï–≤—Ä–æ–ø—ã –Ω–æ—Å—è—Ç –Ω–∞ –≥—Ä—É–¥–∏, –æ—Ç–ø—Ä–∞–≤–ª—è—è—Å—å –≤ –±–æ–π , –∫–∞–∫ –ø—Ä–µ–¥–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–µ–¥—Å—Ç–≤–æ –æ—Ç –±–æ–ª–µ–∑–Ω–µ–π –∏ –ø—É–ª—å [ 7 ] . 1 –í–µ—Ä—Å–∏–∏ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è 2 –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ –í–µ—Ä—Å–∏–∏ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è ◊© ◊ë ◊® ◊ô ◊® ◊ô ◊ë ◊® ◊ô ◊® ◊ô ◊® ◊ô ◊® ◊ô ◊ô ◊® ◊ô ◊® ◊ô ◊ô –í –¢–∞–ª–º—É–¥–µ –ü—Å–∞—Ö–∏–º 112–∞ –Ω–∞–ø–∏—Å–∞–Ω–∞ –ø–æ–¥–æ–±–Ω–∞—è –º–∞–≥–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞, —è–∫–æ–±—ã —É–±–µ—Ä–µ–≥–∞—é—â–∞—è –æ—Ç –¥–µ–º–æ–Ω–∞ —Å–ª–µ–ø–æ—Ç—ã –≤ –≤–∏–¥–µ —É–±—ã–≤–∞—é—â–µ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞: ¬´–í—Ç–æ—Ä–∏–ª–∏ —Ä–∞–≤–≤–∏–Ω—ã –Ω–∞—à–∏, —á—Ç–æ–±—ã –Ω–µ –ø–∏–ª –µ–≤—Ä–µ–π –≤–æ–¥—ã –Ω–∏ –∏–∑ —Ä–µ–∫–∏ –Ω–∏ –∏–∑ –ø—Ä—É–¥–∞ –≤ –Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è. –ê –µ—Å–ª–∏ –≤—ã–ø–∏–ª, —Ç–æ —Å–∞–º –≤–∏–Ω–æ–≤–∞—Ç, –∏–±–æ –æ–ø–∞—Å–Ω–æ. –í —á—ë–º –∂–µ –æ–ø–∞—Å–Ω–æ—Å—Ç—å? –û–ø–∞—Å–Ω–æ—Å—Ç—å ‚Äî –º–æ–∂–µ—Ç –æ—Å–ª–µ–ø–Ω—É—Ç—å. –ê –µ—Å–ª–∏ –∂–∞–∂–¥–µ—Ç, –∫–∞–∫ –±—ã—Ç—å? –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø—É—Ç–Ω–∏–∫, —Ç–æ —Å–∫–∞–∑–∞—Ç—å –µ–º—É: —Ç–∞–∫–æ–π-—Ç–æ! —Ö–æ—á—É –ø–∏—Ç—å! –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ—Ç —Ä—è–¥–æ–º, —Å–∫–∞–∑–∞—Ç—å —Å–µ–±–µ: –º–∞—Ç—å –≥–æ–≤–æ—Ä–∏–ª–∞ –±–µ—Ä–µ—á—å—Å—è —Å–ª–µ–ø–æ—Ç—ã (—à–∞–±—Ä–∏—Ä–∏). –®–∞–±—Ä–∏—Ä–∏-–±—Ä–∏—Ä–∏-—Ä–∏—Ä–∏-–∏—Ä–∏-—Ä–∏. –ñ–µ–ª–∞—é –≤–æ–¥—ã –≤ —Å—Ç–∞–∫–∞–Ω–µ –≥–ª–∏–Ω—è–Ω–æ–º¬ª [ 8 ] . ◊ô◊ë◊®◊õ◊ö ◊ô◊î◊ï◊î ◊ï◊ô◊©◊û◊®◊ö ◊ô◊ê◊® ◊ô◊î◊ï◊î ◊§◊†◊ô◊ï ◊ê◊ú◊ô◊ö ◊ï◊ô◊ó◊†◊ö ◊ô◊©◊ê ◊ô◊î◊ï◊î ◊§◊†◊ô◊ï ◊ê◊ú◊ô◊ö ◊ï◊ô◊©◊ù ◊ú◊ö ◊©◊ú◊ï◊ù –¢–µ–∫—Å—Ç –Ω–∞ –µ–≤—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω–∏–∫–æ–≤ (—Å–º. –ê–∞—Ä–æ–Ω–æ–≤–æ –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–µ ) –ø–æ—Å—Ç—Ä–æ–µ–Ω –≤ –≤–∏–¥–µ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞, –ø–æ —Ñ–æ—Ä–º—É–ª–µ: –æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É –∏ —Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—è —Å–æ–±–æ–π –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–µ –æ—Ç –ë–æ–≥–∞ –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–∫–ª—è—Ç–∏—é, –∑–∞–ø–∏—Å—ã–≤–∞–≤—à–µ–≥–æ—Å—è –≤ –≤–∏–¥–µ –ø–µ—Ä–µ–≤—ë—Ä–Ω—É—Ç–æ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞ (–∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞). –ü–µ—Ä–≤—ã–π —Å—Ç–∏—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä—ë—Ö —Å–ª–æ–≤ –∏ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç–∏ –±—É–∫–≤, –≤—Ç–æ—Ä–æ–π ‚Äî –∏–∑ –ø—è—Ç–∏ —Å–ª–æ–≤ –∏ –¥–≤–∞–¥—Ü–∞—Ç–∏ –±—É–∫–≤, —Ç—Ä–µ—Ç–∏–π ‚Äî –∏–∑ —Å–µ–º–∏ —Å–ª–æ–≤ –∏ –¥–≤–∞–¥—Ü–∞—Ç–∏ –ø—è—Ç–∏ –±—É–∫–≤ (3-5-7, 15-20-25), –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏—è –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏—è –ë–æ–∂—å–µ–≥–æ –∏–º–µ–Ω–∏ (◊ô◊î◊ï◊î) (—Å–º. —Ç–µ—Ç—Ä–∞–≥—Ä–∞–º–º–∞—Ç–æ–Ω ) –ø—Ä–∏ –ø—Ä–æ–∏–∑–Ω–µ—Å–µ–Ω–∏–∏ —Å–≤—è—â–µ–Ω–Ω–∏–∫–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ç–∏—Ö–∞. –í –¢–∞–ª–º—É–¥–µ —É–∫–∞–∑–∞–Ω–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç—å —ç—Ç–æ –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –µ–≤—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –Ω–∞ –µ–≤—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ —ç—Ç–æ –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–µ –∏ –±—ã–ª–æ –ø—Ä–∏–¥—É–º–∞–Ω–æ. 7 –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–π , –∫–æ—Ç–æ—Ä—ã–º–∏ –±–ª–∞–≥–æ—Å–ª–æ–≤–ª—è—é—Ç –Ω–æ–≤–æ–±—Ä–∞—á–Ω—ã—Ö –Ω–∞ –µ–≤—Ä–µ–π—Å–∫–æ–π —Å–≤–∞–¥—å–±–µ, —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –æ—Ç –∫—Ä–∞—Ç–∫–æ–≥–æ –∫ –¥–æ–ª–≥–æ–º—É, –Ω–∞–ø–æ–¥–æ–±–∏–µ –ê–∞—Ä–æ–Ω–æ–≤–∞ –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏—è –≤ –≤–∏–¥–µ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –±–ª–∞–≥–æ—Å–ª–æ–≤–µ–Ω–∏–µ –Ω–æ–≤–æ–±—Ä–∞—á–Ω—ã—Ö –≤ –¥–µ–Ω—å —Å–≤–∞–¥—å–±—ã –∏ –Ω–∞ –≤—Å—é –∂–∏–∑–Ω—å. –°—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤–∞—è –∫–∞–±–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–ª–∏—Ç–≤–∞ –ê–Ω–∞ –±–µ-–∫–æ–∞—Ö , –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –∏–∑ 7 —Å—Ç—Ä–æ–∫, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞–∫—Ä–æ—Å—Ç–∏—Ö –∏–∑ –∏–º–µ–Ω–∏ –ë–æ–≥–∞. ◊ê◊ë◊í ◊ô◊™◊¶ ◊ß◊®◊¢ ◊©◊ò◊† ◊†◊í◊ì ◊ô◊õ◊© ◊ë◊ò◊® ◊¶◊™◊í ◊ó◊ß◊ë ◊ò◊†◊¢ ◊ô◊í◊ú ◊§◊ñ◊ß ◊©◊ß◊ï ◊¶◊ô◊™ –û–¥–Ω–∞ –∏–∑ –≤–µ—Ä—Å–∏–π –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –µ–≥–æ —Å–æ–∑–≤—É—á–∏–∏ —Å —Ñ—Ä–∞–∑–∞–º–∏ –Ω–∞ –∏—É–¥–µ–π—Å–∫–æ–º –∞—Ä–∞–º–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ : ◊¢÷∑◊ë÷∞◊ì÷∏◊ê ◊õ÷∞÷º◊ì÷∑◊ë◊®÷∏◊ê , avda kedavra , ¬´—á—Ç–æ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ, –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–¥–µ–ª–∞–Ω–æ¬ª, –∏ ◊¢◊ë◊®◊ê ◊õ◊ì◊ë◊®◊ê , avra kedavra —Å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º ¬´—á—Ç–æ —Å–∫–∞–∑–∞–Ω–æ, –¥–æ–ª–∂–Ω–æ —Å–≤–µ—Ä—à–∏—Ç—å—Å—è¬ª. –í –∞—Ä–∞–º–µ–π—Å–∫–æ–º b –∏ v –º–æ–≥–ª–∏ –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è—Ç—å—Å—è, –æ—Ç—Å—é–¥–∞ –≤–∞—Ä–∏–∞–Ω—Ç ¬´abra kedabra¬ª –∏ –¥–∞–ª–µ–µ ¬´abracadabra¬ª [ 9 ] . –ü–æ –≤–µ—Ä—Å–∏–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è –í–µ—Ç—Ö–æ–≥–æ –ó–∞–≤–µ—Ç–∞ –î–∂–æ–Ω–∞ –ê–ª–ª–µ–≥—Ä–æ —Å–ª–æ–≤–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ—Ç –º–µ—Å–æ–ø–æ—Ç–∞–º—Å–∫–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è ¬´–ê–ë-–ë–ê-–¢–ê–ë-–ë–ê-–†–ò¬ª, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏–ª–æ—Å—å –≤–æ –≤—Ä–µ–º—è —Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã—Ö –æ–±—Ä—è–¥–æ–≤ —É –¥—Ä–µ–≤–Ω–∏—Ö —à—É–º–µ—Ä–æ–≤ . –¢–∞–∫–∂–µ –î–∂–æ–Ω –ê–ª–ª–µ–≥—Ä–æ —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ —Å–æ—á–µ—Ç–∞–Ω–∏–µ —Å–æ–∑–≤—É—á–Ω–æ –∞—Ä–∞–º–µ–π—Å–∫–æ–º—É –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—é –∏–º–µ–Ω–∏ –ë–æ–≥–∞, –∫–æ—Ç–æ—Ä–æ–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –ù–æ–≤–æ–º –ó–∞–≤–µ—Ç–µ –≤ –º–æ–ª–∏—Ç–≤–µ ¬´ –û—Ç—á–µ –Ω–∞—à ¬ª. –ì. –•–∏–≥–≥–∏–Ω—Å —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–∑ –∫–µ–ª—å—Ç—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –∏–∑ —Å–ª–æ–≤ ¬´–ê–±—Ä–∞¬ª –∏–ª–∏ ¬´–ê–±–∞—Ä¬ª ‚Äî ¬´–±–æ–≥¬ª –∏ ¬´–∫–∞–¥¬ª ‚Äî ¬´—Å–≤—è—Ç–æ–π¬ª. –ü–æ –º–Ω–µ–Ω–∏—é –ï–ª–µ–Ω—ã –ë–ª–∞–≤–∞—Ç—Å–∫–æ–π , —Å–ª–æ–≤–æ ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª —è–≤–ª—è–µ—Ç—Å—è –ø–æ–∑–¥–Ω–∏–º –∏—Å–∫–∞–∂–µ–Ω–∏–µ–º —Å–≤—è—â–µ–Ω–Ω–æ–≥–æ –≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ ¬´ –ê–±—Ä–∞–∫—Å–∞—Å ¬ª, –∫–∞–∫ –≤–∞—Å–∏–ª–∏–¥–∏–∞–Ω–µ –∏–º–µ–Ω–æ–≤–∞–ª–∏ –≤—ã—Å—à–µ–µ –±–æ–∂–µ—Å—Ç–≤–æ. [ 10 ] –•—É–∞–Ω –ö–µ—Ä–ª–æ—Ç —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ—Ç –µ–≤—Ä–µ–π—Å–∫–æ–π —Ñ—Ä–∞–∑—ã ¬´abreg ad habra¬ª ‚Äî ¬´–º–µ—á–∏ —Å–≤–æ—é –º–æ–ª–Ω–∏—é –¥–∞–∂–µ –≤ —Å–º–µ—Ä—Ç—å¬ª [ 11 ] . –£ –°—ç–º–ø—Å–æ–Ω–∞ –ú–∞–∫–∫–∏ –≤ –∫–Ω–∏–≥–µ ¬´–ú–∏—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è –¥—Ä–µ–≤–Ω–∏—Ö¬ª –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –≤–µ—Ä—Å–∏—è, —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ç–æ—Ä–æ–π ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª ‚Äî –æ–¥–Ω–æ –∏–∑ –∏–º—ë–Ω –±–æ–∂–µ—Å—Ç–≤–∞ –≤ –î—Ä–µ–≤–Ω–µ–º –ï–≥–∏–ø—Ç–µ. –†–∞–∑–ª–æ–∂–∏–≤ —Ñ–æ—Ä–º—É–ª—É ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª –Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ, –°. –ú–∞–∫–∫–∏ –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: ¬´Ab‚Äôr ‚Äî achad ‚Äî ab‚Äôra¬ª. ¬´Ab‚Äôr¬ª ‚Äî ¬´–ë—ã–∫¬ª; ¬´achad¬ª ‚Äî ¬´–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π¬ª. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, ¬´Achad¬ª ‚Äî –æ–¥–Ω–æ –∏–∑ –∏–º—ë–Ω –°–æ–ª–Ω—Ü–∞ [ 12 ] . –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Ñ–æ—Ä–º—É–ª–∞ ¬´Abracadabra¬ª —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É–µ—Ç –∑–Ω–∞–∫ –¢–µ–ª—å—Ü–∞ , –∞, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç–∏ –±—É–∫–≤, —Ç–æ –µ—â—ë –∏ –∑–∞–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 11 –∑–Ω–∞–∫–æ–≤ –ó–æ–¥–∏–∞–∫–∞ . –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∑–æ—Ç–µ—Ä–∏–∫–∏ –ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é —Ö–∞–ª–¥–µ–π—Å–∫–æ–≥–æ –∑–∞–∫–ª–∏–Ω–∞–Ω–∏—è ¬´–∞–± –±–∞–¥–∞ –∫–µ –¥–∞–∞–±—Ä–∞¬ª ‚Äî ¬´—Å–≥–∏–Ω—å, –∫–∞–∫ —Å–ª–æ–≤–æ¬ª [ –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω 923 –¥–Ω—è ] . –≠. –õ–µ–≤–∏ –≤ –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–µ —É—Å–º–∞—Ç—Ä–∏–≤–∞–ª –∫–ª—é—á –∫ –ø–µ–Ω—Ç–∞–≥—Ä–∞–º–º–µ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–ª —Å–ª–µ–¥—É—é—â–µ–µ –∏—Å—Ç–æ–ª–∫–æ–≤–∞–Ω–∏–µ –º–∞–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞: –ê –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–æ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞, —Ç–æ –µ—Å—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ, –∏–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ, –∞–≥–µ–Ω—Ç–∞. –ê –≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–∏ —Å –ë ‚Äî –æ–ø–ª–æ–¥–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –±–∏–Ω–µ—Ä–∞ –µ–¥–∏–Ω–∏—Ü–µ–π. –† ‚Äî –µ—Å—Ç—å –∑–Ω–∞–∫ –¢–µ—Ä–Ω–µ—Ä–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–æ–≥–ª–∏—Ñ–∏—á–µ—Å–∫–∏ –∏—Å—Ç–µ—á–µ–Ω–∏–µ, –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–µ –æ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–≤—É—Ö –Ω–∞—á–∞–ª. –ß–∏—Å–ª–æ –±—É–∫–≤ —Å–ª–æ–≤–∞ –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ ¬´11¬ª ‚Äî –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ—Ç –∫ –¥–µ—Å—è—Ç–∫–µ –ü–∏—Ñ–∞–≥–æ—Ä–∞ –µ–¥–∏–Ω–∏—Ü—É –ø–æ—Å–≤—è—â—ë–Ω–Ω–æ–≥–æ. –°—É–º–º–∞ –≤—Å–µ—Ö —Å–ª–æ–∂–µ–Ω–Ω—ã—Ö –±—É–∫–≤ –æ–±—Ä–∞–∑—É–µ—Ç ¬´66¬ª ‚Äî –∫–∞–±–±–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ , ¬´12¬ª ‚Äî –∫–≤–∞–¥—Ä–∞—Ç –¢—ë—Ä–Ω–µ—Ä–∞ –∏ –∫–≤–∞–¥—Ä–∞—Ç—É—Ä–∞ –∫—Ä—É–≥–∞. [ 13 ] . –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ –í –ø–µ—Å–Ω–µ Abracadabra –≥—Ä—É–ø–ø—ã Steve Miller Band . –í –≥–ª–∞–≤–Ω–æ–π –º—É–∑—ã–∫–∞–ª—å–Ω–æ–π —Ç–µ–º–µ —Ç–µ–ª–µ—Å–µ—Ä–∏–∞–ª–∞ ¬´ –ú–∞—Å—Ç–µ—Ä –∏ –ú–∞—Ä–≥–∞—Ä–∏—Ç–∞ ¬ª (–ø–µ—Å–Ω–µ —Ö–æ—Ä–∞) –≤ —á–∏—Å–ª–µ –¥—Ä—É–≥–∏—Ö –ª–∞—Ç–∏–Ω—Å–∫–∏—Ö —Å–ª–æ–≤ –∏ –æ–±–æ—Ä–æ—Ç–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∑–∞–∫–ª—è—Ç–∏—è–º–∏ –∏ –±–∏–±–ª–µ–π—Å–∫–∏–º —Å—é–∂–µ—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–æ Abracadabra . –í–æ –≤—Å–µ–ª–µ–Ω–Ω–æ–π ¬´ –ü–æ–∫–µ–º–æ–Ω–∞ ¬ª –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–∫–µ–º–æ–Ω—ã –ê–±—Ä–∞ –∏ –ö–∞–¥–∞–±—Ä–∞ , –æ–±–ª–∞–¥–∞—é—â–∏–µ —ç–∫—Å—Ç—Ä–∞—Å–µ–Ω—Å–æ—Ä–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏. –í –¥–µ—Ç–µ–∫—Ç–∏–≤–Ω–æ–º —Ä–æ–º–∞–Ω–µ –ï. –ò. –ü–∞—Ä–Ω–æ–≤–∞ ¬´–ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∞—è –≥–µ–º–º–∞¬ª. –í —Ä–æ–º–∞–Ω–µ ¬´ –ö–æ–º—å—é–Ω–∏—Ç–∏ ¬ª –ê–ª–µ–∫—Å–µ—è –ò–≤–∞–Ω–æ–≤–∞ . –í –∫–Ω–∏–≥–µ –ê–º–±—Ä–æ–∑–∞ –ë–∏—Ä—Å–∞ ¬´ –°–ª–æ–≤–∞—Ä—å –°–∞—Ç–∞–Ω—ã ¬ª —Å–ª–æ–≤—É ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª –ø–æ—Å–≤—è—â–µ–Ω–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –≤—ã–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—ç—Ç–∞ –î–∂–∞–º—Ä–∞—Ö–∞ –ì–æ–ª–æ–±–æ–º–∞. –í —Ä–æ–º–∞–Ω–µ –°—Ç–∏–≤–µ–Ω–∞ –ö–∏–Ω–≥–∞ ¬´ –ù—É–∂–Ω—ã–µ –≤–µ—â–∏ ¬ª. –í –ø–µ—Å–Ω–µ Abrakadabra –≥—Ä—É–ø–ø—ã Saltatio Mortis . –í —Ñ–∏–ª—å–º–µ-—Å–∫–∞–∑–∫–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –†–æ—É ¬´ –ú–∞—Ä—å—è-–∏—Å–∫—É—Å–Ω–∏—Ü–∞ ¬ª, –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ ‚Äî –≤–æ–ª—à–µ–±–Ω–æ–µ —Å–ª–æ–≤–æ, –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–µ –≤—Ä–∞—Ç–∞ –≤ –ø–æ–¥–≤–æ–¥–Ω–æ–µ —Ü–∞—Ä—Å—Ç–≤–æ. –í —Ñ–∏–ª—å–º–µ ¬´ –ü—Ä–µ—Å—Ç–∏–∂ ¬ª –µ–≥–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç –≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π, –ø–µ—Ä–µ–¥ —Å–≤–æ–µ–π –∫–∞–∑–Ω—å—é. –û–¥–∏–Ω –∏–∑ –∞–ª—å–±–æ–º–æ–≤ —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫–æ–π –±–ª—ç–∫-–º–µ—Ç–∞–ª –≥—Ä—É–ø–ø—ã Dimmu Borgir –Ω–æ—Å–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´Abrahadabra¬ª. –ù–∞ –æ–±–ª–æ–∂–∫–µ –∞–ª—å–±–æ–º–∞ ¬´ –ì–æ–≤–æ—Ä–∏—Ç –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç ¬ª —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ä–æ–∫-–≥—Ä—É–ø–ø—ã ¬´ –ü–∏–∫–Ω–∏–∫ ¬ª —Å–ª–æ–≤–æ ¬´ABRACADABRA¬ª –Ω–∞–ø–∏—Å–∞–Ω–æ 11 —Ä–∞–∑ –≤ –≤–∏–¥–µ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞. –° —Å–µ–Ω—Ç—è–±—Ä—è 2012 –≥–æ–¥–∞ –Ω–∞ –ü–µ—Ä–≤–æ–º –∫–∞–Ω–∞–ª–µ –≤—ã—Ö–æ–¥–∏—Ç —Ç–µ–ª–µ–ø–µ—Ä–µ–¥–∞—á–∞ ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª, –≤–∫–ª—é—á–∞—é—â–∞—è –≤ —Å–µ–±—è –æ—Ç—Ä—ã–≤–∫–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º, –≤—ã—Ö–æ–¥—è—â–∏—Ö –Ω–∞ –∫–∞–Ω–∞–ª–µ. –í 2015 –≥–æ–¥—É –≥—Ä—É–ø–ø–∞ ¬´ –õ–µ—Å—Ç–Ω–∏—Ü–∞ –≠—à–µ—Ä–∞ ¬ª –≤—ã–ø—É—Å—Ç–∏–ª–∞ –∞–ª—å–±–æ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª. –£ –ø–µ–≤—Ü–∞ –í–∞–ª–µ—Ä–∏—è –õ–µ–æ–Ω—Ç—å–µ–≤–∞ –µ—Å—Ç—å –ø–µ—Å–Ω—è ¬´–ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª. –í —Å–æ–≤–µ—Ç—Å–∫–æ–º –º—É–ª—å—Ç—Ñ–∏–ª—å–º–µ ¬´ –ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –ú—é–Ω—Ö–∞—É–∑–µ–Ω–∞ ¬ª, –±–∞—Ä–æ–Ω –∏ –¥–∂–∏–Ω–Ω –†–∞—Ö–∞—Ç –∏–±–Ω –õ—É–∫—É–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ ¬´–ê–±—Ä–∞-—à–≤–∞–±—Ä–∞-–∫–∞–¥–∞–±—Ä–∞¬ª –∫–∞–∫ –≤–æ–ª—à–µ–±–Ω–æ–µ —Å–ª–æ–≤–æ . –í –ø–µ—Å–Ω–µ Abracadabra –≥—Ä—É–ø–ø—ã Brown Eyed Girls . –°–ª–æ–≤–∞ –°–º–µ—Ä—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–∫–ª—è—Ç–∏—è (Killing Curse) ‚Äî –æ–¥–Ω–æ–≥–æ –∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –∑–∞–∫–ª–∏–Ω–∞–Ω–∏–π –≤–æ –≤—Å–µ–ª–µ–Ω–Ω–æ–π ¬´ –ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä–∞ ¬ª ‚Äï –ê–≤–∞–¥–∞ –ö–µ–¥–∞–≤—Ä–∞ ( Avada Kedavra ). –ó–∞–∫–ª–∏–Ω–∞–Ω–∏–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ –¥–ª—è –ø—Ä–∏—á–∏–Ω–µ–Ω–∏—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–π –±–µ–∑–±–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ–π —Å–º–µ—Ä—Ç–∏ –±–µ–∑ —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π. –ü–æ –æ–¥–Ω–æ–π –∏–∑ –≤–µ—Ä—Å–∏–π —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º ¬´–∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞¬ª –∏ –ª–∞—Ç–∏–Ω—Å–∫–æ–≥–æ cadaver ‚Äï ¬´—Ç—Ä—É–ø¬ª. –¢–∞–∫–∂–µ –µ—Å—Ç—å –≤–µ—Ä—Å–∏—è, —á—Ç–æ —ç—Ç–æ –∑–∞–∫–ª–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏–∑ –∞—Ä–∞–º–µ–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ . –í –∫–Ω–∏–≥–µ ¬´–î–Ω–µ–≤–Ω–∏–∫ —á—É–º–Ω–æ–≥–æ –≥–æ–¥–∞¬ª (–∞–≤—Ç–æ—Ä ‚Äî –î–∞–Ω–∏–µ–ª—å –î–µ—Ñ–æ ) –∫–∞–∫ –æ–±–µ—Ä–µ–≥ –æ—Ç —á—É–º—ã. –í –ø–µ—Å–Ω–µ Abracadabra –≥—Ä—É–ø–ø—ã Lord of the Lost . –í –ø–µ—Å–Ω–µ Abracadabra –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∏—Ü—ã Lady Gaga . ‚Üë –°–ª–æ–≤–∞—Ä—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤. ‚Äî –ú. : –†—É—Å. —è–∑., 1989. ‚Äî 624 —Å. ISBN 5-200-00408-8 ‚Üë –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞! (—Ä—É—Å.) Newslab.ru . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ 9 —è–Ω–≤–∞—Ä—è 2021 –≥–æ–¥–∞. ‚Üë –•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ (—Ä—É—Å.) . –ò–∑–≤–µ—Å—Ç–∏—è.—Ä—É . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ 8 —è–Ω–≤–∞—Ä—è 2021 –≥–æ–¥–∞. ‚Üë –§. –î–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π. –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ (—Ä—É—Å.) . –ö—É–ª—å—Ç—É—Ä–∞.–†–§ . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ 10 —è–Ω–≤–∞—Ä—è 2021 –≥–æ–¥–∞. ‚Üë –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ (–Ω–µ–æ–ø—Ä.) . slovardalja.net . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ 21 –∞–ø—Ä–µ–ª—è 2021 –≥–æ–¥–∞. ‚Üë –ö–æ–ª–ª. –∞–≤—Ç–æ—Ä–æ–≤. –°–ª–æ–≤–∞—Ä—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤. ‚Äî –ú. : –†—É—Å—Å–∫–∏–π —è–∑—ã–∫, 1989. ‚Äî 624 —Å. ‚Üë –ë. –ò. –ò–º—à–µ–Ω–µ—Ü–∫–∏–π , ¬´–¢–æ–ª–∫–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –≤–æ–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤¬ª, –°–ü–±. , 1914 –≥–æ–¥. ‚Üë –¢–∞–ª–º—É–¥ –ü—Å–∞—Ö–∏–º 112–∞ ‚Üë –ö—ç—Ä—Ä–æ–ª–ª –†. –¢. –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ // –≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–π: —Å–æ–±—Ä–∞–Ω–∏–µ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π –∏ –æ–ø–∞—Å–Ω—ã—Ö –ø–æ–≤–µ—Ä–∏–π . ‚Äî –ú. : –ò–∑–¥–∞—Ç–µ–ª—å—Å–∫–∏–π –¥–æ–º ¬´–í–∏–ª—å—è–º—Å¬ª, 2005. ‚Äî 672 —Å. ‚Äî ISBN 5-8459-0830-2 , ISBN 0-471-27242-6 . ‚Üë Abracadabra (–Ω–µ–æ–ø—Ä.) . www.skepdic.com . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ 25 —Ñ–µ–≤—Ä–∞–ª—è 2021 –≥–æ–¥–∞. –†–æ–±–µ—Ä—Ç –¢. –ö—ç—Ä—Ä–æ–ª–ª , –≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–π ‚Üë –•—É–∞–Ω –≠–¥—É–∞—Ä–¥–æ –ö–µ—Ä–ª–æ—Ç. –°–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤. ‚Äî –ú. : REFL-book, 1994. ‚Äî 608 —Å. ‚Äî ISBN 5-87983-014-4 . ‚Üë –≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è —Å–∏–º–≤–æ–ª–æ–≤, –∑–Ω–∞–∫–æ–≤, —ç–º–±–ª–µ–º / –ê–≤—Ç–æ—Ä-—Å–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ö, –ö–æ—Ä–æ–ª–µ–≤. ‚Äî –ú. : –≠–∫—Å–º–æ; –°–ü–±.: –ú–∏–¥–≥–∞—Ä–¥, 2008. ‚Äî –°. 17-18. ‚Üë –ü–∞–ø—é—Å. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –º–∞–≥–∏—è. –ù–∞—Å—Ç–æ–ª—å–Ω–∞—è –∫–Ω–∏–≥–∞ –º–∞–≥–∞. ‚Äî –ú. : –ü–æ–ø—É—Ä—Ä–∏, 2005. ‚Äî 624 —Å. ‚Äî ISBN 978-985-15-0331-1 . –ë. –ò. –ò–º—à–µ–Ω–µ—Ü–∫–∏–π , ¬´–¢–æ–ª–∫–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –≤–æ–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤¬ª, –°–ü–±. , 1914 –≥–æ–¥. –ê–º—É–ª–µ—Ç –∏ –µ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (–Ω–µ–æ–ø—Ä.) . ru.wikisource.org . –î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è: 6 —è–Ω–≤–∞—Ä—è 2021. . –ï–≤—Ä–µ–π—Å–∫–∞—è —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è –ë—Ä–æ–∫–≥–∞—É–∑–∞ –∏ –ï—Ñ—Ä–æ–Ω–∞ –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ // –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è : –≤ 11 —Ç. ‚Äî [ –ú. ], 1929‚Äî1939. –ö—ç—Ä—Ä–æ–ª–ª –†. –¢. –ê–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ // –≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–π: —Å–æ–±—Ä–∞–Ω–∏–µ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π –∏ –æ–ø–∞—Å–Ω—ã—Ö –ø–æ–≤–µ—Ä–∏–π . ‚Äî –ú. : –ò–∑–¥–∞—Ç–µ–ª—å—Å–∫–∏–π –¥–æ–º ¬´–í–∏–ª—å—è–º—Å¬ª, 2005. ‚Äî 672 —Å. ‚Äî ISBN 5-8459-0830-2 , ISBN 0-471-27242-6 . –ë–æ–ª—å—à–∞—è –Ω–æ—Ä–≤–µ–∂—Å–∫–∞—è –ë–æ–ª—å—à–∞—è —Å–æ–≤–µ—Ç—Å–∫–∞—è (1 –∏–∑–¥.) –ë—Ä–æ–∫–≥–∞—É–∑–∞ –∏ –ï—Ñ—Ä–æ–Ω–∞ –ï–≤—Ä–µ–π—Å–∫–∞—è –ë—Ä–æ–∫–≥–∞—É–∑–∞ –∏ –ï—Ñ—Ä–æ–Ω–∞ –õ–∏—Ç–æ–≤—Å–∫–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ú–∞–ª—ã–π –ë—Ä–æ–∫–≥–∞—É–∑–∞ –∏ –ï—Ñ—Ä–æ–Ω–∞ American Cycl. (1879) Britannica (9-th) Britannica (11-th) –í–∏–∫–∏–ø–µ–¥–∏—è:Cite web (–Ω–µ —É–∫–∞–∑–∞–Ω —è–∑—ã–∫) –°—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –≤–æ–ª—à–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏ ISBN –í–∏–∫–∏–ø–µ–¥–∏—è:–°—Ç–∞—Ç—å–∏ –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –ø–æ —Ç–∏–ø–∞–º) –í–∏–∫–∏–ø–µ–¥–∏—è:–ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Å–µ–Ω—Ç—è–±—Ä—è 2022 –í–∏–∫–∏–ø–µ–¥–∏—è:–°—Ç–∞—Ç—å–∏ —Å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏ –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –±–æ–ª–µ–µ 14 –¥–Ω–µ–π –°—Ç–∞—Ç—å–∏ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å",
    "extraction_method": "requests+bs4",
    "content_length": 10455,
    "status": "success",
    "error_message": null
  },
  {
    "query": "–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —á–µ–ø—É—Ö–∞ –∞–±—Ä–∞–∫–∞–¥–∞–±—Ä–∞ xyzzy —Ñ—ã–≤–∞–æ–ª–¥–∂",
    "plan_item": "–¢–µ—Å—Ç –æ—à–∏–±–∫–∏ –ø–æ–∏—Å–∫–∞",
    "plan_item_id": "plan_3",
    "query_id": "q_3_0",
    "source": "duckduckgo",
    "url": "https://pyx-1.pretendyoure.xyz/zy/game.jsp",
    "title": "Pretend You're Xyzzy",
    "text": "This webapp is still in development. There will be bugs, but hopefully they won't affect gameplay very much. If this is your first time playing, you may wish to readthe changelog and list of known issues. Your computer's IP address willalwaysbe logged when you load the game client. It is not tied in any way to your username, except possibly if a server error occurs. Gameplay results are logged permanently, but without information identifying you. Most recent update: 3 September 2018: Hey, this is important:Read the privacy page for details about what gameplay information is collected and how it's shared. Pretend You're Xyzzy is a Cards Against Humanity clone, which is available atcardsagainsthumanity.com, where you can buy it or download and print it out yourself. It is distributed under aCreative Commons - Attribution - Noncommercial - Share Alike license. This web version is in no way endorsed or sponsored by cardsagainsthumanity.com. You may download the source code to this version fromGitHub. For full license information, including information about included libraries, see thefull license information.",
    "extraction_method": "requests+bs4",
    "content_length": 1122,
    "status": "success",
    "error_message": null
  },
  {
    "query": "",
    "plan_item": "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å",
    "plan_item_id": "plan_empty",
    "query_id": "q_empty_0",
    "source": "task_error",
    "status": "invalid_query",
    "error_message": "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–¥–∞—á–µ",
    "url": null,
    "title": null,
    "text": null,
    "content_length": 0
  }
]